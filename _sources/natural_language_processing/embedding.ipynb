{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "788e855f-d8f2-49cf-937a-a500e375a5eb",
   "metadata": {},
   "source": [
    "# 単語埋め込み\n",
    "\n",
    "ディープラーニングが自然言語処理に活用され始める前は、単語を計算可能なベクトルに変換する処理としては、文書集合内の各単語がある文書に出現した場合に1を、そうでない場合は0を出すようなBag of Wordsというものがよく用いられた。これは扱うデータが大きく語彙数が多くなると非常に高次元になるため、特異値分解などで次元削減を行うこともあった。\n",
    "\n",
    "ディープラーニングが活用されるようになり登場したものが単語の**埋め込み**（embedding）や**分散表現**（distributed representation）と呼ばれるもので、これらは$\\{0, 1\\}$に限らない値をとり、また次元数を語彙の数よりも小さく（次元圧縮）することもできる（実はニューラルネットワークを使った言語モデルの中間層を埋め込みとするので、それゆえ任意の次元数にできる）。\n",
    "\n",
    "有名なのはWord2VecやElMoである。両者の大まかな違いとしては、Word2Vecは単語ごとに一意な分散表現になるが、ElMoは文脈によって分散表現が変わるということ。\n",
    "例えば「彼はスポーツが下手だ」と「彼はいつも下手に出がちだ」は同じ「下手」という語だが意味が異なる。Word2Vecはこのような文脈を考慮しないが、ElMoは考慮する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44685c9-191e-423c-b4a7-d125d676b2c2",
   "metadata": {},
   "source": [
    "## 共起行列\n",
    "\n",
    "文章$S = (w_1, \\dots, w_d)$のある単語$w_i$の周囲の単語の集合（例えば両側$c$個をとって$C=\\{ w_{i-c}, \\dots, w_{i-1}, w_{i+1}, \\dots, w_{i+c} \\}$）を**文脈**（context）$C$といい、各単語$w_j (i \\neq j)$が$C$に含まれるかどうかを$\\{0, 1\\}$で表現する。\n",
    "\n",
    "この関係性を表す行列を**共起行列**（co-occurence matrix）という。\n",
    "\n",
    "|       | $w_1$ | $w_2$ | $w_3$ | $w_4$ | $w_5$ |\n",
    "|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n",
    "| $w_3$ |   0   |   1   |   0   |   1   |   0   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f426cd2-ea2c-487e-aeb4-97a5e7e7fffa",
   "metadata": {},
   "source": [
    "## 言語モデル\n",
    "\n",
    "embeddingを直接推定するのではなく、単語予測モデル（言語モデル）を構築して副次的にembeddingを取得することになる。\n",
    "\n",
    "言語モデルとは、尤もらしい文章を生成できるような確率分布を習得するために、文章の確率を推定するモデルのこと。\n",
    "\n",
    "ある文章$S$をトークン化したのを$(w_1, w_2, \\cdots, w_n)$と表記するならば、\n",
    "\n",
    "$$\n",
    "P(S)=P(w_1, w_2, \\cdots, w_n)\n",
    "$$\n",
    "\n",
    "を求めたいということになる。これは条件付き確率の積として表せる\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(w_1, w_2, \\cdots, w_n)\n",
    "&= P(w_1) \\times P(w_2|w_1) \\times P(w_3|w_1, w_2) \\times \\cdots\\\\\n",
    "&= \\prod_{i=1}^n p(w_i|\\boldsymbol{c}_i)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "ここで$\\boldsymbol{c}_i$は$w_i$より前のトークン列$\\boldsymbol{c}_i=(w_1,w_2,\\cdots,w_n)$で、**文脈**（context）と呼ばれる\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4fe48a-15f3-45d0-840d-8698dc579c9e",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "```\n",
    "King - Man + Woman = Queen\n",
    "```\n",
    "のような語彙間の類似度や計算が可能な表現\n",
    "\n",
    "\n",
    "### Word2Vecのアプローチは複数ある\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Word2Vecの方法は、\n",
    "\n",
    "1. CBOW (continuous bag of words)\n",
    "2. Skip-Gram\n",
    "\n",
    "の2種類あり、またそのままでは計算量が多すぎるために計算量削減の方法として\n",
    "\n",
    "1. Hierarchical Softmax\n",
    "2. Noise Constrastive Estimation\n",
    "3. Negative Sampling\n",
    "\n",
    "の方法があるため、組み合わせとしては2×3の6通りのモデルの組み方がある\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c8df3b-6129-4c5a-9562-32ad093027a0",
   "metadata": {},
   "source": [
    "### CBOW\n",
    "\n",
    "**CBOW** (continuous bag of words)モデルは文章$S=(w_1, w_2, \\dots, w_n)$が与えられた時、その$i$番目の単語$w_i$を、\n",
    "その周りの単語である文脈$\\boldsymbol{C}_i=(w_{i-c}, \\dots, w_{i-1}, w_{i+1}, \\dots, w_{i+c})$から予測するモデルである。ここで$c$はウィンドウサイズと呼ばれるハイパーパラメータ。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0232fb8-5c83-40e2-81f8-3dcc9fcee7dc",
   "metadata": {},
   "source": [
    "## gensimによる実装\n",
    "\n",
    "https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "48f41bd2-6100-4e6f-9261-1ad844e6118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# トークンに分割した文章の集合の例\n",
    "sentences = [\n",
    "    [\"king\", \"male\", \"ruler\"],\n",
    "    [\"queen\", \"female\", \"ruler\"],\n",
    "    [\"man\", \"male\"],\n",
    "    [\"woman\", \"female\"],\n",
    "]\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a4a3b04c-f2d1-4e47-8c03-69d28de1fd8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('male', 0.1459505707025528),\n",
       " ('woman', 0.041577354073524475),\n",
       " ('man', 0.03476494178175926)]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kingに意味が近い上位3個（similarの基準はコサイン類似度）\n",
    "sims = model.wv.most_similar('king', topn=3)\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "492d7f5e-fb39-473e-9e86-4a18057b38df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.0066775488667190075)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# king + woman - man ≒ queen というアレ\n",
    "model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7072b846-dd0c-44ef-81c8-ac42540fc65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen: 0.00656\n",
      "ruler: -0.0263\n",
      "female: -0.0395\n",
      "male: -0.0251\n"
     ]
    }
   ],
   "source": [
    "# 自前で足し引きして類似度計算してみる\n",
    "import numpy as np\n",
    "def cosine_sim(a, b):\n",
    "    return a @ b / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "v = model.wv[\"king\"] - model.wv[\"man\"] + model.wv[\"woman\"]\n",
    "words = set(sum(sentences, [])) - {\"king\", \"man\", \"woman\"} # 計算に使ったものは必然的に類似度が高くなっちゃうので除く\n",
    "for word in words:\n",
    "    print(f\"{word}: {cosine_sim(v, model.wv[word]):.3g}\")\n",
    "\n",
    "# queenが一番近くなった"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe88711c-6f8f-473a-98fb-89efc614f81d",
   "metadata": {},
   "source": [
    "### 参考文献\n",
    "\n",
    "- https://code.google.com/archive/p/word2vec/\n",
    "- [[1310.4546] Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
