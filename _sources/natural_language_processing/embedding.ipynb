{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "788e855f-d8f2-49cf-937a-a500e375a5eb",
   "metadata": {},
   "source": [
    "# 単語埋め込み\n",
    "\n",
    "ディープラーニングが自然言語処理に活用され始める前は、単語を計算可能なベクトルに変換する処理としては、文書集合内の各単語がある文書に出現した場合に1を、そうでない場合は0を出すようなBag of Wordsというものがよく用いられた。\n",
    "\n",
    "ディープラーニングが活用されるようになり登場したものが単語の**埋め込み**（embedding）や**分散表現**（distributed representation）と呼ばれるもので、これらは$\\{0, 1\\}$に限らない値をとり、また次元数を語彙の数よりも小さく（次元圧縮）することもできる。\n",
    "\n",
    "有名なのはWord2VecやElMoである。両者の大まかな違いとしては、Word2Vecは単語ごとに一意な分散表現になるが、ElMoは文脈によって分散表現が変わるということ。\n",
    "例えば「彼はスポーツが下手だ」と「彼はいつも下手に出がちだ」は同じ「下手」という語だが意味が異なる。Word2Vecはこのような文脈を考慮しないが、ElMoは考慮する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f426cd2-ea2c-487e-aeb4-97a5e7e7fffa",
   "metadata": {},
   "source": [
    "## 言語モデル\n",
    "\n",
    "embeddingを直接推定するのではなく、単語予測モデル（言語モデル）を構築して副次的にembeddingを取得することになる。\n",
    "\n",
    "言語モデルとは、尤もらしい文章を生成できるような確率分布を習得するために、文章の確率を推定するモデルのこと。\n",
    "\n",
    "ある文章$S$をトークン化したのを$(w_1, w_2, \\cdots, w_n)$と表記するならば、\n",
    "\n",
    "$$\n",
    "P(S)=P(w_1, w_2, \\cdots, w_n)\n",
    "$$\n",
    "\n",
    "を求めたいということになる。これは条件付き確率の積として表せる\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(w_1, w_2, \\cdots, w_n)\n",
    "&= P(w_1) \\times P(w_2|w_1) \\times P(w_3|w_1, w_2) \\times \\cdots\\\\\n",
    "&= \\prod_{i=1}^n p(w_i|\\boldsymbol{c}_i)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "ここで$\\boldsymbol{c}_i$は$w_i$より前のトークン列$\\boldsymbol{c}_i=(w_1,w_2,\\cdots,w_n)$で、**文脈**（context）と呼ばれる\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4fe48a-15f3-45d0-840d-8698dc579c9e",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "```\n",
    "King - Man + Woman = Queen\n",
    "```\n",
    "のような語彙間の類似度や計算が可能な表現\n",
    "\n",
    "\n",
    "### Word2Vecのアプローチは複数ある\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Word2Vecの方法は、\n",
    "\n",
    "1. CBOW (continuous bag of words)\n",
    "2. Skip-Gram\n",
    "\n",
    "の2種類あり、またそのままでは計算量が多すぎるために計算量削減の方法として\n",
    "\n",
    "1. Hierarchical Softmax\n",
    "2. Noise Constrastive Estimation\n",
    "3. Negative Sampling\n",
    "\n",
    "の方法があるため、組み合わせとしては2×3の6通りのモデルの組み方がある\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c8df3b-6129-4c5a-9562-32ad093027a0",
   "metadata": {},
   "source": [
    "### CBOW\n",
    "\n",
    "**CBOW** (continuous bag of words)モデルは文章$S=(w_1, w_2, \\dots, w_n)$が与えられた時、その$i$番目の単語$w_i$を、\n",
    "その周りの単語である文脈$\\boldsymbol{C}_i=(w_{i-c}, \\dots, w_{i-1}, w_{i+1}, \\dots, w_{i+c})$から予測するモデルである。ここで$c$はウィンドウサイズと呼ばれるハイパーパラメータ。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0232fb8-5c83-40e2-81f8-3dcc9fcee7dc",
   "metadata": {},
   "source": [
    "gensim\n",
    "\n",
    "https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0265b1ad-95fa-4349-b834-f869df9d39af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "218e0b49-c556-40ab-8f44-097e009734fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# 逐次学習\n",
    "model.train([[\"hello\", \"world\"]], total_examples=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a69918d0-fd58-42ff-9e25-0e679de37cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00515774, -0.00667028, -0.0077791 ,  0.00831315, -0.00198292,\n",
       "       -0.00685696, -0.0041556 ,  0.00514562, -0.00286997, -0.00375075,\n",
       "        0.0016219 , -0.0027771 , -0.00158482,  0.0010748 , -0.00297881,\n",
       "        0.00852176,  0.00391207, -0.00996176,  0.00626142, -0.00675622,\n",
       "        0.00076966,  0.00440552, -0.00510486, -0.00211128,  0.00809783,\n",
       "       -0.00424503, -0.00763848,  0.00926061, -0.00215612, -0.00472081,\n",
       "        0.00857329,  0.00428459,  0.0043261 ,  0.00928722, -0.00845554,\n",
       "        0.00525685,  0.00203994,  0.0041895 ,  0.00169839,  0.00446543,\n",
       "        0.0044876 ,  0.0061063 , -0.00320303, -0.00457706, -0.00042664,\n",
       "        0.00253447, -0.00326412,  0.00605948,  0.00415534,  0.00776685,\n",
       "        0.00257002,  0.00811905, -0.00138761,  0.00808028,  0.0037181 ,\n",
       "       -0.00804967, -0.00393476, -0.0024726 ,  0.00489447, -0.00087241,\n",
       "       -0.00283173,  0.00783599,  0.00932561, -0.0016154 , -0.00516075,\n",
       "       -0.00470313, -0.00484746, -0.00960562,  0.00137242, -0.00422615,\n",
       "        0.00252744,  0.00561612, -0.00406709, -0.00959937,  0.00154715,\n",
       "       -0.00670207,  0.0024959 , -0.00378173,  0.00708048,  0.00064041,\n",
       "        0.00356198, -0.00273993, -0.00171105,  0.00765502,  0.00140809,\n",
       "       -0.00585215, -0.00783678,  0.00123305,  0.00645651,  0.00555797,\n",
       "       -0.00897966,  0.00859466,  0.00404816,  0.00747178,  0.00974917,\n",
       "       -0.0072917 , -0.00904259,  0.0058377 ,  0.00939395,  0.00350795],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = model.wv['computer']  # get numpy vector of a word\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3d604e4-9495-440b-84f5-8cdf66bc800b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('system', 0.21617139875888824),\n",
       " ('survey', 0.04468922317028046),\n",
       " ('interface', 0.015203381888568401),\n",
       " ('time', 0.0019510635174810886),\n",
       " ('trees', -0.03284316882491112),\n",
       " ('human', -0.07424270361661911),\n",
       " ('response', -0.09317591041326523),\n",
       " ('graph', -0.09575342386960983),\n",
       " ('eps', -0.10513808578252792),\n",
       " ('user', -0.16911619901657104)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = model.wv.most_similar('computer', topn=10)  # get other similar words\n",
    "sims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe88711c-6f8f-473a-98fb-89efc614f81d",
   "metadata": {},
   "source": [
    "### 参考文献\n",
    "\n",
    "- https://code.google.com/archive/p/word2vec/\n",
    "- [[1310.4546] Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b6da5b-9463-4a1f-88ca-2dc8a11a5db9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5150edb6-45fa-4e68-8990-b7324554fcea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
