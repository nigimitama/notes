{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e235f694-7c88-49ba-bf77-baed9844cc28",
   "metadata": {},
   "source": [
    "# LLMの評価"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a649b674-560e-4ec5-a894-7fcbbfc8ba46",
   "metadata": {},
   "source": [
    "Chang, et al. (2024). [A survey on evaluation of large language models.](https://dl.acm.org/doi/pdf/10.1145/3641289)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8f6fdb-7718-43c8-849d-15dd88eb8af8",
   "metadata": {},
   "source": [
    "### 人による評価\n",
    "\n",
    "- Human-in-the-loop testing：人間のフィードバックを集める。\n",
    "- Crowd-sourcing testing：クラウドワーカーに外注する。質は落ちるかもしれないが量が確保できる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01613b91-3d4f-4800-b490-58add4354c8a",
   "metadata": {},
   "source": [
    "## テスト / IRTによる評価\n",
    "\n",
    "- Blog: [Automated evaluation of RAG pipelines with exam generation - Amazon Science](https://www.amazon.science/blog/automated-evaluation-of-rag-pipelines-with-exam-generation)\n",
    "- Code: [amazon-science/auto-rag-eval: Code repo for the ICML 2024 paper \"Automated Evaluation of Retrieval-Augmented Language Models with Task-Specific Exam Generation\"](https://github.com/amazon-science/auto-rag-eval)\n",
    "\n",
    "手法の概要：LLMで4択テストを生成 → LLM+RAGで解く → テストの成績をIRTで評価\n",
    "\n",
    "所感：\n",
    "- 評価部分については、これまで人間相手に培ってきたIRTの研究成果をそのままLLMへ応用できるのは大きな強みとなりそう。\n",
    "- 一方で「どうやって良いテストを生成するか」はなかなかむずかしそう。\n",
    "- ただ、IRTの強みは等化だが、その必要性はあるのか？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
