{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ebcd054-73f7-4c0b-960d-6b89521c990c",
   "metadata": {},
   "source": [
    "# 深層学習の理論\n",
    "\n",
    "深層学習はなぜうまくいくのか？についての理論的な考察のまとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65d73e7-4141-4dac-ac61-eae34dec77dd",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c48c46-b35b-4cf5-b44a-a54675e46d30",
   "metadata": {},
   "source": [
    "## 普遍近似性\n",
    "\n",
    "80年代に普遍近似性 (Universal Approximation Property, or Universality)という性質があることが示された\n",
    "\n",
    ":::{card}\n",
    "\n",
    "無限のデータと素子があれば、2層のニューラルネットワークは任意の関数を任意の精度で近似できる\n",
    "\n",
    "[Hecht-Nielsen,1987][Cybenko,1989]\n",
    ":::\n",
    "\n",
    "以下のSNNなどはわかりやすいが、関数の和で関数近似するFourier変換などと似た構造\n",
    "\n",
    ":::{card} 浅いニューラルネット (shallow newral network:SNN)\n",
    "\n",
    "- 緩増加超関数（\"活性化関数\"） $\\sigma: \\mathbb{R} \\rightarrow \\mathbb{C}$ を固定する.\n",
    "    - 例えば$\\exp \\left(-t^2 / 2\\right), \\tanh (t), \\max \\{0, t\\}$(=\"ReLU\")\n",
    "\n",
    "$$\n",
    "\\operatorname{SNN}(\\boldsymbol{x} ; \\boldsymbol{\\theta}):=\\sum_{i=1}^p c_i \\sigma\\left(\\boldsymbol{a}_i \\cdot \\boldsymbol{x}-b_i\\right), \\quad \\boldsymbol{x} \\in \\mathbb{R}^m\n",
    "$$\n",
    "\n",
    "ここで $\\boldsymbol{\\theta}=\\left\\{\\left(\\boldsymbol{a}_i, b_i, \\boldsymbol{c}_i\\right)\\right\\}_{i=1}^p \\subset \\mathbb{R}^m \\times \\mathbb{R} \\times \\mathbb{R}$ はパラメータ\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eab3e7-2804-41fe-b1a3-2ae685ae803b",
   "metadata": {},
   "source": [
    "## Deepは不連続な関数も近似できる\n",
    "\n",
    "カーネル法も万能近似能力をもつ\n",
    "\n",
    "それらとDeepが違うのは、不連続な（ジャンプのある）関数も近似できること（今泉 2021）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79b03f1-e79f-4468-b354-611e0bc16a1f",
   "metadata": {},
   "source": [
    "## 参考\n",
    "\n",
    "- [鈴木 (2023) 深層学習の数理](https://ibis.t.u-tokyo.ac.jp/suzuki/lecture/2023/TohokuUniv/%E6%9D%B1%E5%8C%97%E5%A4%A7%E5%AD%A62023.pdf)（スライド）\n",
    "- [今泉允聡. (2021). 深層学習の原理に迫る　数学の挑戦. 岩波書店.](https://www.amazon.co.jp/dp/B09FPBQM9C)\n",
    "- [積分表現でニューラルネットを理解する（園田 翔 氏、FD研修会「人工知能と数学」） - YouTube](https://www.youtube.com/watch?v=z8TXubu3Uko&ab_channel=OCAMI_math)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd16801-4230-4c11-adc4-88508521ac0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
