{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2321a5cd-e251-4d46-a412-44b72151ecd2",
   "metadata": {},
   "source": [
    "# 汎化誤差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e81fc56-9a53-4996-b7b4-b0c244c7b449",
   "metadata": {},
   "source": [
    "## 様々な誤差の指標\n",
    "\n",
    "### 汎化誤差\n",
    "\n",
    "訓練データ$\\mathcal{T} = \\{(x_i, y_i)\\}_{i=1}^n$の下で、新しいデータ点$(X^0, Y^0)$でのモデル$\\hat{f}$の誤差の期待値をとったもの\n",
    "\n",
    "$$\n",
    "\\text{GE}\n",
    "= E_{X^0, Y^0} [ L(Y^0, \\hat{f}(X^0) ) | \\mathcal{T} ]\n",
    "$$\n",
    "\n",
    "\n",
    "は**真の誤差（true error）**あるいは**汎化誤差（generatization error）**あるいはextra-sample errorと呼ばれる\n",
    "\n",
    "\n",
    "### 期待誤差\n",
    "\n",
    "訓練セットで汎化誤差の期待値をとった\n",
    "\n",
    "$$\n",
    "\\text{EE} = E_{\\mathcal{T}}E_{X^0, Y^0} [ L(Y^0, \\hat{f}(X^0) ) | \\mathcal{T} ]\n",
    "$$\n",
    "\n",
    "を**期待誤差（expected error）**という。\n",
    "\n",
    "期待誤差のほうが統計的に扱いやすい\n",
    "\n",
    "### 訓練誤差\n",
    "\n",
    "訓練データで誤差の平均値をとったもの\n",
    "\n",
    "$$\n",
    "\\text{TE} = \\frac{1}{N} \\sum^N_{i=1} L(y_i, \\hat{f}(x_i))\n",
    "$$\n",
    "\n",
    "を**訓練誤差（training error）**あるいは**再代入誤り率（resubstitution error）**という。\n",
    "\n",
    "訓練誤差は汎化誤差以下になることが知られている\n",
    "\n",
    "\n",
    "はじパタによれば、再代入誤り率$TE$と$HoldOutError$と真の誤差$GE$の間には\n",
    "\n",
    "$$\n",
    "E_{D_L}[TE] \\leq GE \\leq E_{D_T}[HoldOutError]\n",
    "$$\n",
    "\n",
    "の関係性があるとされる（ここで$E_{D_L}[]$は多数の訓練データで計算して期待値をとったもの、$E_{D_T}[]$は訓練データは1つで多数のテストデータで期待値を摂ったもの）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f89adf9-9438-4f46-8faf-1975a9d15444",
   "metadata": {},
   "source": [
    "Conditional Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8176e5a4-7bb4-4c59-a24f-e31a5df2eea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ac2b602-0df0-433e-9d51-a040cf8a694e",
   "metadata": {},
   "source": [
    "## 汎化誤差\n",
    "\n",
    "目的変数$Y$、訓練済みモデル$\\hat{f}(X)$、誤差関数$L(Y, \\hat{f}(X))$について期待値をとったもの\n",
    "\n",
    "$$\n",
    "E[L(Y, \\hat{f}(X))]\n",
    "$$\n",
    "\n",
    "を汎化誤差という（厳密にはY,Xの同時分布での期待値）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a30572-cc76-41a9-9fce-3402f9f4e13c",
   "metadata": {},
   "source": [
    "## 汎化誤差の推定\n",
    "\n",
    "Hold out\n",
    "\n",
    "Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0ba0af-11a6-4342-b429-50f6e59dc5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa85c652-a104-472f-8234-433113ad6328",
   "metadata": {},
   "source": [
    "## 汎化誤差上界\n",
    "\n",
    "\n",
    "### Chernoff bound\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### PAC-Bayesian bound\n",
    "\n",
    "Catoni bound\n",
    "\n",
    "[[2110.11216] User-friendly introduction to PAC-Bayes bounds](https://arxiv.org/abs/2110.11216)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e1d71-9c4c-4126-8cb2-23fe5d7afc61",
   "metadata": {},
   "source": [
    "### 訓練誤差ベース\n",
    "\n",
    "訓練誤差ベースの汎化誤差上界は実験してみると100%近くの意味のない値になることも多い[[2012.04115] Generalization bounds for deep learning](https://arxiv.org/abs/2012.04115)\n",
    "\n",
    "### テスト誤差ベース\n",
    "\n",
    "テスト用データを使って汎化誤差上界を計算したもの\n",
    "\n",
    "### ノイズ付加\n",
    "\n",
    "ノイズ耐性と汎化性能は相関する\n",
    "\n",
    "そこでノイズ付加汎化誤差上界を計算するアプローチがある\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c97621-f514-479b-8e0e-31f2fa1820eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
