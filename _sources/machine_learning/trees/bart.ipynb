{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bccde794-0a66-4511-b83d-d66bd50349cb",
   "metadata": {},
   "source": [
    "# BART: Bayesian additive regression trees\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6ec821-5d5c-4d79-90e3-6098ee9eaa49",
   "metadata": {},
   "source": [
    "## BART\n",
    "\n",
    "[[0806.3286] BART: Bayesian additive regression trees](https://arxiv.org/abs/0806.3286)\n",
    "\n",
    "\n",
    "BARTはベイジアンなGBDTのような手法で、予測値の事後分布を得られる\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b9656a-266b-4159-933c-469b9e0b4cc9",
   "metadata": {},
   "source": [
    "### 問題設定\n",
    "\n",
    "連続変数の目的変数$y$が、特徴ベクトル$x = (x_1, \\dots, x_p)$ と 未知の関数$f(x)$ とで\n",
    "\n",
    "$$\n",
    "y = f(x) + \\varepsilon, \\quad \\varepsilon \\sim N(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "という関係性になっているとする。この$f(x)$を推定したい。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fa3456-3975-4975-b957-a72127454853",
   "metadata": {},
   "source": [
    "### モデル\n",
    "\n",
    "二分木 $T$ が $b$ 個の終端ノードを持っているとし、そのパラメータ集合を $M = \\{ \\mu_1, \\dots, \\mu_b \\}$ とする。入力$x$に対し出力$\\mu_i \\in M$を割り振る写像を$g(x \\mid T, M)$とすると、木の加法モデルは次のように表される。\n",
    "\n",
    "$$\n",
    "y = \\sum_{j=1}^m g(x \\mid T_j, M_j) + \\varepsilon, \\quad \\varepsilon \\sim N(0, \\sigma^2)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcabca4-4934-4e03-a70e-7225de6a04a0",
   "metadata": {},
   "source": [
    "### 事前分布による正則化\n",
    "\n",
    "### $T_j$の事前分布\n",
    "\n",
    "二分木 $T_j$の事前分布$p(T_j)$は、深さ$d$が終端でないとき、\n",
    "\n",
    "$$\n",
    "\\alpha(1+d)^{-\\beta}, \\quad \\alpha \\in(0,1), \\beta \\in[0, \\infty)\n",
    "$$\n",
    "\n",
    "で与えられる。\n",
    "\n",
    "### $\\mu_{ij}\\mid T_j$ の事前分布\n",
    "\n",
    "終端ノードのパラメータ$\\mu_{ij}$について、\n",
    "\n",
    "$$\n",
    "\\mu_{i j} \\sim N\\left(0, \\sigma_\\mu^2\\right) \\quad \\text { where } \\sigma_\\mu=\\frac{0.5}{k \\sqrt{m}}\n",
    "$$\n",
    "\n",
    "とする。ここで$k$はハイパーパラメータで、cross validationなどで決める。$m$は木の本数なので、多数の木を使う複雑なモデルほど$\\sigma_\\mu$は小さくなり、出力がゼロになる確率が上がる\n",
    "\n",
    "### $\\sigma$の事前分布\n",
    "\n",
    "逆カイ二乗分布に従うとする\n",
    "\n",
    "$$\n",
    "\\sigma^2 \\sim \\frac{\\nu \\lambda}{ \\chi_\\nu^2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5831e8e-bcb8-4527-bf77-938960ee6e82",
   "metadata": {},
   "source": [
    "### パラメータ推定\n",
    "\n",
    "事後分布 $p((T_1, M_1), \\ldots,(T_m, M_m), \\sigma \\mid y)$ の推定には **Bayesian backfitting MCMC (Hastie and Tibshirani, 2000)** を使う。これは残差に対して適応して段階的に加法モデルを作っていくアプローチで、ようは勾配ブースティングのベイズ版である。\n",
    "\n",
    "\n",
    "$T_{\\not \\ j}$を $T_j$以外の$m-1$個の木のパラメータ集合とし、同様に$M_{\\not \\ j}$ も定義する。\n",
    "\n",
    "$j=1,\\dots, m$について、$j$番目の木は\n",
    "\n",
    "$$\n",
    "(T_j, M_j) \\mid T_{(j)}, M_{(j)}, \\sigma, y\n",
    "$$\n",
    "\n",
    "とサンプリングする。この$T_{(j)}, M_{(j)}, \\sigma, y$による条件付けは、$j$番目の木を除いた残差\n",
    "\n",
    "$$\n",
    "R_j \\equiv y-\\sum_{k \\neq j} g\\left(x ; T_k, M_k\\right)\n",
    "$$\n",
    "\n",
    "を使って\n",
    "\n",
    "$$\n",
    "(T_j, M_j) \\mid R_j, \\sigma\n",
    "$$\n",
    "\n",
    "と実装できる。\n",
    "\n",
    "\n",
    "\n",
    "$\\sigma$は完全条件からサンプリングする\n",
    "\n",
    "$$\n",
    "\\sigma \\mid T_1, \\ldots, T_m, M_1, \\ldots, M_m, y\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba606ebf-4e92-4c0d-a5d8-6a2e496028d4",
   "metadata": {},
   "source": [
    "## BARTによる因果推論\n",
    "\n",
    "[Bayesian Nonparametric Modeling for Causal Inference: Journal of Computational and Graphical Statistics: Vol 20, No 1](https://www.tandfonline.com/doi/abs/10.1198/jcgs.2010.08162)\n",
    "\n",
    "BARTを使って推定した1つの予測モデル $f(x, z)$ を使って、処置変数$z$だけ変えて差分をとってCATEを推定するというもの\n",
    "\n",
    "$$\n",
    "\\tau(x) = f(x, 1) - f(x, 0)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd1d7a2-321f-4dd6-ae8e-15a6037c9498",
   "metadata": {},
   "source": [
    "### Bayesian Causal Forest\n",
    "\n",
    "[[1706.09523] Bayesian regression tree models for causal inference: regularization, confounding, and heterogeneous effects](https://arxiv.org/abs/1706.09523)\n",
    "\n",
    "$$\n",
    "E\\left(Y_i \\mid x_i, z_i\\right)=\\mu\\left(x_i, \\hat{\\pi}\\left(x_i\\right)\\right)+\\tau\\left(x_i\\right) z_i\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a94c3d0-8b94-430d-9aa0-44ba3464ed39",
   "metadata": {},
   "source": [
    "## 参考\n",
    "\n",
    "- [BART: Bayesian additive regression treesによる因果推論 - 勉強の記録](https://tmitani-tky.hatenablog.com/entry/2019/11/14/014056)\n",
    "- [Chipman, H. A., George, E. I., & McCulloch, R. E. (2010). BART: Bayesian additive regression trees.](https://arxiv.org/abs/0806.3286)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278076e3-319e-4527-93a0-338d8caf4e57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
