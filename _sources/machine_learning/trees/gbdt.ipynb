{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3002c51f-61df-40bc-936a-c029a8df9f52",
   "metadata": {},
   "source": [
    "# 勾配ブースティング決定木"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5970fb5-ca5d-4f4f-9bef-36047a2b8a6d",
   "metadata": {},
   "source": [
    "## ブースティング\n",
    "\n",
    "ブースティング（boosting）は「（0.5よりわずかに正解率が高い程度の）弱い学習器を使って強い学習器を作ることはできるか？」という仮説（boosting hypothethis）から生まれた\n",
    "\n",
    "Schapireの提案は\n",
    "\n",
    "1. 分類器$h_1$を$N$個のサンプルで訓練する\n",
    "2. $h_1$で分類に失敗したサンプルと正しく分類できたサンプルを半々にした$N$個のサンプルで$h_2$を訓練する\n",
    "3. $h_1$と$h_2$で分類結果が一致しない$N$個で$h_3$を訓練する\n",
    "3. ブーストされた分類器$h_B$は$h_1, h_2, h_3$の分類結果の多数決で分類を行う\n",
    "\n",
    "\n",
    "複数の学習器$f_k(x)$の線形和\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\sum^K_{k=1} w_k f_k(x_i)\n",
    "$$\n",
    "\n",
    "によって予測値$\\hat{y}_i$を出力する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628c15b6-9470-4750-add2-bda7e660e23a",
   "metadata": {},
   "source": [
    "## Gradient Boosting Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fffe75-6835-4ea7-92a0-a05d6df68198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56e8cc0a-f746-47b8-a0ec-a5b6e022cb62",
   "metadata": {},
   "source": [
    "## 正則化つきGBDT\n",
    "\n",
    "$n$個の観測データがあり、$m$次元の特徴量があるとする。\n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\{ (x_i, y_i) \\}, |D| = n, x_i \\in \\mathbb{R}^m, y \\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "\n",
    "勾配ブースティング決定木のモデルは次のように表される\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\phi(x_i) = \\sum^K_{k=1} f_k(x_i)\n",
    "$$\n",
    "\n",
    "ここで\n",
    "- $f_k \\in \\mathcal{F}$は予測器\n",
    "- $\\mathcal{F} = \\{ f(x) = w_{q(x)} \\}$は回帰木の空間\n",
    "  - $q: \\mathbb{R}^m \\to T$は入力データ$x$を木の各葉のインデックスに割り振る写像。$T$は各木の葉の数\n",
    "  - $w \\in \\mathbb{R}^T$は葉の重み（weights）と呼ばれ、予測に使われた葉の出力値。予測値は$w_{q(x)}$の和となるので予測値のベースでもある\n",
    "\n",
    "\n",
    "学習の際は正則化付き誤差関数\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\phi) = \\sum^n_{i=1} l(\\hat{y}, y_i) + \\sum_k \\Omega(f_k)\n",
    "$$\n",
    "\n",
    "を最小化する。ここで$l$は微分可能な凸関数である誤差関数で、$\\Omega$は正則化項\n",
    "\n",
    "$$\n",
    "\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda ||w||^2\n",
    "$$\n",
    "\n",
    "\n",
    "学習は加法的に行うため$t$番目の誤差は次のようになる。\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{(t)}(\\phi) = \\sum^n_{i=1} l(y_i, \\hat{y}^{(t-1)} + f_t(x_i)) + \\Omega(f_t)\n",
    "$$\n",
    "\n",
    "\n",
    "### テイラー展開による近似\n",
    "\n",
    "この誤差を二次近似したものを使うことで計算量を削減することもできることが知られている（[Friedman et al., 2000](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=6f4493eff2531536a7aeb3fc11d62c30a8f487f6)）\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{(t)} \\approx \n",
    "\\sum^n_{i=1} [\n",
    "     l(y_i, \\hat{y}^{(t-1)})\n",
    "     + g_i f_t(x_i)\n",
    "     + \\frac{1}{2} h_i f_t^2(x_i)\n",
    "]\n",
    " + \\Omega(f_t)\n",
    "$$\n",
    "\n",
    "\n",
    "ここで\n",
    "\n",
    "$$\n",
    "g_i = \\frac{ \\partial l(y_i, \\hat{y}^{(t-1)}) }{\\partial \\hat{y}^{(t-1)} }\\\\\n",
    "h_i = \\frac{ \\partial^2 l(y_i, \\hat{y}^{(t-1)}) }{\\partial (\\hat{y}^{(t-1)})^2 }\n",
    "$$\n",
    "\n",
    "定数項を省略すると\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathcal{L}}^{(t)} =\n",
    "\\sum^n_{i=1} [\n",
    "     g_i f_t(x_i)\n",
    "     + \\frac{1}{2} h_i f_t^2(x_i)\n",
    "]\n",
    " + \\Omega(f_t)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15246a5-12c4-498a-8141-e03db95bc801",
   "metadata": {},
   "source": [
    "\n",
    "葉$j$におけるインスタンス（サンプル）の集合を$I_j = \\{i|q(x_i) = j\\}$と表記すると、次のように書き換えることができる\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\tilde{\\mathcal{L}}^{(t)}\n",
    "&= \\sum^n_{i=1} [\n",
    "     g_i f_t(x_i)\n",
    "     + \\frac{1}{2} h_i f_t^2(x_i)\n",
    "] + \\gamma T + \\frac{1}{2} \\lambda \\sum^T_{j=1} w_j^2\n",
    "\\\\\n",
    "&= \\sum^T_{j=1} [\n",
    "     (\\sum_{i\\in I_j} g_i) w_j\n",
    "     + \\frac{1}{2} (\\sum_{i\\in I_j} h_i + \\lambda ) w^2_j\n",
    "] + \\gamma T\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "固定した木の構造$q(x)$について、葉$j$の最適な重みは\n",
    "\n",
    "$$\n",
    "w^*_j = -\\frac{ \\sum_{i\\in I_j} g_i }{ \\sum_{i \\in I_j} h_i + \\lambda }\n",
    "$$\n",
    "\n",
    "となる。\n",
    "\n",
    ":::{dropdown} 導出\n",
    "\n",
    "葉$j$についての部分だけ取り出して導関数を0とおいて整理する\n",
    "\n",
    "$$\n",
    "\\frac{ \\partial \\tilde{\\mathcal{L}}^{(t)}_j }{\\partial w_j }\n",
    "= \\sum_{i\\in I_j} g_i + (\\sum_{i\\in I_j} h_i + \\lambda ) w_j\n",
    "= 0\n",
    "\\\\\n",
    "\\implies\n",
    "(\\sum_{i\\in I_j} h_i + \\lambda ) w_j\n",
    "= -\\sum_{i\\in I_j} g_i\n",
    "\\\\\n",
    "\\implies\n",
    "w_j = -\\frac{ \\sum_{i\\in I_j} g_i }{ \\sum_{i\\in I_j} h_i + \\lambda } = w_j^*\n",
    "$$\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd4ddc9-9ac8-48cb-968e-5490dda7f895",
   "metadata": {},
   "source": [
    "最適な重み$w_j^*$を誤差関数に戻すと\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathcal{L}}^{(t)}(q)\n",
    "= -\\frac{1}{2} \\sum^T_{j=1}\n",
    "\\frac{ (\\sum_{i\\in I_j} g_i)^2 }\n",
    "{ \\sum_{i\\in I_j} h_i + \\lambda }\n",
    "+ \\gamma T\n",
    "$$\n",
    "\n",
    "となり、これ木の構造$q$の品質を評価する関数として使うことができる。\n",
    "\n",
    ":::{dropdown} 導出\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\tilde{\\mathcal{L}}^{(t)}\n",
    "&= \\sum^T_{j=1} [\n",
    "     (\\sum_{i\\in I_j} g_i)\n",
    "     (-\\frac{ \\sum_{i\\in I_j} g_i }{ \\sum_{i \\in I_j} h_i + \\lambda })\n",
    "     + \\frac{1}{2} (\\sum_{i\\in I_j} h_i + \\lambda ) \n",
    "     (-\\frac{ \\sum_{i\\in I_j} g_i }{ \\sum_{i \\in I_j} h_i + \\lambda })^2\n",
    "] + \\gamma T\n",
    "\\\\\n",
    "&= \\sum^T_{j=1} [\n",
    "     -\\frac{ (\\sum_{i\\in I_j} g_i)^2 }{ \\sum_{i \\in I_j} h_i + \\lambda }\n",
    "     + \\frac{1}{2} \\frac{ (\\sum_{i\\in I_j} g_i)^2 }{ \\sum_{i \\in I_j} h_i + \\lambda }\n",
    "] + \\gamma T\n",
    "\\\\\n",
    "&= \\sum^T_{j=1} [\n",
    "     - \\frac{1}{2} \\frac{ (\\sum_{i\\in I_j} g_i)^2 }{ \\sum_{i \\in I_j} h_i + \\lambda }\n",
    "] + \\gamma T\n",
    "\\end{align}\n",
    "$$\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f480c9d3-65b1-40f0-876c-17061900efd0",
   "metadata": {},
   "source": [
    "## 参考\n",
    "\n",
    "- [Schapire, R. E. (1990). The strength of weak learnability. Machine learning, 5, 197-227.](http://rob.schapire.net/papers/strengthofweak.pdf)\n",
    "- [Friedman, J., Hastie, T., & Tibshirani, R. (2000). Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the authors). The annals of statistics, 28(2), 337-407.](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=6f4493eff2531536a7aeb3fc11d62c30a8f487f6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6fb8a5-2952-4cbe-a6d6-dada5dc39a28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
