{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "526fc337-bb15-4e09-ab32-17888c981a1e",
   "metadata": {},
   "source": [
    "# 学習アルゴリズムの評価"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c05559b-b2a2-4bda-b455-97abcf8a0b5e",
   "metadata": {},
   "source": [
    "## 損失関数\n",
    "\n",
    "損失関数を$\\ell(\\hat{y}, y)$とする。\n",
    "\n",
    "- 二乗損失 $\\ell(\\hat{y}, y) = (\\hat{y} - y)^2$\n",
    "- 0-1損失 $\\ell(\\hat{y}, y) = \\mathbf{1}[\\hat{y} \\neq y]$\n",
    "\n",
    "### 予測損失\n",
    "\n",
    "仮説$h$の **予測損失（predictive loss）** $R(h)$を、予測値$h(X)$の損失のテストデータ$(X,Y)$の分布上の期待値\n",
    "\n",
    "$$\n",
    "R(h) := E_{(X,Y) \\sim D}[\\ell(h(X), Y)]\n",
    "$$\n",
    "\n",
    "と定義する。\n",
    "\n",
    "### 経験損失\n",
    "\n",
    "**経験損失（empirical loss）** を、観測値に基づく損失の平均値\n",
    "\n",
    "$$\n",
    "\\hat{R}(h) := \\frac{1}{n} \\sum_{i=1}^n \\ell\\left(h\\left(X_i\\right), Y_i\\right)\n",
    "$$\n",
    "\n",
    "と定義する。\n",
    "\n",
    "経験損失は別の表現もできる。データ数が$n$のとき確率$1/n$で$(X_i,Y_i)$に値を取る確率変数を$(X,Y)$とし、これが従う分布（**経験分布**）を$\\hat{D}$とする。このとき経験損失は\n",
    "\n",
    "$$\n",
    "\\hat{R}(h) := E_{(X,Y) \\sim \\hat{D}}[\\ell(h(X), Y)]\n",
    "$$\n",
    "\n",
    "と表すことができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44b3624-adb9-401d-9ebf-23d1a53011b6",
   "metadata": {},
   "source": [
    "### 経験損失は予測損失の不偏推定量\n",
    "\n",
    "データ$(X_i,Y_i)$が同一の分布$D$に従うとき、経験損失の期待値は予測損失に一致する。\n",
    "実際、$n$個の観測データの同時分布を$D^n$とすると、\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{D^n}[\\hat{R}(h)]\n",
    "=\\frac{1}{n} \\sum_{i=1}^n \\mathbb{E}_{D^n}[\\ell(h(X_i), Y_i)]\n",
    "=\\frac{1}{n} \\sum_{i=1}^n R(h)=R(h)\n",
    "$$\n",
    "\n",
    "となる。したがって経験損失は予測損失の不偏推定量である。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbde0bb7-9ba5-47bb-aedc-70ad8b54887f",
   "metadata": {},
   "source": [
    "## ベイズ規則\n",
    "\n",
    ":::{admonition} 定義\n",
    "\n",
    "損失関数 $\\ell$ を定めたとき、任意の可測関数 $h: \\mathcal{X} \\rightarrow \\mathcal{Y}$ のもとでの予測損失の下限\n",
    "\n",
    "$$\n",
    "\\inf _{h: \\text { 可測 }} R(h)\n",
    "$$\n",
    "\n",
    "\n",
    "を損失関数 $\\ell$ のもとでの **ベイズ誤差（Bayes error）** という。下限を達成する仮説が存在するとき、その仮説を **ベイズ規則（Bayes rule）** という。\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e43c45c-f3c9-4ebf-bdd5-9175d5054344",
   "metadata": {},
   "source": [
    "ベイズ規則が$h_0: \\mathcal{X} \\rightarrow \\mathcal{Y}$のとき、$R(h_0) = \\inf _{h: \\text { 可測 }} R(h)$が成り立つ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221af37e-94b5-430b-ac4b-9ea23a738f28",
   "metadata": {},
   "source": [
    "条件付き期待値\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_Y[\\ell(h(x), Y) \\mid x] = \\int_{\\mathcal{Y}} \\ell(h(x), y) ~ d P(y \\mid x)\n",
    "$$\n",
    "\n",
    "を用いると\n",
    "\n",
    "$$\n",
    "R(h)=\\mathbb{E}_X [\\mathbb{E}_Y[\\ell(h(X), Y) \\mid X]]\n",
    "$$\n",
    "\n",
    "であるため、$\\mathbb{E}_Y[\\ell(h(x), Y) \\mid x]$を最小にする仮説$h$を選べば予測誤差が最小になる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d20ead-1fcb-41b5-9207-8cfc6da2eaf2",
   "metadata": {},
   "source": [
    "簡単のため$\\hat{y}=h(x)$とし、$X$を省略して、\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_Y[\\ell(\\hat{y}, Y)]\n",
    "= \\int \\ell(\\hat{y}, y) ~ d P(y)\n",
    "$$\n",
    "\n",
    "を最小にする$\\hat{y} \\in \\mathcal{Y}$を求める問題を考える\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e54973-98cc-431a-925f-03de16354f2b",
   "metadata": {},
   "source": [
    "### 0-1損失のベイズ規則\n",
    "\n",
    "損失関数として0-1損失 $\\ell(\\hat{y}, y) = \\mathbf{1}[\\hat{y} \\neq y]$を利用するとき、\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}_Y[\\ell(\\hat{y}, Y)]\n",
    "&= \\sum_{y\\in\\mathcal{Y}} \\ell(\\hat{y}, y) P(Y=y)\\\\\n",
    "&= 1 - \\underbrace{ P(Y = \\hat{y}) }_{正答率}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となるため、$P(Y = \\hat{y})$を最大化するような$\\hat{y}$、つまり\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\mathop{\\rm arg~max}\\limits_{y\\in\\mathcal{Y}} P(Y = y)\n",
    "$$\n",
    "\n",
    "が予測誤差を最小とする。条件付き期待値に戻って考えると、ベイズ規則$h_0(x)$は\n",
    "\n",
    "$$\n",
    "h_0(x) = \\mathop{\\rm arg~max}\\limits_{y\\in\\mathcal{Y}} P(Y = y | x)\n",
    "$$\n",
    "\n",
    "となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debe572b-99cb-47de-b501-c131e4085e0d",
   "metadata": {},
   "source": [
    "### 二乗損失のベイズ規則\n",
    "\n",
    "損失関数として二乗損失 $\\ell(\\hat{y}, y) = (\\hat{y} - y)^2$を利用するとき、\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}_Y[\\ell(\\hat{y}, Y)]\n",
    "&= \\mathbb{E}[(\\hat{y} - Y)^2]\\\\\n",
    "&= \\mathbb{E}[(\\hat{y} - \\mathbb{E}[Y] + \\mathbb{E}[Y]  - Y)^2] \\quad (同じ数を足して引く)\\\\\n",
    "&= \\mathbb{E}[\n",
    "    (\\hat{y} - \\mathbb{E}[Y])^2\n",
    "    + 2(\\hat{y} - \\mathbb{E}[Y])(\\mathbb{E}[Y]  - Y)\n",
    "    + (\\mathbb{E}[Y]  - y)^2]\\\\\n",
    "&= \\underbrace{ \\mathbb{E}[(\\hat{y} - \\mathbb{E}[Y])^2] }_{=(\\hat{y} - \\mathbb{E}[Y])^2}\n",
    "     + \\underbrace{ 2 \\mathbb{E}[(\\hat{y} - \\mathbb{E}[Y])(\\mathbb{E}[Y]  - Y)] }_{\n",
    "    \\begin{aligned}\n",
    "        &=2 \\mathbb{E}[\\hat{y}\\mathbb{E}[Y] - \\hat{y}Y - \\mathbb{E}[Y]^2 + \\mathbb{E}[Y]Y]\\\\\n",
    "        &=2 \\hat{y}\\mathbb{E}[Y] - 2\\hat{y}\\mathbb{E}[Y] - 2\\mathbb{E}[Y]^2 + 2\\mathbb{E}[Y]^2\\\\\n",
    "        &=0\n",
    "        \\end{aligned}\n",
    "    }\n",
    "    + \\mathbb{E}[(\\mathbb{E}[Y]  - Y)^2]\\\\\n",
    "&= (\\hat{y} - \\mathbb{E}[Y])^2\n",
    "    + \\operatorname{Var}[Y]\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となる。よって$\\hat{y}=\\mathbb{E}[Y]$とすれば予測誤差が最小になる。\n",
    "\n",
    "条件付き期待値に戻って考えると、ベイズ規則は\n",
    "\n",
    "$$\n",
    "h_0(x) = \\mathbb{E}[Y|x]\n",
    "$$\n",
    "\n",
    "によって与えられる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db814482-d771-434b-83f7-1a798f1e6ac7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "ベイズ誤差$R^*$は\n",
    "\n",
    "$$\n",
    "R^* = R\\left(h_0\\right)\n",
    "=\\mathbb{E}_X[V[Y \\mid X]]\n",
    "=\\mathbb{E}_X\\left[\\int(y-E[Y \\mid X])^2 d P(y \\mid X)\\right]\n",
    "$$\n",
    "\n",
    "となる。条件付き分散$V[Y|x]$が入力によらず一定の値$\\sigma^2$をとるとき、ベイズ誤差は$\\sigma^2$となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b5824a-e365-42ac-a60e-f9ebb20fb2de",
   "metadata": {},
   "source": [
    "## アルゴリズムの評価\n",
    "\n",
    "観測したサンプル$S=\\{(X_1, Y_1), \\dots, (X_n, Y_n)\\}$から得られる仮説を$h_S$とする。\n",
    "\n",
    "### 期待予測損失\n",
    "\n",
    "学習された仮説$h_S$の評価尺度の1つとして、観測データ$S$の分布$D^n$に関する予測損失の期待値$\\mathbb{E}_{S \\sim D^n}[R(h_S)]$が考えられる。これは **期待予測損失** と呼ばれる。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1eed7d-f819-4ac6-8159-3a9444324bfa",
   "metadata": {},
   "source": [
    "### 統計的一致性\n",
    "\n",
    "別の評価尺度として、$R(h_S)$の分布に着目する方法もある。ベイズ誤差$R^* = \\inf_h R(h)$と$\\delta\\in(0,1)$と$\\varepsilon>0$に対して\n",
    "\n",
    "$$\n",
    "\\operatorname*{Pr}_{S \\sim D^n}\\left(R(h_S) - R^* < \\varepsilon \\right)>1-\\delta\n",
    "$$\n",
    "\n",
    "が成り立つとし、$1-\\delta$に対して$\\varepsilon$がどのような値になるかを調べることで学習アルゴリズムの性能を評価する。\n",
    "\n",
    "ベイズ誤差に近い予測損失を達成する仮説が得られる場合、統計的一致性をもつ学習アルゴリズムと呼ばれる\n",
    "\n",
    ":::{admonition} 定義（統計的一致性）\n",
    "\n",
    "任意の分布 $D$ と任意の $\\varepsilon>0$ に対して\n",
    "\n",
    "$$\n",
    "\\lim _{n \\rightarrow \\infty} \\operatorname*{Pr}_{S \\sim D^n}\\left(R\\left(h_S\\right) \\leq R^*+\\varepsilon\\right)=0\n",
    "$$\n",
    "\n",
    "が成り立つとき、学習アルゴリズム $S \\mapsto h_S$ は **統計的一致性 (statistical consistency)** をもつという。\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be620ffe-8ea0-4c2c-85e7-be4dc27e3243",
   "metadata": {},
   "source": [
    "## 有限な仮説集合を用いた学習\n",
    "\n",
    "仮説集合が有限集合の場合の予測損失の評価について。2値判別問題で考え、入力空間$\\mathcal{X}$から2値ラベル$\\{+1,-1\\}$への関数$h$の有限な仮説集合\n",
    "\n",
    "$$\n",
    "\\mathcal{H}=\\left\\{h_1, \\ldots, h_T\\right\\}\n",
    "$$\n",
    "\n",
    "を用いて学習を行う（なお、ベイズ規則が$\\mathcal{H}$に含まれるとは限らない）。ある分布$P$に独立に従う学習データ$S=\\{(X_1, Y_1), \\dots, (X_n, Y_n)\\}$が与えられたとき、経験判別誤差を最小にする仮説\n",
    "\n",
    "$$\n",
    "h_S=\\operatorname*{argmin}_{h \\in \\mathcal{H}} \\hat{R}(h)\n",
    "$$\n",
    "\n",
    "を出力する学習アルゴリズムを考える。また、$\\mathcal{H}$の中で予測誤差を最小にする仮説を$h_\\mathcal{H}$とする。\n",
    "\n",
    "仮説\n",
    "\n",
    "- $h_0$：ベイズ規則$h_0 = \\inf R(h)$\n",
    "- $h_\\mathcal{H}$：$\\mathcal{H}$の中で予測誤差を最小にする仮説$h_\\mathcal{H}=\\operatorname*{argmin}_{h \\in \\mathcal{H}} R(h)$\n",
    "- $h_S$：仮説集合とサンプルのもとで経験損失を最小化する仮説 $h_S=\\operatorname*{argmin}_{h \\in \\mathcal{H}} \\hat{R}(h)$\n",
    "\n",
    "の関係は、定義より\n",
    "\n",
    "$$\n",
    "R(h_0) \\leq R(h_\\mathcal{H}) \\leq R(h_S),\n",
    "\\quad\n",
    "\\hat{R}(h_S) \\leq \\hat{R}(h_\\mathcal{H})\n",
    "$$\n",
    "\n",
    "となる。\n",
    "\n",
    "差 $R(h_S) - R(h_0)$ の上界を次のように評価する。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&R(h_S) - R(h_0)\\\\\n",
    "&= R(h_S) - \\hat{R}(h_S) + \\hat{R}(h_S) - R(h_\\mathcal{H}) + R(h_\\mathcal{H}) - R(h_0)\\\\\n",
    "&\\leq R(h_S) - \\hat{R}(h_S) + \\hat{R}(h_\\mathcal{H}) - R(h_\\mathcal{H}) + R(h_\\mathcal{H}) - R(h_0)\\\\\n",
    "&\\leq 2 \\max_{h\\in \\mathcal{H}} | \\hat{R}(h) - R(h) | + R(h_\\mathcal{H}) - R(h_0)\\\\\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efee5d7-f5b5-4762-b51b-8ab696a06240",
   "metadata": {},
   "source": [
    ":::{admonition} 補題（ヘフディングの不等式 Hoeffding's inequality）\n",
    "\n",
    "\n",
    "確率変数 $Z$ は有界区間 $[0,1]$ に値をとり、 また確率変数 $Z_1, \\ldots, Z_n$ は独立に $Z$ と同じ分布にしたがうとする。\n",
    "このとき、 任意の $\\varepsilon>0$ に対して\n",
    "\n",
    "$$\n",
    "\\operatorname{Pr}\\left(\\left|\\frac{1}{n} \\sum_{i=1}^n Z_i-\\mathbb{E}[Z]\\right| \\geq \\varepsilon\\right) \\leq 2 e^{-2 n \\varepsilon^2}\n",
    "$$\n",
    "\n",
    "が成り立つ。\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a1cf60-043a-4e12-bed4-14dcc3a387ec",
   "metadata": {},
   "source": [
    "$Z = \\mathbf{1}[h(X)\\neq Y]$として、$2 \\max_{h\\in \\mathcal{H}} | \\hat{R}(h) - R(h) |$ にヘフディングの不等式を用いると\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\operatorname{Pr}\\left(2 \\max _{h \\in \\mathcal{H}}\\left|\\hat{R}(h) - R(h)\\right| \\geq \\varepsilon\\right) \\\\\n",
    "\\leq & \\sum_{h \\in \\mathcal{H}} \\operatorname{Pr}\\left(\\left|\\hat{R}(h) - R(h)\\right| \\geq \\varepsilon / 2\\right) \\\\\n",
    "\\leq & \\sum_{h \\in \\mathcal{H}} 2 e^{-2 n(\\varepsilon / 2)^2}\n",
    "=2|\\mathcal{H}| e^{-n \\varepsilon^2 / 2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となり、$\\delta = 2|\\mathcal{H}| e^{-n \\varepsilon^2 / 2}$とおくと、学習データの分布のもとで確率$1-\\delta$以上で\n",
    "\n",
    "$$\n",
    "2 \\max _{h \\in \\mathcal{H}}\\left|R(h)-\\hat{R}(h)\\right|\n",
    "\\leq \\sqrt{\\frac{2}{n} \\log \\frac{2|\\mathcal{H}|}{\\delta}}\n",
    "$$\n",
    "\n",
    "が成り立つ。よって\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&R(h_S) - R(h_0)\\\\\n",
    "&\\leq 2 \\max_{h\\in \\mathcal{H}} | \\hat{R}(h) - R(h) | + R(h_\\mathcal{H}) - R(h_0)\\\\\n",
    "&\\leq \\sqrt{\\frac{2}{n} \\log \\frac{2|\\mathcal{H}|}{\\delta}} + R(h_\\mathcal{H}) - R(h_0)\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a373d0d6-8ee6-4b0a-8016-09ea1b7aa572",
   "metadata": {},
   "source": [
    "仮説集合$\\mathcal{H}$にベイズ規則$h_0$が含まれる場合、\n",
    "\n",
    "$$\n",
    "R(h_\\mathcal{H}) - R(h_0) = 0\n",
    "$$\n",
    "\n",
    "となるため、\n",
    "\n",
    "$$\n",
    "R(h_S) - R(h_0) \\leq \\sqrt{\\frac{2}{n} \\log \\frac{2|\\mathcal{H}|}{\\delta}}\\\\\n",
    "\\iff \n",
    "R(h_S) \\leq R(h_0) + \\sqrt{\\frac{2}{n} \\log \\frac{2|\\mathcal{H}|}{\\delta}}\n",
    "$$\n",
    "\n",
    "となり、データ数$n$が十分大きければ仮説$h_S$の誤差はベイズ誤差に収束し、その確率オーダーは\n",
    "\n",
    "$$\n",
    "R(h_S) = R(h_0) + O_P\\left( \\sqrt{\\frac{\\log |\\mathcal{H}|}{n} } \\right)\n",
    "$$\n",
    "\n",
    "となる。このオーダーは最悪評価であり、問題設定によってはより速い収束レートが達成できる場合もある。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a019164e-50bd-414f-ae74-5edf22f56d4b",
   "metadata": {},
   "source": [
    ":::{margin}\n",
    "\n",
    "確率変数列 $\\left\\{Z_n\\right\\}_{n \\in \\mathbb{N}}$ の確率オーダーが $O_p\\left(r_n\\right)$ とは $\\lim _{z \\rightarrow \\infty} \\limsup _{n \\rightarrow \\infty} \\operatorname{Pr}\\left(\\frac{|Z_n|}{r_n} >z\\right)=0$ ということ\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c17ce-0890-4a5b-8a53-c8d93bb265f6",
   "metadata": {},
   "source": [
    ":::{card} 関連文献\n",
    "\n",
    "\n",
    "- [Chen, X., & White, H. (1999). Improved rates and asymptotic normality for nonparametric neural network estimators. IEEE Transactions on Information Theory, 45(2), 682-691.](https://www.researchgate.net/profile/Xiaohong-Chen-7/publication/3079737_Improved_Rates_and_Asymptotic_Normality_for_Nonparametric_Neural_Network_Estimators/links/00b49519902e099b2e000000/Improved-Rates-and-Asymptotic-Normality-for-Nonparametric-Neural-Network-Estimators.pdf)\n",
    "- [鈴木大慈. (2021). 深層学習の統計理論. 日本統計学会誌, 50(2), 229-256.](https://www.jstage.jst.go.jp/article/jjssj/50/2/50_229/_pdf)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ead01f7-3892-4116-b49e-01d24339d764",
   "metadata": {},
   "source": [
    "### 近似誤差と推定誤差のトレードオフ\n",
    "\n",
    "一般には仮説集合$\\mathcal{H}$にベイズ規則$h_0$が含まれるとは仮定できず、\n",
    "\n",
    "$$\n",
    "R(h_\\mathcal{H}) - R(h_0) > 0\n",
    "$$\n",
    "\n",
    "となる。近似誤差（approximation error）と推定誤差（estimation error）を次のように定義する\n",
    "\n",
    "- 近似誤差 : $\\rm{bias}_{\\mathcal{H}}=R_{\\mathrm{err}}\\left(h_{\\mathcal{H}}\\right)-R_{\\mathrm{err}}\\left(h_0\\right)$\n",
    "- 推定誤差 : $\\rm{var}_{\\mathcal{H}}=\\sqrt{\\frac{2}{n} \\log \\frac{2|\\mathcal{H}|}{\\delta}}$\n",
    "\n",
    "このとき、\n",
    "\n",
    "$$\n",
    "R(h_S) - R(h_0)\n",
    "\\leq \n",
    "\\rm{bias}_{\\mathcal{H}} + \\rm{var}_{\\mathcal{H}}\n",
    "$$\n",
    "\n",
    "となり、仮説集合を適切に設定することで$h_S$の予測誤差を小さくできる。\n",
    "\n",
    "近似誤差 $\\rm{bias}_{\\mathcal{H}}$と推定誤差$\\rm{var}_{\\mathcal{H}}$の間にはトレードオフの関係がある。複数の有限の仮説集合$\\mathcal{H}_1, \\mathcal{H}_2, \\ldots, \\mathcal{H}_M$の関係が\n",
    "\n",
    "$$\n",
    "\\mathcal{H}_1 \\subset \\mathcal{H}_2 \\subset \\cdots \\subset \\mathcal{H}_M\n",
    "$$\n",
    "\n",
    "を満たすとする。定義より近似誤差と推定誤差は\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\operatorname{bias}_{\\mathcal{H}_1} \\geq \\operatorname{bias}_{\\mathcal{H}_2} \\geq \\cdots \\geq \\operatorname{bias}_{\\mathcal{H}_M} \\\\\n",
    "& \\operatorname{var}_{\\mathcal{H}_1} \\leq \\operatorname{var}_{\\mathcal{H}_2} \\leq \\cdots \\leq \\operatorname{var}_{\\mathcal{H}_M}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となる。仮説集合が大きいほど近似誤差は小さくなるが、推定誤差は大きくなる。\n",
    "なので最適な仮説集合を選ぶには、これらの和を小さくする仮説集合、すなわち\n",
    "\n",
    "$$\n",
    "\\hat{m}=\\underset{m}{\\operatorname{argmin}} \\operatorname{bias}_{\\mathcal{H}_m}+\\operatorname{var}_{\\mathcal{H}_m}\n",
    "$$\n",
    "\n",
    "とするときの$\\mathcal{H}_\\hat{m}$を選べばよい。\n",
    "\n",
    "適切な仮説集合はデータの分布やデータ数などによって変わる。推定誤差は$\\sqrt{\\frac{1}{n}}$が含まれるためデータ数$n$が大きくなれば小さくなる。データ数が少ないときは近似誤差と推定誤差のバランスを考えて仮説集合のサイズを決める必要がある。\n",
    "実用的な方法として正則化法がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e19df4-7554-4ed9-9759-263d04621289",
   "metadata": {},
   "source": [
    "## 正則化\n",
    "\n",
    "**正則化 (regularization)** は適切な大きさの仮説集合を学習するための代表的な方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258fcd99-6a00-40ab-96cb-f4344e7dabf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
