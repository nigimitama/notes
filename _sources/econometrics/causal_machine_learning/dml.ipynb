{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f1d49eb-9627-4dc1-a162-1c460adee592",
   "metadata": {},
   "source": [
    "# Double/Debiased Machine Learning\n",
    "\n",
    "- Paper: [Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., & Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters.](https://academic.oup.com/ectj/article/21/1/C1/5056401?login=false)\n",
    "- Python Package: [DoubleML](https://docs.doubleml.org/stable/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6f2ed0-309a-45a5-9ea9-8d64236641fb",
   "metadata": {},
   "source": [
    "## 概要"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c236da-29af-4b5d-b1b5-865337fa6dcf",
   "metadata": {},
   "source": [
    "### motivation\n",
    "\n",
    "世の中の多くの現象は非線形な関係性が想定される。回帰分析は線形モデルであるため、モデルの定式化の誤りに起因するバイアスが生じかねない。\n",
    "\n",
    "実際に関心のあるパラメータは少なく、交絡のコントロールのために入れている局外母数（nuisance parameters）は高次元になりがち。\n",
    "\n",
    "局外母数を非線形モデルで表し、関心のあるパラメータは線形モデルで表現する部分線形モデル\n",
    "\n",
    "$$\n",
    "Y = \\theta D + g(X) + e\n",
    "$$\n",
    "\n",
    "を作り、非線形モデル$g(X)$を機械学習で構築したい"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4d7c9b-0829-4f39-8d66-9555a95909a2",
   "metadata": {},
   "source": [
    "### 残差回帰\n",
    "\n",
    "DMLでは、FWL定理を利用した残差回帰を一般化する。\n",
    "\n",
    "#### 残差回帰\n",
    "\n",
    "通常の残差回帰は線形回帰モデル\n",
    "\n",
    "$$\n",
    "Y = X_1 \\beta_1 + X_2 \\beta_2 + e\n",
    "$$\n",
    "\n",
    "を用いて\n",
    "\n",
    "$$\n",
    "Y - X_2 \\hat{\\gamma} = (X_1 - X_2 \\hat{\\delta}) \\beta_1\n",
    "$$\n",
    "\n",
    "のようにして関心のあるパラメータ$\\beta_1$を推定する。\n",
    "\n",
    "線形回帰モデルは回帰関数$E[Y|X]$を近似するため、上記の残差回帰は期待値を用いて\n",
    "\n",
    "$$\n",
    "Y - E[Y|X_2] = (X_1 - E[X_1|X_2]) \\beta_1\n",
    "$$\n",
    "\n",
    "と表すことができる。\n",
    "\n",
    "### 部分線形モデルへの拡張\n",
    "\n",
    "部分線形モデル\n",
    "\n",
    "$$\n",
    "Y = \\theta D + g(X) + e\n",
    "$$\n",
    "\n",
    "においても同様に\n",
    "\n",
    "$$\n",
    "Y - E[Y|X] = (D - E[D|X]) \\beta_1\n",
    "$$\n",
    "\n",
    "とするアプローチをとる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37143532-9a0a-4b66-80fe-6f00eb5135de",
   "metadata": {},
   "source": [
    ":::{admonition} FWL定理\n",
    "\n",
    "目的変数のベクトル$Y \\in \\mathbb{R}^{n\\times 1}$、説明変数の行列$X \\in \\mathbb{R}^{n\\times d}$と誤差項$e \\in \\mathbb{R}^{n\\times 1}$による線形回帰モデル\n",
    "\n",
    "$$\n",
    "Y = X \\beta + e\n",
    "$$\n",
    "\n",
    "があるとする。\n",
    "\n",
    "説明変数を$X = (X_1 | X_2)$と2つのグループに分割し、回帰係数ベクトル$\\beta$も合わせて$\\beta = (\\beta_1 | \\beta_2)^T$と2つに分割して\n",
    "\n",
    "$$\n",
    "Y = X_1 \\beta_1 + X_2 \\beta_2 + e\n",
    "$$\n",
    "\n",
    "\n",
    "と表す。\n",
    "\n",
    "この回帰モデルの$\\beta_1$は、**残差回帰**（residual regression）と呼ばれる以下の手順に従うことでも得ることができる。\n",
    "\n",
    "1. $X_1$を$X_2$に回帰して残差$\\tilde{X}_1$を得る：$\\tilde{X}_1 = X_1 - X_2 \\hat{\\delta}$\n",
    "2. $Y$を$X_2$に回帰して残差$\\tilde{Y}$を得る：$\\tilde{Y} = Y - X_2 \\hat{\\gamma}$\n",
    "3. $\\tilde{Y}$を$\\tilde{X}_1$に回帰させる：$\\tilde{Y} = \\tilde{X}_1 \\beta_1$\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6f0cf4-a7c6-475d-92cf-6c9ff16eab0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2302983-7b98-4bff-a08d-1c2008dcc6a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a0c9180-ae63-4e2a-af12-ecc8d8f6678d",
   "metadata": {},
   "source": [
    "良い推定量であるために必要な条件\n",
    "\n",
    "- モーメント条件\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69205f3c-e44d-43e9-ae48-01dd74d85f55",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecf05b85-f8ba-454b-8218-ac00f99e60fd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecde7aaf-87a1-491a-992c-d79171202f18",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d49b4121-c86b-4198-b9a5-ddfd0fe4cee6",
   "metadata": {},
   "source": [
    "## モーメント条件\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62298d13-3cb1-4d05-a70e-ff82497a9c4d",
   "metadata": {},
   "source": [
    "### モーメント条件と非線形モデル\n",
    "\n",
    "GMM推定量は非線形モデルに対しても用いることができる（末石2015, p.78）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad72778c-a493-419c-b2a1-8ee7d5364e5a",
   "metadata": {},
   "source": [
    "## 部分線形モデル\n",
    "\n",
    "結果変数$Y$、説明変数$X, Z$についての次のようなモデルを**部分線形モデル（Partially Linear Model）**という。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Y &= X^T \\beta + g(Z) + u\\\\\n",
    "E[u|X,Z] &= 0\\\\\n",
    "E[u^2|X,Z] &= \\sigma^2(X, Z)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "ここで、$g(\\cdot)$は任意の関数（非線形の関数でもよい）である。$\\sigma^2(\\cdot, \\cdot)$も未知の関数で、不均一分散を許容する。\n",
    "\n",
    "関心のあるパラメータ$\\beta$を解釈性の高い線形モデルで推定（パラメトリック推定）しつつ、影響を統制するためだけにモデルに投じている$Z$は関数形を特定せず推定（ノンパラメトリック推定）することができるため、部分線形モデルはセミパラメトリックモデルと呼ばれる。\n",
    "\n",
    "\n",
    "### Robinson (1983)の推定量\n",
    "\n",
    "$Z$で両辺の条件付き期待値をとった\n",
    "\n",
    "$$\n",
    "E[Y|Z] = E[X|Z]^T \\beta + g(Z)\n",
    "$$\n",
    "\n",
    "を差し引くと\n",
    "\n",
    "$$\n",
    "Y - E[Y|Z] = (X - E[X|Z])^T \\beta + u\n",
    "$$\n",
    "\n",
    "$\\tilde{Y}_i = Y_i - E[Y_i|Z]$、$\\tilde{X}_i = X_i - E[X_i|Z]$とおけば、OLS推定量の形になる\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\mathrm{inf}}=\\left(\\sum_{i=1}^n \\tilde{X}_i \\tilde{X}_i^{\\prime}\\right)^{-1} \\sum_{i=1}^n \\tilde{X}_i \\tilde{Y}_i\n",
    "$$\n",
    "\n",
    "が$E[Y|Z], E[X|Z]$が未知のため実行不可能である。$E[Y|Z], E[X|Z]$をそれぞれのノンパラメトリック推定量で置換して推定を行うことは可能であり、その推定量はroot-N consistentで漸近正規性を持つ\n",
    "\n",
    "\n",
    "### 参考\n",
    "\n",
    "- [末石直也 - セミ・ノンパラメトリック計量分析 (京都大学)](https://sites.google.com/site/naoyasueishij/teaching/nonpara) 第5回 [部分線形モデルとセミパラメトリック推定量の性質](https://drive.google.com/file/d/0B6W_J4QoAI6wcWdwYkNwUU5DWTA/view?resourcekey=0--WtAUb3PgzgBpsw1XtvhzQ)\n",
    "- 西山・人見（2023）『ノン・セミパラメトリック統計解析』、共立出版"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce25d2b-17b7-4ee6-8338-b80c47395c7d",
   "metadata": {},
   "source": [
    "### （疑問）Robinsonの推定量とDMLの違い、そうなった原因は？\n",
    "\n",
    "#### Robinsonの推定量とDMLの違い\n",
    "\n",
    "- 期待値を差し引くというFWL-style or Neiman-orthogonalityを使うのは一緒\n",
    "- モーメント条件が議論の根幹なのでOLS推定量の形には限らない？？\n",
    "- 差し引く期待値の推定をMLにしたのがDML\n",
    "\n",
    "#### 新規性は\n",
    "\n",
    "- ML推定ゆえoverfitting biasが入る → Cross Fitting\n",
    "  - なんでMLだとbiasがはいる？高次元や複雑なモデルはDonsker conditionを満たさないから？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae31e8d1-08cf-47df-99f5-49c12ab9ab1a",
   "metadata": {},
   "source": [
    "## Donsker条件\n",
    "\n",
    "機械学習モデルの収束レートが遅い（Donsker条件）→Cross Fitting\n",
    "\n",
    "[XユーザーのMasahiro Katoさん: 「Double/debiased machine learningはどういう手法かというと，機械学習はDonsker条件を満たさないので，サンプルを分割することによって，異なる部分集合間で独立だと思えるような局外母数の推定量を構築することで，収束率だけで漸近正規性を出す手法です．」 / X](https://twitter.com/masakat0/status/1314897112316870657)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7709ed0c-6194-4ce0-977a-9ec4498f931a",
   "metadata": {},
   "source": [
    "## 考察\n",
    "\n",
    "### 機械学習を使うと常にバイアスが入る？？\n",
    "\n",
    "X-learnerとかどうなる？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d207765e-b350-440f-b8e5-51ed2675ea42",
   "metadata": {},
   "source": [
    "## 参考\n",
    "\n",
    "- 解説記事: [機械学習×計量経済学：Double/Debiased Machine Learning | Web日本評論](https://www.web-nippyo.jp/13331/)\n",
    "- [[勉強会資料メモ] Double/Debiased ML - Speaker Deck](https://speakerdeck.com/masa_asa/debiased-ml?slide=32)\n",
    "- [22 - Debiased/Orthogonal Machine Learning — Causal Inference for the Brave and True](https://matheusfacure.github.io/python-causality-handbook/22-Debiased-Orthogonal-Machine-Learning.html)\n",
    "- [CausalML Book](https://causalml-book.org/)の[Chapter 13](https://causalml-book.org/assets/chapters/CausalML_chap_13.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd937274-6e4d-4aa9-b604-5723d6d782dc",
   "metadata": {},
   "source": [
    "### DMLによるDID\n",
    "\n",
    "[Double/debiased machine learning for difference-in-differences models | The Econometrics Journal | Oxford Academic](https://academic.oup.com/ectj/article/23/2/177/5722119)\n",
    "\n",
    "- 解説: [DMLによる差分の差推定 - Speaker Deck](https://speakerdeck.com/masakat0/dmlniyoruchai-fen-falsechai-tui-ding)\n",
    "- 関連: [Double-Debiased-Machine-learning-estimator-for-Difference-in-Difference-with-Multiple-Periods.pdf](https://www.researchgate.net/profile/Di-Liu-124/publication/370440876_DoubleDebiased_Machine-learning_estimator_for_Difference-in-Difference_with_Multiple_Periods/links/645015ce5762c95ac3676c6e/Double-Debiased-Machine-learning-estimator-for-Difference-in-Difference-with-Multiple-Periods.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af36bf40-5c3c-40bc-bb71-8787981bebab",
   "metadata": {},
   "source": [
    "### 講義動画（Youtube）\n",
    "\n",
    "[Double Machine Learning for Causal and Treatment Effects - YouTube](https://www.youtube.com/watch?v=eHOjmyoPCFU&list=PLru50RuxzKFAsi9x3La3pidYURmi17ci6&index=4&t=479s)\n",
    "\n",
    "- MLでのcausal parametersの推定は良いとは限らない\n",
    "- double or orthogonalized MLとsample splittingによって、causal parametersの高精度な推定が可能\n",
    "\n",
    "Partially Linear Modelを使う\n",
    "\n",
    "$$\n",
    "Y = D \\theta_0 + g_0(Z) + U\n",
    ", \\hspace{1em} E[U|Z, D] = 0\n",
    "$$\n",
    "\n",
    "- MLをそのまま使うと一致推定量にならない（例えば$Y - D$で$g_0(Z)$をRandom Forestで学習しても、予測精度は良いがバイアスがある）\n",
    "- FWL定理を用いて、残差の回帰にするとよい\n",
    "\n",
    "$$\n",
    "\\hat{W} = Y - \\hat{E[Y|Z]}\\\\\n",
    "\\hat{V} = D - \\hat{E[D|Z]}\n",
    "$$\n",
    "\n",
    "\n",
    "モーメント条件\n",
    "\n",
    "1. Regression adjustment: $E[(Y - D \\theta_0 - g_0(Z) ) D] = 0$\n",
    "2. \"propensity score adjustment\": $E[(Y - D \\theta_0) (D - E[D|Z])] = 0$\n",
    "3. Neyman-orthogonal (semi-parametrically efficient under homoscedasticity): $E[(\\hat{W} - \\hat{V}\\theta_0) \\hat{V}] = E[\\{(Y - E[Y|Z]) - (D - E[D|Z])\\theta_0\\} (D - E[D|Z])] = 0$\n",
    "\n",
    "3は不偏\n",
    "\n",
    "\n",
    "Sample Splitting\n",
    "\n",
    "\n",
    "Splittingによるefficiencyの低下問題\n",
    "\n",
    "- 2個に分けて2回やって平均とればfull efficiency → k個に分けての分析をk回やって平均とってもfull efficiency\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d75e8c3-e47b-4837-893b-93e8d4e9b7ea",
   "metadata": {},
   "source": [
    "## 応用研究\n",
    "\n",
    "[[2002.08536] Debiased Off-Policy Evaluation for Recommendation Systems](https://arxiv.org/abs/2002.08536)\n",
    "- PR: [AI Lab、推薦システム分野におけるトップカンファレンス「RecSys2021」にて共著論文採択 ー高次元なデータを使った意思決定評価の信頼性を改善ー | 株式会社サイバーエージェント](https://www.cyberagent.co.jp/news/detail/id=26585)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30000948-1e7d-4308-bb74-224a8e0d9ab1",
   "metadata": {},
   "source": [
    "## 関連研究\n",
    "\n",
    "[[2305.04174] Root-n consistent semiparametric learning with high-dimensional nuisance functions under minimal sparsity](https://arxiv.org/abs/2305.04174)\n",
    "\n",
    "- 先行研究まとめがある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93153934-45f9-4fdf-b028-2c39a8c280ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "472c2f39-fb84-484a-b008-967718d6161e",
   "metadata": {},
   "source": [
    "[Library Flow Chart — econml 0.15.0 documentation](https://econml.azurewebsites.net/spec/flowchart.html)\n",
    "- 派生モデルの使い分けについて"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca31696d-c3b5-4ddd-8267-99a41333bcbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
