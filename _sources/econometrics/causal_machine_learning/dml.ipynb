{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f1d49eb-9627-4dc1-a162-1c460adee592",
   "metadata": {},
   "source": [
    "# Double/Debiased Machine Learning\n",
    "\n",
    "- Paper: [Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., & Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters.](https://academic.oup.com/ectj/article/21/1/C1/5056401?login=false)\n",
    "- Python Package: [DoubleML](https://docs.doubleml.org/stable/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c236da-29af-4b5d-b1b5-865337fa6dcf",
   "metadata": {},
   "source": [
    "世の中の多くの現象は非線形な関係性が想定される。回帰分析は線形モデルであるため、モデルの定式化の誤りに起因するバイアスが生じかねない。\n",
    "\n",
    "実際に関心のあるパラメータは少なく、交絡のコントロールのために入れている局外母数（nuisance parameters）は高次元になりがち。\n",
    "\n",
    "局外母数を非線形モデルで表し、関心のあるパラメータは線形モデルで表現する部分線形モデル\n",
    "\n",
    "$$\n",
    "Y = \\theta D + g(X) + e\n",
    "$$\n",
    "\n",
    "を作り、非線形モデル$g(X)$を機械学習で行いたい"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49b4121-c86b-4198-b9a5-ddfd0fe4cee6",
   "metadata": {},
   "source": [
    "## モーメント条件\n",
    "\n",
    "### 外生性\n",
    "\n",
    "単回帰モデル$Y = \\alpha + \\beta X + u$を例にとる。\n",
    "\n",
    "\n",
    ":::{admonition} 外生性\n",
    "\n",
    "説明変数$X$と誤差項$u$が\n",
    "\n",
    "$$\n",
    "E(u | X) = 0\n",
    "$$\n",
    "\n",
    "を満たすとき、$X$は外生変数であるという。\n",
    "\n",
    ":::\n",
    "\n",
    "また、外生性の条件は別の表現もできる\n",
    "\n",
    ":::{admonition} $X$と$u$の直交\n",
    "説明変数$X$が外生変数ならば、\n",
    "\n",
    "$$\n",
    "E(u) = 0, E(X, u) = 0\n",
    "$$\n",
    ":::\n",
    "\n",
    ":::{dropdown} 証明\n",
    "$$\n",
    "E(X u) = E_{X}[ E(X u | X) ] = E_{X}[ X \\underbrace{ E(u|X) }_{ =0 } ] = E_{X}(X\\cdot 0) = 0\n",
    "$$\n",
    "\n",
    "- TODO: E(u) = 0の証明\n",
    "\n",
    "上の式を直交条件と呼ぶ。\n",
    ":::\n",
    "\n",
    "さらに、共分散との関係も導出できる\n",
    "\n",
    ":::{admonition} $X$と$u$の無相関\n",
    "説明変数$X$が外生変数ならば、\n",
    "\n",
    "$$\n",
    "Cov(X, u) = 0\n",
    "$$\n",
    ":::\n",
    "\n",
    ":::{dropdown} 証明\n",
    "\n",
    "$$\n",
    "Cov(X, u) = \\underbrace{ E(X u) }_{ =0 }\n",
    "- E(X) \\underbrace{ E(u) }_{ =0 }\n",
    "= 0\n",
    "$$\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd16833f-6063-42fd-9e63-20f6cd4121f3",
   "metadata": {},
   "source": [
    "### OLS\n",
    "\n",
    "単回帰モデル$Y = \\alpha + \\beta X + u$のOLS推定量$\\beta$の確率極限は\n",
    "\n",
    "$$\n",
    "\\DeclareMathOperator{\\cov}{\\text{Cov}}\n",
    "\\DeclareMathOperator{\\var}{\\text{Var}}\n",
    "\\DeclareMathOperator{\\plim}{\\text{plim}}\n",
    "\\plim \\beta = \\beta + \\frac{\\cov(X, u)}{\\var(X)}\n",
    "$$\n",
    "\n",
    "となる。外生性が満たされるとき$\\cov(X, u) = 0$であるため、$\\plim \\beta = \\beta$となり、OLS推定量は母回帰係数の不偏推定量となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95631763-5f4f-49e5-b244-d4dc3f559f14",
   "metadata": {},
   "source": [
    "::::{dropdown} 証明\n",
    "$$\n",
    "\\DeclareMathOperator{\\cov}{\\text{Cov}}\n",
    "\\DeclareMathOperator{\\var}{\\text{Var}}\n",
    "\\DeclareMathOperator{\\plim}{\\text{plim}}\n",
    "\\begin{align}\n",
    "\\plim \\beta\n",
    "&= \\frac{\\cov(X, Y)}{\\var(X)}\\\\\n",
    "&= \\frac{\\cov(X, \\alpha + \\beta X + u)}{\\var(X)}\\\\\n",
    "&= \\frac{\\cov(X, \\alpha) + \\cov(X, \\beta X) + \\cov(X, u) }{\\var(X)}\\\\\n",
    "&= \\frac{\\beta \\var(X) + \\cov(X, u) }{\\var(X)}\\\\\n",
    "&= \\beta + \\frac{\\cov(X, u)}{\\var(X)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    ":::{admonition}（参考）和の共分散\n",
    "標本値、確率変数の和は、加える前の個々の共分散の和になる。すなわち、共分散においては分配法則が成り立つ。\n",
    "\n",
    "$$\n",
    "\\cov(X + Z, Y) = \\cov(X, Y) + \\cov(Z, Y)\n",
    "$$\n",
    "\n",
    "参考：[確率統計 – 分散と共分散 – TauStation](http://taustation.com/statistics-variance-and-covariance/)\n",
    ":::\n",
    "\n",
    ":::{admonition} （参考）共分散と定数\n",
    "\n",
    "$\\alpha, \\beta$は定数と考えると、\n",
    "\n",
    "$$\n",
    "\\cov(X, \\alpha)\n",
    "= E(X \\alpha) - E(X) E(\\alpha)\\\\\n",
    "= \\alpha E(X) - \\alpha E(X)\\\\\n",
    "= 0\n",
    "$$\n",
    "\n",
    "であり\n",
    "\n",
    "$$\n",
    "\\cov(X, \\beta X)\n",
    "= E(X \\beta X) - E(X) E(\\beta X)\\\\\n",
    "= \\beta E(X^2) - \\beta E(X)^2\\\\\n",
    "= \\beta \\var(X)\n",
    "$$\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62298d13-3cb1-4d05-a70e-ff82497a9c4d",
   "metadata": {},
   "source": [
    "### モーメント条件と非線形モデル\n",
    "\n",
    "GMM推定量は非線形モデルに対しても用いることができる（末石2015, p.78）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad72778c-a493-419c-b2a1-8ee7d5364e5a",
   "metadata": {},
   "source": [
    "## 部分線形モデル\n",
    "\n",
    "結果変数$Y$、説明変数$X, Z$についての次のようなモデルを**部分線形モデル（Partially Linear Model）**という。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Y &= X^T \\beta + g(Z) + u\\\\\n",
    "E[u|X,Z] &= 0\\\\\n",
    "E[u^2|X,Z] &= \\sigma^2(X, Z)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "ここで、$g(\\cdot)$は任意の関数（非線形の関数でもよい）である。$\\sigma^2(\\cdot, \\cdot)$も未知の関数で、不均一分散を許容する。\n",
    "\n",
    "関心のあるパラメータ$\\beta$を解釈性の高い線形モデルで推定（パラメトリック推定）しつつ、影響を統制するためだけにモデルに投じている$Z$は関数形を特定せず推定（ノンパラメトリック推定）することができるため、部分線形モデルはセミパラメトリックモデルと呼ばれる。\n",
    "\n",
    "\n",
    "### Robinson (1983)の推定量\n",
    "\n",
    "$Z$で両辺の条件付き期待値をとった\n",
    "\n",
    "$$\n",
    "E[Y|Z] = E[X|Z]^T \\beta + g(Z)\n",
    "$$\n",
    "\n",
    "を差し引くと\n",
    "\n",
    "$$\n",
    "Y - E[Y|Z] = (X - E[X|Z])^T \\beta + u\n",
    "$$\n",
    "\n",
    "$\\tilde{Y}_i = Y_i - E[Y_i|Z]$、$\\tilde{X}_i = X_i - E[X_i|Z]$とおけば、OLS推定量の形になる\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\mathrm{inf}}=\\left(\\sum_{i=1}^n \\tilde{X}_i \\tilde{X}_i^{\\prime}\\right)^{-1} \\sum_{i=1}^n \\tilde{X}_i \\tilde{Y}_i\n",
    "$$\n",
    "\n",
    "が$E[Y|Z], E[X|Z]$が未知のため実行不可能である。$E[Y|Z], E[X|Z]$をそれぞれのノンパラメトリック推定量で置換して推定を行うことは可能であり、その推定量はroot-N consistentで漸近正規性を持つ\n",
    "\n",
    "\n",
    "### 参考\n",
    "\n",
    "- [末石直也 - セミ・ノンパラメトリック計量分析 (京都大学)](https://sites.google.com/site/naoyasueishij/teaching/nonpara) 第5回 [部分線形モデルとセミパラメトリック推定量の性質](https://drive.google.com/file/d/0B6W_J4QoAI6wcWdwYkNwUU5DWTA/view?resourcekey=0--WtAUb3PgzgBpsw1XtvhzQ)\n",
    "- 西山・人見（2023）『ノン・セミパラメトリック統計解析』、共立出版"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce25d2b-17b7-4ee6-8338-b80c47395c7d",
   "metadata": {},
   "source": [
    "### （疑問）Robinsonの推定量とDMLの違い、そうなった原因は？\n",
    "\n",
    "#### Robinsonの推定量とDMLの違い\n",
    "\n",
    "- 期待値を差し引くというFWL-style or Neiman-orthogonalityを使うのは一緒\n",
    "- モーメント条件が議論の根幹なのでOLS推定量の形には限らない？？\n",
    "- 差し引く期待値の推定をMLにしたのがDML\n",
    "\n",
    "#### 新規性は\n",
    "\n",
    "- ML推定ゆえoverfitting biasが入る → Cross Fitting\n",
    "  - なんでMLだとbiasがはいる？\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae31e8d1-08cf-47df-99f5-49c12ab9ab1a",
   "metadata": {},
   "source": [
    "## Donsker条件\n",
    "\n",
    "機械学習モデルの収束レートが遅い（Donsker条件）→Cross Fitting\n",
    "\n",
    "[XユーザーのMasahiro Katoさん: 「Double/debiased machine learningはどういう手法かというと，機械学習はDonsker条件を満たさないので，サンプルを分割することによって，異なる部分集合間で独立だと思えるような局外母数の推定量を構築することで，収束率だけで漸近正規性を出す手法です．」 / X](https://twitter.com/masakat0/status/1314897112316870657)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7344f7f1-b211-47de-918d-ff3d19b198b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7709ed0c-6194-4ce0-977a-9ec4498f931a",
   "metadata": {},
   "source": [
    "## 考察\n",
    "\n",
    "### 機械学習を使うと常にバイアスが入る？？\n",
    "\n",
    "X-learnerとかどうなる？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d207765e-b350-440f-b8e5-51ed2675ea42",
   "metadata": {},
   "source": [
    "## 参考\n",
    "\n",
    "- 解説記事: [機械学習×計量経済学：Double/Debiased Machine Learning | Web日本評論](https://www.web-nippyo.jp/13331/)\n",
    "- [[勉強会資料メモ] Double/Debiased ML - Speaker Deck](https://speakerdeck.com/masa_asa/debiased-ml?slide=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd937274-6e4d-4aa9-b604-5723d6d782dc",
   "metadata": {},
   "source": [
    "### DMLによるDID\n",
    "\n",
    "[Double/debiased machine learning for difference-in-differences models | The Econometrics Journal | Oxford Academic](https://academic.oup.com/ectj/article/23/2/177/5722119)\n",
    "\n",
    "- 解説: [DMLによる差分の差推定 - Speaker Deck](https://speakerdeck.com/masakat0/dmlniyoruchai-fen-falsechai-tui-ding)\n",
    "- 関連: [Double-Debiased-Machine-learning-estimator-for-Difference-in-Difference-with-Multiple-Periods.pdf](https://www.researchgate.net/profile/Di-Liu-124/publication/370440876_DoubleDebiased_Machine-learning_estimator_for_Difference-in-Difference_with_Multiple_Periods/links/645015ce5762c95ac3676c6e/Double-Debiased-Machine-learning-estimator-for-Difference-in-Difference-with-Multiple-Periods.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af36bf40-5c3c-40bc-bb71-8787981bebab",
   "metadata": {},
   "source": [
    "### 講義動画（Youtube）\n",
    "\n",
    "[Double Machine Learning for Causal and Treatment Effects - YouTube](https://www.youtube.com/watch?v=eHOjmyoPCFU&list=PLru50RuxzKFAsi9x3La3pidYURmi17ci6&index=4&t=479s)\n",
    "\n",
    "- MLでのcausal parametersの推定は良いとは限らない\n",
    "- double or orthogonalized MLとsample splittingによって、causal parametersの高精度な推定が可能\n",
    "\n",
    "Partially Linear Modelを使う\n",
    "\n",
    "$$\n",
    "Y = D \\theta_0 + g_0(Z) + U\n",
    ", \\hspace{1em} E[U|Z, D] = 0\n",
    "$$\n",
    "\n",
    "- MLをそのまま使うと一致推定量にならない（例えば$Y - D$で$g_0(Z)$をRandom Forestで学習しても、予測精度は良いがバイアスがある）\n",
    "- FWL定理を用いて、残差の回帰にするとよい\n",
    "\n",
    "$$\n",
    "\\hat{W} = Y - \\hat{E[Y|Z]}\\\\\n",
    "\\hat{V} = D - \\hat{E[D|Z]}\n",
    "$$\n",
    "\n",
    "\n",
    "モーメント条件\n",
    "\n",
    "1. Regression adjustment: $E[(Y - D \\theta_0 - g_0(Z) ) D] = 0$\n",
    "2. \"propensity score adjustment\": $E[(Y - D \\theta_0) (D - E[D|Z])] = 0$\n",
    "3. Neyman-orthogonal (semi-parametrically efficient under homoscedasticity): $E[(\\hat{W} - \\hat{V}\\theta_0) \\hat{V}] = E[\\{(Y - E[Y|Z]) - (D - E[D|Z])\\theta_0\\} (D - E[D|Z])] = 0$\n",
    "\n",
    "3は不偏\n",
    "\n",
    "\n",
    "Sample Splitting\n",
    "\n",
    "\n",
    "Splittingによるefficiencyの低下問題\n",
    "\n",
    "- 2個に分けて2回やって平均とればfull efficiency → k個に分けての分析をk回やって平均とってもfull efficiency\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d75e8c3-e47b-4837-893b-93e8d4e9b7ea",
   "metadata": {},
   "source": [
    "## 応用研究\n",
    "\n",
    "[[2002.08536] Debiased Off-Policy Evaluation for Recommendation Systems](https://arxiv.org/abs/2002.08536)\n",
    "- PR: [AI Lab、推薦システム分野におけるトップカンファレンス「RecSys2021」にて共著論文採択 ー高次元なデータを使った意思決定評価の信頼性を改善ー | 株式会社サイバーエージェント](https://www.cyberagent.co.jp/news/detail/id=26585)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30000948-1e7d-4308-bb74-224a8e0d9ab1",
   "metadata": {},
   "source": [
    "## 関連研究\n",
    "\n",
    "[[2305.04174] Root-n consistent semiparametric learning with high-dimensional nuisance functions under minimal sparsity](https://arxiv.org/abs/2305.04174)\n",
    "\n",
    "- 先行研究まとめがある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93153934-45f9-4fdf-b028-2c39a8c280ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "472c2f39-fb84-484a-b008-967718d6161e",
   "metadata": {},
   "source": [
    "[Library Flow Chart — econml 0.15.0 documentation](https://econml.azurewebsites.net/spec/flowchart.html)\n",
    "- 派生モデルの使い分けについて"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca31696d-c3b5-4ddd-8267-99a41333bcbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
