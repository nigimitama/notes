{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a9c00c8-8a82-4efe-ad3d-2db4d0663e7c",
   "metadata": {},
   "source": [
    "# NOTEARS\n",
    "\n",
    "NOTEARS（Zheng et al., 2018） は、**因果探索を“勾配最適化”として解く**ことに成功した画期的な手法であり、伝統的な PC（制約ベース）や GES（スコアベース）とは異なる第3のアプローチとして注目されている。\n",
    "\n",
    ":::{margin}\n",
    "\n",
    "NOTEARS は Non-combinatorial Optimization via Trace Exponential and Augmented lagRangian for Structure learning の略\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99498e22-d0f7-4659-8d2a-d0e151af733e",
   "metadata": {},
   "source": [
    ":::{card} 論文\n",
    "\n",
    "[[1803.01422] DAGs with NO TEARS: Continuous Optimization for Structure Learning](https://arxiv.org/abs/1803.01422) (NeurIPS 2018)\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096edbfc-b1be-4099-bac0-d94776ad1c8c",
   "metadata": {},
   "source": [
    "従来の因果探索は\n",
    "\n",
    "- PC：条件付き独立性検定  \n",
    "- GES：BIC などのスコアによる組合せ探索  \n",
    "\n",
    "のように **離散最適化** をベースにして因果探索を行う。\n",
    "\n",
    "NOTEARS はこれを捨て、\n",
    "\n",
    "> **“DAG である”という制約そのものを連続最適化できるようにした。**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441f74fc-6911-489f-a0b7-a947f82df384",
   "metadata": {},
   "source": [
    "## モデル：線形 SEM\n",
    "\n",
    "NOTEARS は、次の線形構造方程式モデル（SEM）を仮定する\n",
    "\n",
    "$$\n",
    "X = WX + Z,\\quad Z\\sim \\mathcal{N}(0, \\sigma^2 I)\n",
    "$$\n",
    "\n",
    "- $W$ の非ゼロ要素 $W_{ij}$ がエッジ $j \\rightarrow i$ の因果効果を表す  \n",
    "- $W$ を推定することが、因果グラフを推定することに相当する\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09598d90-854b-4b63-b218-cd8440c2df2d",
   "metadata": {},
   "source": [
    "## DAGを連続最適化問題へ転換\n",
    "\n",
    "DAG とはサイクルのない有向グラフであるが、サイクル存在の有無は離散的であり、最適化には扱いにくい。\n",
    "\n",
    "NOTEARS はこれを **微分可能な制約**として次のように表す。\n",
    "\n",
    "$$\n",
    "h(W) = \\mathrm{tr}(e^{W \\circ W}) - d = 0\n",
    "$$\n",
    "\n",
    "- $e^{A}$ は行列指数  \n",
    "- $W\\circ W$ は アダマール積（要素ごとの積）\n",
    "- $h(W)=0$ が **W が DAG を表すための必要十分条件**となる\n",
    "\n",
    "この $h(W)$ は滑らかであり、微分可能であるため、**通常の連続最適化（L-BFGS, Adam 等）で扱える**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cfd656-f80c-4066-ad74-5919e08f2713",
   "metadata": {},
   "source": [
    "## 最適化\n",
    "\n",
    "最適化するパラメータは $W$ であり、目的関数は次のように与えられる。\n",
    "\n",
    "$$\n",
    "\\min_{W}\n",
    "\\frac{1}{2n}\\|X - WX\\|_F^2 + \\lambda \\|W\\|_1\n",
    "\\quad\\text{s.t. } h(W)=0\n",
    "$$\n",
    "\n",
    "- 最初の項は **再構成誤差（平方誤差）**  \n",
    "- $\\lambda \\|W\\|_1$ は **スパース化**のための L1 正則化  \n",
    "- $h(W)=0$ が **DAG 制約**\n",
    "\n",
    "したがって NOTEARS は\n",
    "\n",
    "> **「誤差を最小化しつつ DAG を保つ $W$」**\n",
    "\n",
    "を連続最適化の枠組みで求める手法である。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496976df-ad32-4d95-a4b7-159d9d9e1911",
   "metadata": {},
   "source": [
    "## 参考\n",
    "\n",
    "- [Zheng, X., Aragam, B., Ravikumar, P. K., & Xing, E. P. (2018). Dags with no tears: Continuous optimization for structure learning. Advances in neural information processing systems, 31.](https://arxiv.org/abs/1803.01422)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4120163-c224-4505-9e8c-1deebba404ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
