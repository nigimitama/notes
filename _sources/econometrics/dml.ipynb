{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f1d49eb-9627-4dc1-a162-1c460adee592",
   "metadata": {},
   "source": [
    "# Double/Debiased Machine Learning\n",
    "\n",
    "- Paper: [Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., & Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters.](https://academic.oup.com/ectj/article/21/1/C1/5056401?login=false)\n",
    "- Python Package: [DoubleML](https://docs.doubleml.org/stable/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c236da-29af-4b5d-b1b5-865337fa6dcf",
   "metadata": {},
   "source": [
    "世の中の多くの現象は非線形な関係性が想定される。回帰分析は線形モデルであるため、モデルの定式化の誤りに起因するバイアスが生じかねない。\n",
    "\n",
    "実際に関心のあるパラメータは少なく、交絡のコントロールのために入れている局外母数（nuisance parameters）は高次元になりがち。\n",
    "\n",
    "局外母数を非線形モデルで表し、関心のあるパラメータは線形モデルで表現する部分線形モデル\n",
    "\n",
    "$$\n",
    "Y = \\theta D + g(X) + e\n",
    "$$\n",
    "\n",
    "を作り、非線形モデル$g(X)$を機械学習で行いたい"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49b4121-c86b-4198-b9a5-ddfd0fe4cee6",
   "metadata": {},
   "source": [
    "## モーメント条件\n",
    "\n",
    "### 外生性\n",
    "\n",
    "単回帰モデル$Y = \\alpha + \\beta X + u$を例にとる。\n",
    "\n",
    "\n",
    ":::{admonition} 外生性\n",
    "\n",
    "説明変数$X$と誤差項$u$が\n",
    "\n",
    "$$\n",
    "E(u | X) = 0\n",
    "$$\n",
    "\n",
    "を満たすとき、$X$は外生変数であるという。\n",
    "\n",
    ":::\n",
    "\n",
    "また、外生性の条件は別の表現もできる\n",
    "\n",
    ":::{admonition} $X$と$u$の直交\n",
    "説明変数$X$が外生変数ならば、\n",
    "\n",
    "$$\n",
    "E(u) = 0, E(X, u) = 0\n",
    "$$\n",
    ":::\n",
    "\n",
    ":::{dropdown} 証明\n",
    "$$\n",
    "E(X u) = E_{X}[ E(X u | X) ] = E_{X}[ X \\underbrace{ E(u|X) }_{ =0 } ] = E_{X}(X\\cdot 0) = 0\n",
    "$$\n",
    "\n",
    "- TODO: E(u) = 0の証明\n",
    "\n",
    "上の式を直交条件と呼ぶ。\n",
    ":::\n",
    "\n",
    "さらに、共分散との関係も導出できる\n",
    "\n",
    ":::{admonition} $X$と$u$の無相関\n",
    "説明変数$X$が外生変数ならば、\n",
    "\n",
    "$$\n",
    "Cov(X, u) = 0\n",
    "$$\n",
    ":::\n",
    "\n",
    ":::{dropdown} 証明\n",
    "\n",
    "$$\n",
    "Cov(X, u) = \\underbrace{ E(X u) }_{ =0 }\n",
    "- E(X) \\underbrace{ E(u) }_{ =0 }\n",
    "= 0\n",
    "$$\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd16833f-6063-42fd-9e63-20f6cd4121f3",
   "metadata": {},
   "source": [
    "### OLS\n",
    "\n",
    "単回帰モデル$Y = \\alpha + \\beta X + u$のOLS推定量$\\beta$の確率極限は\n",
    "\n",
    "$$\n",
    "\\DeclareMathOperator{\\cov}{\\text{Cov}}\n",
    "\\DeclareMathOperator{\\var}{\\text{Var}}\n",
    "\\DeclareMathOperator{\\plim}{\\text{plim}}\n",
    "\\plim \\beta = \\beta + \\frac{\\cov(X, u)}{\\var(X)}\n",
    "$$\n",
    "\n",
    "となる。外生性が満たされるとき$\\cov(X, u) = 0$であるため、$\\plim \\beta = \\beta$となり、OLS推定量は母回帰係数の不偏推定量となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95631763-5f4f-49e5-b244-d4dc3f559f14",
   "metadata": {},
   "source": [
    "::::{dropdown} 証明\n",
    "$$\n",
    "\\DeclareMathOperator{\\cov}{\\text{Cov}}\n",
    "\\DeclareMathOperator{\\var}{\\text{Var}}\n",
    "\\DeclareMathOperator{\\plim}{\\text{plim}}\n",
    "\\begin{align}\n",
    "\\plim \\beta\n",
    "&= \\frac{\\cov(X, Y)}{\\var(X)}\\\\\n",
    "&= \\frac{\\cov(X, \\alpha + \\beta X + u)}{\\var(X)}\\\\\n",
    "&= \\frac{\\cov(X, \\alpha) + \\cov(X, \\beta X) + \\cov(X, u) }{\\var(X)}\\\\\n",
    "&= \\frac{\\beta \\var(X) + \\cov(X, u) }{\\var(X)}\\\\\n",
    "&= \\beta + \\frac{\\cov(X, u)}{\\var(X)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    ":::{admonition}（参考）和の共分散\n",
    "標本値、確率変数の和は、加える前の個々の共分散の和になる。すなわち、共分散においては分配法則が成り立つ。\n",
    "\n",
    "$$\n",
    "\\cov(X + Z, Y) = \\cov(X, Y) + \\cov(Z, Y)\n",
    "$$\n",
    "\n",
    "参考：[確率統計 – 分散と共分散 – TauStation](http://taustation.com/statistics-variance-and-covariance/)\n",
    ":::\n",
    "\n",
    ":::{admonition} （参考）共分散と定数\n",
    "\n",
    "$\\alpha, \\beta$は定数と考えると、\n",
    "\n",
    "$$\n",
    "\\cov(X, \\alpha)\n",
    "= E(X \\alpha) - E(X) E(\\alpha)\\\\\n",
    "= \\alpha E(X) - \\alpha E(X)\\\\\n",
    "= 0\n",
    "$$\n",
    "\n",
    "であり\n",
    "\n",
    "$$\n",
    "\\cov(X, \\beta X)\n",
    "= E(X \\beta X) - E(X) E(\\beta X)\\\\\n",
    "= \\beta E(X^2) - \\beta E(X)^2\\\\\n",
    "= \\beta \\var(X)\n",
    "$$\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62298d13-3cb1-4d05-a70e-ff82497a9c4d",
   "metadata": {},
   "source": [
    "### モーメント条件と非線形モデル\n",
    "\n",
    "GMM推定量は非線形モデルに対しても用いることができる（末石2015, p.78）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad72778c-a493-419c-b2a1-8ee7d5364e5a",
   "metadata": {},
   "source": [
    "## 部分線形モデル\n",
    "\n",
    "結果変数$Y$、説明変数$X, Z$についての次のようなモデルを**部分線形モデル（Partially Linear Model）**という。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Y &= X^T \\beta + g(Z) + u\\\\\n",
    "E[u|X,Z] &= 0\\\\\n",
    "E[u^2|X,Z] &= \\sigma^2(X, Z)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "ここで、$g(\\cdot)$は任意の関数（非線形の関数でもよい）である。$\\sigma^2(\\cdot, \\cdot)$も未知の関数で、不均一分散を許容する。\n",
    "\n",
    "関心のあるパラメータ$\\beta$を解釈性の高い線形モデルで推定（パラメトリック推定）しつつ、影響を統制するためだけにモデルに投じている$Z$は関数形を特定せず推定（ノンパラメトリック推定）することができるため、部分線形モデルはセミパラメトリックモデルと呼ばれる。\n",
    "\n",
    "\n",
    "\n",
    "### 参考\n",
    "\n",
    "- [末石直也 - セミ・ノンパラメトリック計量分析 (京都大学)](https://sites.google.com/site/naoyasueishij/teaching/nonpara) 第5回 [部分線形モデルとセミパラメトリック推定量の性質](https://drive.google.com/file/d/0B6W_J4QoAI6wcWdwYkNwUU5DWTA/view?resourcekey=0--WtAUb3PgzgBpsw1XtvhzQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae31e8d1-08cf-47df-99f5-49c12ab9ab1a",
   "metadata": {},
   "source": [
    "## Donsker条件\n",
    "\n",
    "機械学習モデルの収束レートが遅い（Donsker条件）→Cross Fitting\n",
    "\n",
    "[XユーザーのMasahiro Katoさん: 「Double/debiased machine learningはどういう手法かというと，機械学習はDonsker条件を満たさないので，サンプルを分割することによって，異なる部分集合間で独立だと思えるような局外母数の推定量を構築することで，収束率だけで漸近正規性を出す手法です．」 / X](https://twitter.com/masakat0/status/1314897112316870657)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7344f7f1-b211-47de-918d-ff3d19b198b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b6c768c-56e1-48ef-bb4a-e91bdfa48ec0",
   "metadata": {},
   "source": [
    "## 応用研究\n",
    "\n",
    "- CAのやつ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d207765e-b350-440f-b8e5-51ed2675ea42",
   "metadata": {},
   "source": [
    "## 参考\n",
    "\n",
    "- 解説記事: [機械学習×計量経済学：Double/Debiased Machine Learning | Web日本評論](https://www.web-nippyo.jp/13331/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd937274-6e4d-4aa9-b604-5723d6d782dc",
   "metadata": {},
   "source": [
    "### DMLによるDID\n",
    "\n",
    "[Double/debiased machine learning for difference-in-differences models | The Econometrics Journal | Oxford Academic](https://academic.oup.com/ectj/article/23/2/177/5722119)\n",
    "\n",
    "- 解説: [DMLによる差分の差推定 - Speaker Deck](https://speakerdeck.com/masakat0/dmlniyoruchai-fen-falsechai-tui-ding)\n",
    "- 関連: [Double-Debiased-Machine-learning-estimator-for-Difference-in-Difference-with-Multiple-Periods.pdf](https://www.researchgate.net/profile/Di-Liu-124/publication/370440876_DoubleDebiased_Machine-learning_estimator_for_Difference-in-Difference_with_Multiple_Periods/links/645015ce5762c95ac3676c6e/Double-Debiased-Machine-learning-estimator-for-Difference-in-Difference-with-Multiple-Periods.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af36bf40-5c3c-40bc-bb71-8787981bebab",
   "metadata": {},
   "source": [
    "### 講義動画（Youtube）\n",
    "\n",
    "[Double Machine Learning for Causal and Treatment Effects - YouTube](https://www.youtube.com/watch?v=eHOjmyoPCFU&list=PLru50RuxzKFAsi9x3La3pidYURmi17ci6&index=4&t=479s)\n",
    "\n",
    "- MLでのcausal parametersの推定は良いとは限らない\n",
    "- double or orthogonalized MLとsample splittingによって、causal parametersの高精度な推定が可能\n",
    "\n",
    "Partially Linear Modelを使う\n",
    "\n",
    "$$\n",
    "Y = D \\theta_0 + g_0(Z) + U\n",
    ", \\hspace{1em} E[U|Z, D] = 0\n",
    "$$\n",
    "\n",
    "- MLをそのまま使うと一致推定量にならない（例えば$Y - D$で$g_0(Z)$をRandom Forestで学習しても、予測精度は良いがバイアスがある）\n",
    "- FWL定理を用いて、残差の回帰にするとよい\n",
    "\n",
    "$$\n",
    "\\hat{W} = Y - \\hat{E[Y|Z]}\\\\\n",
    "\\hat{V} = D - \\hat{E[D|Z]}\n",
    "$$\n",
    "\n",
    "\n",
    "モーメント条件\n",
    "\n",
    "1. Regression adjustment: $E[(Y - D \\theta_0 - g_0(Z) ) D] = 0$\n",
    "2. \"propensity score adjustment\": $E[(Y - D \\theta_0) (D - E[D|Z])] = 0$\n",
    "3. Neyman-orthogonal (semi-parametrically efficient under homoscedasticity): $E[(\\hat{W} - \\hat{V}\\theta_0) \\hat{V}] = E[\\{(Y - E[Y|Z]) - (D - E[D|Z])\\theta_0\\} (D - E[D|Z])] = 0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3365b9c1-090d-49ff-84f5-ceba4ec97f21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
