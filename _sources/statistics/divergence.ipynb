{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "287bd0a2-bab1-420f-ae79-81c3619b8001",
   "metadata": {},
   "source": [
    "# ダイバージェンス\n",
    "\n",
    "情報理論において **ダイバージェンス（divergence）** とは、2つの確率分布がどれだけ異なっているかを数量化する指標"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d451e1-2be7-4da2-8d75-90dd702a6780",
   "metadata": {},
   "source": [
    "## エントロピー\n",
    "\n",
    "### 自己情報量\n",
    "\n",
    "情報理論において、ある事象$x$が起こる確率$P(x)$が小さい（めったに起こらない事象）ほど「情報量が多い」と考える。\n",
    "\n",
    "そこで、確率の逆数の対数を surpriseあるいは **自己情報量（self-information）** という。\n",
    "\n",
    ":::{card} Self-Information\n",
    "\n",
    "$$\n",
    "I(x) = \\log \\frac{1}{P(x)} = - \\log P(x)\n",
    "$$\n",
    "\n",
    ":::\n",
    "\n",
    "（対数を取ることで独立な事象の同時確率を確率の積として計算するときに計算しやすい）\n",
    "\n",
    "### エントロピー\n",
    "\n",
    "自己情報量の期待値を **エントロピー（entropy）** 、 **期待情報量** 、 **シャノン情報量** などと呼ぶ。\n",
    "\n",
    ":::{card} Entropy\n",
    "\n",
    "離散型確率変数の場合：\n",
    "\n",
    "$$\n",
    "H(P) = \\sum_i P(x_i) \\log \\frac{1}{P(x_i)}\n",
    "$$\n",
    "\n",
    "\n",
    "連続型確率変数の場合：\n",
    "\n",
    "$$\n",
    "H(P) = \\int_{-\\infty}^{\\infty} P(x) \\log \\frac{1}{P(x)} dx\n",
    "$$\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d7c2b7-3b03-4bb4-a99c-3e262ac0df25",
   "metadata": {},
   "source": [
    "### 交差エントロピー\n",
    "\n",
    ":::{card} Cross-Entropy\n",
    "\n",
    "離散型確率変数の場合：\n",
    "\n",
    "$$\n",
    "H(P, Q) = \\sum_i P(x_i) \\log \\frac{1}{Q(x_i)}\n",
    "$$\n",
    "\n",
    "\n",
    "連続型確率変数の場合：\n",
    "\n",
    "$$\n",
    "H(P, Q) = \\int_{-\\infty}^{\\infty}  P(x) \\log \\frac{1}{P(x)}\n",
    "$$\n",
    "\n",
    ":::\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c53c389-923f-4eb8-a0c1-1dee54f65a0e",
   "metadata": {},
   "source": [
    "\n",
    ":::{margin}\n",
    "\n",
    "$\\log \\frac{1}{P(x)} = - \\log P(x)$なので書籍によっては\n",
    "\n",
    "$$\n",
    "H(p,q) = - \\int_{-\\infty}^{\\infty}  P(x) \\log Q(x) dx\n",
    "$$\n",
    "\n",
    "という定義で表されるが、同じものである\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e78a56-5ff7-4184-9291-d286f4fc8d16",
   "metadata": {},
   "source": [
    "## KLダイバージェンス\n",
    "\n",
    "**KLダイバージェンス（Kullback-Leibler Divergance）** は 2つの確率分布の間の距離を測る関数。\n",
    "\n",
    ":::{admonition} Kullback-Leibler Divergance\n",
    "\n",
    "離散の場合\n",
    "\n",
    "$$\n",
    "D_{KL} (P||Q) = \\sum_i P(x_i) \\log \\frac{P(x_i)}{Q(x_i)}\n",
    "$$\n",
    "\n",
    "連続の場合\n",
    "\n",
    "$$\n",
    "D_{KL} (P||Q) = \\int_{-\\infty}^{\\infty}\n",
    "P(x) \\log \\frac{ P(x) } { Q(x) } dx\n",
    "$$\n",
    "\n",
    "期待値の記号を使って次のように書くこともできる\n",
    "\n",
    "$$\n",
    "D_{KL} (P||Q) = \\mathbb{E}_{x\\sim P}\\left[ \\log \\frac{P(x_i)}{Q(x_i)} \\right]\n",
    "$$\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "**特徴**\n",
    "\n",
    "- 対称性をもたない：$D_{KL} (P||Q) \\neq D_{KL} (Q||P)$\n",
    "    - → KLダイバージェンスは厳密な意味での距離ではない\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a47f8b-a94f-4b38-9cfc-be6b116f322d",
   "metadata": {},
   "source": [
    ":::{card} 別表現\n",
    "\n",
    "$\\log \\frac{M}{N} = \\log M - \\log N$より、KLダイバージェンスは\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "D_{KL} (P||Q)\n",
    "&= \\sum_i P(x_i) \\log \\frac{P(x_i)}{Q(x_i)}\\\\\n",
    "&= \\sum_i P(x_i) \\log P(x_i) - \\sum_i P(x_i) \\log Q(x_i)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "と変形できる。\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839ecd65-c8f4-4202-b3aa-073f65d8d993",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 交差エントロピーとの関係\n",
    "\n",
    "#### KLダイバージェンスの最小化 = 交差エントロピーの最小化\n",
    "\n",
    "KLダイバージェンスは2つの分布$P,Q$の違いを定量評価する関数であるが、仮に$P$が観測データで$Q$が予測モデルの出力だとすると$P$の部分は定数なので\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "D_{KL} (P||Q)\n",
    "&= \\underbrace{ \\sum_i P(x_i) \\log P(x_i) }_{\\text{定数}} - \\sum_i P(x_i) \\log Q(x_i)\\\\\n",
    "&= - \\sum_i P(x_i) \\log Q(x_i)\\\\\n",
    "&= H(P, Q)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となる。よって、$\\min D_{KL} (P||Q) = \\min H(P,Q)$、つまり、ダイバージェンスの最小化をしたい場合は交差エントロピーを最小化すればよいことになる。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a70473-2e40-4f82-8b57-5e03f03c7309",
   "metadata": {},
   "source": [
    "#### 式変形・エントロピーとの関係\n",
    "\n",
    ":::{card}\n",
    "\n",
    "$$\n",
    "H(P, Q) = H(P) + D_{KL}(P||Q)\n",
    "$$\n",
    "\n",
    "が成り立つ\n",
    "\n",
    ":::\n",
    "\n",
    ":::{dropdown} 導出\n",
    "\n",
    "$\\log \\frac{M}{N} = \\log M - \\log N$より、\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H(P, Q) &= H(P) + D_{KL}(p||q)\n",
    "\\\\\n",
    "&= - \\int P(x) \\log P(x) dx\n",
    "  + \\int P(x) \\log \\frac{ P(x) } { Q(x) } dx\\\\\n",
    "&= - \\int P(x) \\log P(x) dx\n",
    "  + \\int P(x) \\big\\{ \\log P(x) - \\log Q(x) \\big\\} dx\n",
    "\\\\\n",
    "&= \\underbrace{ - \\int P(x) \\log P(x) dx }_{ H(P) }\n",
    "  + \\underbrace{ \\int P(x) \\log P(x) dx }_{ -H(P) }\n",
    "  \\underbrace{ - \\int P(x) \\log Q(x) dx }_{ H(P, Q) }\n",
    "\\\\\n",
    "&= H(P, Q)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    ":::{card}\n",
    "\n",
    "$$\n",
    "D_{KL}(p||q) = H(P, Q) - H(P)\n",
    "$$\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    ":::{dropdown} 導出\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D_{KL}(p||q) &= H(P, Q) - H(P)\\\\\n",
    "&= - \\int P(x) \\log Q(x) dx\n",
    "   + \\int P(x) \\log P(x) dx\\\\\n",
    "&= \\int P(x) \\log P(x) dx - \\int P(x) \\log Q(x) dx\\\\\n",
    "&= \\int P(x) \\log \\frac{P(x)}{Q(x)} dx\\\\\n",
    "\\end{align}\n",
    "$$\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ff5760-343f-483e-a5d7-9d2447518784",
   "metadata": {},
   "source": [
    "## Density Power Divergence\n",
    "\n",
    "$\\beta$-divergenceとも\n",
    "\n",
    "KLダイバージェンスの拡張で、外れ値に頑健\n",
    "\n",
    "[Basu et al. (1998). Robust and efficient estimation by minimising a density power divergence. Biometrika, 85(3), 549-559.](https://academic.oup.com/biomet/article-abstract/85/3/549/228993)\n",
    "\n",
    "\n",
    "$$\n",
    "D_{\\beta}(Q, P) = d_{\\beta}(Q, P) - d_{\\beta}(Q, Q)\\\\\n",
    "d_{\\beta}(Q, P) = - \\frac{1}{\\beta} \\int P(x)^{\\beta} dQ(x) + r\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea1f92f-b6d7-4461-83c6-f62dbf6754ac",
   "metadata": {},
   "source": [
    "### 積分を回避する案\n",
    "\n",
    "- paper: [[2307.05251] A stochastic optimization approach to minimize robust density power-based divergences for general parametric density models](https://arxiv.org/abs/2307.05251)\n",
    "- 著者tweet: [A. OkunoさんはTwitterを使っています: 「公開しました．ざっくりいうと，悪名高い累乗の積分項があり今まで正規分布や一部の確率モデルでしか最適化ができなかったロバストダイバージェンスを\"一般の\"確率モデルに対して最適化する方法を提案しています．個人的にはかなり非自明で，言われれば当たり前だけど，思いつくまで10年かかりました．」 / Twitter](https://twitter.com/public_aokn/status/1678938719414554624?t=fPhnFiLFTQbWT0o0tS-Chw&s=19)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
