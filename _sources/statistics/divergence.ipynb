{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "287bd0a2-bab1-420f-ae79-81c3619b8001",
   "metadata": {},
   "source": [
    "# ダイバージェンス"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d451e1-2be7-4da2-8d75-90dd702a6780",
   "metadata": {},
   "source": [
    "## エントロピー\n",
    "\n",
    "### エントロピー\n",
    "\n",
    "$$\n",
    "H(p) = - \\int p(x) \\log p(x) dx\n",
    "$$\n",
    "\n",
    "\n",
    "### 交差エントロピー\n",
    "\n",
    "$$\n",
    "H(p, q) = - \\int p(x) \\log q(x) dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e78a56-5ff7-4184-9291-d286f4fc8d16",
   "metadata": {},
   "source": [
    "## Kullback-Leibler Divergance\n",
    "\n",
    "離散の場合\n",
    "\n",
    "$$\n",
    "D_{KL} (P||Q) = \\sum_i P(i) \\log \\frac{P(i)}{Q(i)}\n",
    "$$\n",
    "\n",
    "連続の場合\n",
    "\n",
    "$$\n",
    "D_{KL} (P||Q) = \\int_{-\\infty}^{\\infty}\n",
    "p(x) \\log \\frac{ p(x) } { q(x) } dx\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839ecd65-c8f4-4202-b3aa-073f65d8d993",
   "metadata": {},
   "source": [
    "### 交差エントロピーとの関係\n",
    "\n",
    "$\\log \\frac{M}{N} = \\log M - \\log N$より、\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H(p, q) &= H(p) + D_{KL}(p||q)\n",
    "\\\\\n",
    "&= - \\int p(x) \\log p(x) dx\n",
    "  + \\int p(x) \\log \\frac{ p(x) } { q(x) } dx\\\\\n",
    "&= - \\int p(x) \\log p(x) dx\n",
    "  + \\int p(x) \\big\\{ \\log p(x) - \\log q(x) \\big\\} dx\n",
    "\\\\\n",
    "&= - \\underbrace{ \\int p(x) \\log p(x) dx }_{ H(p) }\n",
    "  + \\underbrace{ \\int p(x) \\log p(x) dx }_{ H(p) }\n",
    "  - \\underbrace{ \\int p(x) \\log q(x) dx }_{ H(p, q) }\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D_{KL}(p||q) &= H(p, q) - H(p)\\\\\n",
    "&= - \\int p(x) \\log q(x) dx\n",
    "   - \\int p(x) \\log p(x) dx\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ff5760-343f-483e-a5d7-9d2447518784",
   "metadata": {},
   "source": [
    "## Density Power Divergence\n",
    "\n",
    "$\\beta$-divergenceとも\n",
    "\n",
    "KLダイバージェンスの拡張で、外れ値に頑健\n",
    "\n",
    "[Basu et al. (1998). Robust and efficient estimation by minimising a density power divergence. Biometrika, 85(3), 549-559.](https://academic.oup.com/biomet/article-abstract/85/3/549/228993)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a33d4fc-bb5f-41b5-98c0-b626ab853084",
   "metadata": {},
   "source": [
    "$$\n",
    "D_{\\beta}(Q, P) = d_{\\beta}(Q, P) - d_{\\beta}(Q, Q)\\\\\n",
    "d_{\\beta}(Q, P) = - \\frac{1}{\\beta} \\int p(x)^{\\beta} dQ(x) + r\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea1f92f-b6d7-4461-83c6-f62dbf6754ac",
   "metadata": {},
   "source": [
    "### 積分を回避する案\n",
    "\n",
    "- paper: [[2307.05251] A stochastic optimization approach to minimize robust density power-based divergences for general parametric density models](https://arxiv.org/abs/2307.05251)\n",
    "- 著者tweet: [A. OkunoさんはTwitterを使っています: 「公開しました．ざっくりいうと，悪名高い累乗の積分項があり今まで正規分布や一部の確率モデルでしか最適化ができなかったロバストダイバージェンスを\"一般の\"確率モデルに対して最適化する方法を提案しています．個人的にはかなり非自明で，言われれば当たり前だけど，思いつくまで10年かかりました．」 / Twitter](https://twitter.com/public_aokn/status/1678938719414554624?t=fPhnFiLFTQbWT0o0tS-Chw&s=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf27524-daac-4307-8559-40fd99790817",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
