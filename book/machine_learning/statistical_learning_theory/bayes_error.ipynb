{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "043c70ee-35b3-4e1b-9bba-b4aaf0bddbd9",
   "metadata": {},
   "source": [
    "# ベイズ誤差\n",
    "\n",
    "ベイズ誤差はその問題設定と関数集合のもとでの最小の（どうあがいてもそれ以上削減できない）誤差\n",
    "\n",
    ":::{admonition} 定義\n",
    "\n",
    "損失関数 $\\ell$ を定めたとき、任意の可測関数 $h: \\mathcal{X} \\rightarrow \\mathcal{Y}$ のもとでの予測損失の下限\n",
    "\n",
    "$$\n",
    "\\inf _{h: \\text { 可測 }} R(h)\n",
    "$$\n",
    "\n",
    "\n",
    "を損失関数 $\\ell$ のもとでの **ベイズ誤差（Bayes error）** という。下限を達成する仮説が存在するとき、その仮説を **ベイズ規則（Bayes rule）** という。\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9304cc6c-7fc0-4cc5-84b8-2bb15d30f5c4",
   "metadata": {},
   "source": [
    "条件付き期待値\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_Y[\\ell(h(x), Y) \\mid x] = \\int_{\\mathcal{Y}} \\ell(h(x), y) ~ d P(y \\mid x)\n",
    "$$\n",
    "\n",
    "を用いると\n",
    "\n",
    "$$\n",
    "R(h)=\\mathbb{E}_X [\\mathbb{E}_Y[\\ell(h(X), Y) \\mid X]]\n",
    "$$\n",
    "\n",
    "であるため、$\\mathbb{E}_Y[\\ell(h(x), Y) \\mid x]$を最小にする仮説$h$を選べば予測誤差が最小になる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d65b58e-dc13-49c5-b138-1e95821b0363",
   "metadata": {},
   "source": [
    "## ベイズ誤差の推定\n",
    "\n",
    "[Tumer, K., & Ghosh, J. (2003). Bayes error rate estimation using classifier ensembles. International Journal of Smart Engineering System Design, 5(2), 95-109.](https://www.tandfonline.com/doi/abs/10.1080/10255810305042)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8618525a-06c1-4454-b7cc-4c960127411d",
   "metadata": {},
   "source": [
    ":::{card} 利用例\n",
    "\n",
    "Booking.comでMLモデルを作るとき「問題がどれくらい解けそうか」を事前に調査している。\n",
    "\n",
    "その際は\n",
    "\n",
    "1. （推薦の場合）人気度順やランダムな推薦というbaselineに対してシンプルなMLモデルでどれだけ改善できるかを見る\n",
    "2. Tumer & Ghosh (2003)の方法でBayes errorを推定する\n",
    "\n",
    "を使っているらしい（[Bernardi et al., 2019](https://blog.kevinhu.me/2021/04/25/25-Paper-Reading-Booking.com-Experiences/bernardi2019.pdf)）。\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422ddda3-55c5-42d8-b079-9d1dc6e0f5d0",
   "metadata": {},
   "source": [
    "### BN-BER\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32ca850-8fb6-4987-b121-441d252037ef",
   "metadata": {},
   "source": [
    "\n",
    ":::{card}\n",
    "\n",
    "[Chen, Q., Cao, F., Xing, Y., & Liang, J. (2025). An efficient Bayes error rate estimation method. Machine Learning, 114(6), 1-25.](https://dl.acm.org/doi/10.1007/s10994-025-06761-w?utm_source=chatgpt.com)\n",
    "\n",
    "BN-BERの空間計算量は$O(n^2)$だったのを$O(n (\\log_2 n)^2 )$へ改善\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ae9443-6c79-4699-9815-ac5747b90c81",
   "metadata": {},
   "source": [
    "### Ishida et al. 2022\n",
    "\n",
    "[[2202.00395] Is the Performance of My Deep Network Too Good to Be True? A Direct Approach to Estimating the Bayes Error in Binary Classification](https://arxiv.org/abs/2202.00395)\n",
    "\n",
    "- 二値分類におけるディープネットワークのベイズ誤差を推定する\n",
    "- ソフトラベル（soft or uncertainty labels）の平均からBayes誤差を直接推定するシンプルな方法を提案。\n",
    "- 必要な情報はラベルの確率推定値のみで、モデルも学習もインスタンスの特徴量も一切不要（model- and instance-free）。\n",
    "- ハイパーパラメータがなく、 バイアスなく一貫性（consistent）のある推定量を構成できると理論的に示している\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f2e007-7d49-4e88-8faa-5fac0b88e6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
