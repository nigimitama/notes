{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3002c51f-61df-40bc-936a-c029a8df9f52",
   "metadata": {},
   "source": [
    "# 勾配ブースティング決定木"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628c15b6-9470-4750-add2-bda7e660e23a",
   "metadata": {},
   "source": [
    "## Gradient Boosting Machine\n",
    "\n",
    "複数の関数に重みをかけて足し合わせた関数\n",
    "\n",
    "$$\n",
    "\\newcommand{\\argmin}{\\mathop{\\rm arg~min}\\limits}\n",
    "f(x) = f_0(x; \\theta_0) + \\beta_1 f_1(x; \\theta_1) + \\cdots + \\beta_M f_M(x; \\theta_M)\n",
    "$$\n",
    "\n",
    "の形で予測モデルを構築することを考える。ここで$\\theta_0, \\dots, \\theta_M$は関数を形づくるパラメータ（例えば線形回帰の重みや決定木の分岐の閾値）である。\n",
    "\n",
    "このモデルは$\\beta_1, \\beta_2, \\dots, \\beta_M$と$\\theta_0, \\theta_1, \\dots, \\theta_M$のパラメータを推定する必要がある。\n",
    "今回はすべてのパラメータを一度に学習するのではなく、$\\beta_m f_m(x; \\theta_m)$を一つずつ学習していく方法を考える。具体的には次のように行う。\n",
    "\n",
    ":::{card}\n",
    "**前向き段階的加法モデリング（forward stagewise additive modeling）**\n",
    "\n",
    "1.  $f_0(x) = 0$で初期化\n",
    "2.  $m = 1$から$M$までについて、\n",
    "    1.  パラメータを推定する：$(\\beta_m, \\theta_m) = \\argmin_{\\beta, \\theta} \\sum^N_{i=1} L(y_i, f_{m-1}(x_i) + \\beta f(x_i; \\theta))$\n",
    "    2.  新たなモデルを足す：$f_m(x) = f_{m-1}(x) + \\beta_m f(x; \\theta_m)$\n",
    ":::\n",
    "\n",
    "これを前向き段階的加法モデリング（forward stagewise additive\n",
    "modeling）という。ブースティングはこの方法でアンサンブル学習を行う。\n",
    "\n",
    "誤差関数が二乗誤差$L(y, f(x)) = (y - f(x))^2$の場合、\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(y_i, f_{m-1}(x_i) + \\beta f(x_i; \\theta))\n",
    "&= (y_i - f_{m-1}(x_i) - \\beta f(x_i; \\theta))^2 \\\\\n",
    "&= (\\text{residual}_{i,m-1} - \\beta f(x_i; \\theta))^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "となり、$m-1$回目のモデルの残差$\\text{residual}_{i,m-1} = y_i - f_{m-1}(x_i)$を近似するように$m$回目のモデル$\\beta f(x_i; \\theta)$を学習させていると捉えることができる。残差が大きければそれだけ訓練中に重視されるため「間違えた箇所を重点的に学習する手法」とも捉えることができる。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0409373-96c2-4ee6-b014-28033859d5ed",
   "metadata": {},
   "source": [
    "### 最適化の観点からの説明\n",
    "\n",
    "#### 勾配降下法\n",
    "\n",
    "数理最適化において関数の最小化問題\n",
    "\n",
    "$$\n",
    "\\newcommand{\\b}[1]{ \\boldsymbol{#1} }\n",
    "\\min_\\b{x} f(\\b{x})\n",
    "$$\n",
    "\n",
    "を解く方法のひとつに**勾配降下法**（gradient descent method）あるいは**最急降下法**（steepest descent method）と呼ばれるものがある。これは目的関数の微分のベクトルである勾配\n",
    "\n",
    "$$\n",
    "\\nabla f(\\b{x}) = \n",
    "\\begin{bmatrix}\n",
    "    \\frac{ \\partial f(x_1) }{ \\partial x_1 }\\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{ \\partial f(x_m) }{ \\partial x_m }\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "を用いて\n",
    "\n",
    "$$\n",
    "\\b{x}_{m} = \\b{x}_{m-1} - \\alpha_{m-1} \\nabla f(\\b{x}_{m-1})\n",
    "$$\n",
    "\n",
    "という値の更新を何度も繰り返して最適化を行っていく。ここで$\\alpha$は学習率と呼ばれるパラメータで、値の更新量が多すぎると最適解を通り過ぎてしまうことがあるので小さめの値を乗じて更新幅を抑えるために用いられる。\n",
    "\n",
    "\n",
    "最終的に$M$回反復して得た最適解$x^*$は\n",
    "\n",
    "$$\n",
    "x^* = x_0\n",
    "- \\alpha_1 \\frac{\\partial f(x_{1})}{\\partial x_{1}}\n",
    "- \\alpha_2 \\frac{\\partial f(x_{2})}{\\partial x_{2}}\n",
    "- \\cdots\n",
    "- \\alpha_M \\frac{\\partial f(x_{M})}{\\partial x_{M}}\n",
    "$$\n",
    "\n",
    "となり、ブースティングにより得られる予測モデル\n",
    "\n",
    "$$\n",
    "f(x) = f_0(x; \\theta_0) + \\beta_1 f_1(x; \\theta_1) + \\cdots + \\beta_M f_M(x; \\theta_M)\n",
    "$$\n",
    "\n",
    "と同様に重み付き和の形になる。\n",
    "\n",
    "#### ブースティング\n",
    "\n",
    "ブースティングは勾配降下法を機械学習で行っていると捉えることができる。\n",
    "\n",
    "機械学習においては予測値$f(x)$と実測値$y$の誤差の最小化問題\n",
    "\n",
    "$$\n",
    "\\min_{f(x)} L(y, f(x))\n",
    "$$\n",
    "\n",
    "を解きたいため、勾配は誤差関数の予測モデルによる微分$\\frac{ \\partial L(y, f(x)) }{ \\partial f(x) }$によって得られる。\n",
    "\n",
    "二乗誤差$L(y, f(x)) = \\frac{1}{2} (y - f(x))^2$の場合、負の勾配は残差である\n",
    "\n",
    "$$\n",
    "- \\frac{ \\partial L(y, f(x)) }{ \\partial f(x) } = y - f(x) = \\text{residual}\n",
    "$$\n",
    "\n",
    "前向き段階的加法モデルの節で「二乗誤差の場合は残差を近似するように学習している」と述べた。\n",
    "\n",
    "$$\n",
    "L(y_i, f_{m-1}(x_i) + \\beta f(x_i; \\theta)) = (\\text{residual}_{i,m-1} - \\beta f(x_i; \\theta))^2\n",
    "$$\n",
    "\n",
    "これにより、学習されるモデル$\\beta f(x; \\theta)$は負の勾配を学習するようになり、最終的にそれらの和となるモデルは勾配降下法を解いた状態を近似することになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e8cc0a-f746-47b8-a0ec-a5b6e022cb62",
   "metadata": {},
   "source": [
    "## 正則化つきGBDT\n",
    "\n",
    "$n$個の観測データがあり、$m$次元の特徴量があるとする。\n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\{ (x_i, y_i) \\}, |D| = n, x_i \\in \\mathbb{R}^m, y \\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "\n",
    "勾配ブースティング決定木のモデルは次のように表される\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\phi(x_i) = \\sum^K_{k=1} f_k(x_i)\n",
    "$$\n",
    "\n",
    "ここで\n",
    "- $f_k \\in \\mathcal{F}$は予測器\n",
    "- $\\mathcal{F} = \\{ f(x) = w_{q(x)} \\}$は回帰木の空間\n",
    "  - $q: \\mathbb{R}^m \\to T$は入力データ$x$を木の各葉のインデックスに割り振る写像。$T$は各木の葉の数\n",
    "  - $w \\in \\mathbb{R}^T$は葉の重み（weights）と呼ばれ、予測に使われた葉の出力値。予測値は$w_{q(x)}$の和となるので予測値のベースでもある\n",
    "\n",
    "\n",
    "学習の際は正則化付き誤差関数\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\phi) = \\sum^n_{i=1} l(\\hat{y}, y_i) + \\sum_k \\Omega(f_k)\n",
    "$$\n",
    "\n",
    "を最小化する。ここで$l$は微分可能な凸関数である誤差関数で、$\\Omega$は正則化項\n",
    "\n",
    "$$\n",
    "\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda ||w||^2\n",
    "$$\n",
    "\n",
    "\n",
    "学習は加法的に行うため$t$番目の誤差は次のようになる。\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{(t)}(\\phi) = \\sum^n_{i=1} l(y_i, \\hat{y}^{(t-1)} + f_t(x_i)) + \\Omega(f_t)\n",
    "$$\n",
    "\n",
    "\n",
    "### テイラー展開による近似\n",
    "\n",
    "この誤差を二次近似したものを使うことで計算量を削減することもできることが知られている（[Friedman et al., 2000](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=6f4493eff2531536a7aeb3fc11d62c30a8f487f6)）\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{(t)} \\approx \n",
    "\\sum^n_{i=1} [\n",
    "     l(y_i, \\hat{y}^{(t-1)})\n",
    "     + g_i f_t(x_i)\n",
    "     + \\frac{1}{2} h_i f_t^2(x_i)\n",
    "]\n",
    " + \\Omega(f_t)\n",
    "$$\n",
    "\n",
    "\n",
    "ここで\n",
    "\n",
    "$$\n",
    "g_i = \\frac{ \\partial l(y_i, \\hat{y}^{(t-1)}) }{\\partial \\hat{y}^{(t-1)} }\\\\\n",
    "h_i = \\frac{ \\partial^2 l(y_i, \\hat{y}^{(t-1)}) }{\\partial (\\hat{y}^{(t-1)})^2 }\n",
    "$$\n",
    "\n",
    "定数項を省略すると\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathcal{L}}^{(t)} =\n",
    "\\sum^n_{i=1} [\n",
    "     g_i f_t(x_i)\n",
    "     + \\frac{1}{2} h_i f_t^2(x_i)\n",
    "]\n",
    " + \\Omega(f_t)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15246a5-12c4-498a-8141-e03db95bc801",
   "metadata": {},
   "source": [
    "\n",
    "葉$j$におけるインスタンス（サンプル）の集合を$I_j = \\{i|q(x_i) = j\\}$と表記すると、次のように書き換えることができる\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\tilde{\\mathcal{L}}^{(t)}\n",
    "&= \\sum^n_{i=1} [\n",
    "     g_i f_t(x_i)\n",
    "     + \\frac{1}{2} h_i f_t^2(x_i)\n",
    "] + \\gamma T + \\frac{1}{2} \\lambda \\sum^T_{j=1} w_j^2\n",
    "\\\\\n",
    "&= \\sum^T_{j=1} [\n",
    "     (\\sum_{i\\in I_j} g_i) w_j\n",
    "     + \\frac{1}{2} (\\sum_{i\\in I_j} h_i + \\lambda ) w^2_j\n",
    "] + \\gamma T\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "固定した木の構造$q(x)$について、葉$j$の最適な重みは\n",
    "\n",
    "$$\n",
    "w^*_j = -\\frac{ \\sum_{i\\in I_j} g_i }{ \\sum_{i \\in I_j} h_i + \\lambda }\n",
    "$$\n",
    "\n",
    "となる。\n",
    "\n",
    ":::{dropdown} 導出\n",
    "\n",
    "葉$j$についての部分だけ取り出して導関数を0とおいて整理する\n",
    "\n",
    "$$\n",
    "\\frac{ \\partial \\tilde{\\mathcal{L}}^{(t)}_j }{\\partial w_j }\n",
    "= \\sum_{i\\in I_j} g_i + (\\sum_{i\\in I_j} h_i + \\lambda ) w_j\n",
    "= 0\n",
    "\\\\\n",
    "\\implies\n",
    "(\\sum_{i\\in I_j} h_i + \\lambda ) w_j\n",
    "= -\\sum_{i\\in I_j} g_i\n",
    "\\\\\n",
    "\\implies\n",
    "w_j = -\\frac{ \\sum_{i\\in I_j} g_i }{ \\sum_{i\\in I_j} h_i + \\lambda } = w_j^*\n",
    "$$\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022d8719-fc60-4d35-974e-7a9462a9abe4",
   "metadata": {},
   "source": [
    "L1の場合\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\tilde{\\mathcal{L}}^{(t)}\n",
    "&= \\sum^n_{i=1} [\n",
    "     g_i f_t(x_i)\n",
    "     + \\frac{1}{2} h_i f_t^2(x_i)\n",
    "]\n",
    "+ \\gamma T + \\frac{1}{2} \\lambda \\sum^T_{j=1} w_j^2\n",
    "+ \\alpha \\sum^T_{j=1} |w_j|\n",
    "\\\\\n",
    "&= \\sum^T_{j=1} [\n",
    "     (\\sum_{i\\in I_j} g_i) w_j\n",
    "     + \\frac{1}{2} (\\sum_{i\\in I_j} h_i + \\lambda ) w^2_j\n",
    "] + \\gamma T\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cf5524-14d2-4a57-b75b-6e082cdc6573",
   "metadata": {},
   "source": [
    "L1の場合\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\tilde{\\mathcal{L}}^{(t)}\n",
    "&= \\sum^n_{i=1} [\n",
    "     g_i f_t(x_i)\n",
    "     + \\frac{1}{2} h_i f_t^2(x_i)\n",
    "]\n",
    "+ \\gamma T + \\frac{1}{2} \\lambda \\sum^T_{j=1} w_j^2\n",
    "+ \\alpha \\sum^T_{j=1} |w_j|\n",
    "\\\\\n",
    "&= \\sum^T_{j=1} [\n",
    "     (\\sum_{i\\in I_j} g_i) w_j\n",
    "     + \\alpha |w_j|\n",
    "     + \\frac{1}{2} (\\sum_{i\\in I_j} h_i + \\lambda ) w^2_j\n",
    "] + \\gamma T\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\\sum^T_{j=1} [\\cdot]$の内側を整理すると\n",
    "\n",
    "$$\n",
    "w^*_j = -\\frac\n",
    "{ \\sum_{i\\in I_j} g_i \\pm \\alpha }\n",
    "{ \\sum_{i \\in I_j} h_i + \\lambda }\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5d040d-72ee-41d3-88ca-ddd67e12daa6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcd4ddc9-9ac8-48cb-968e-5490dda7f895",
   "metadata": {},
   "source": [
    "最適な重み$w_j^*$を誤差関数に戻すと\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathcal{L}}^{(t)}(q)\n",
    "= -\\frac{1}{2} \\sum^T_{j=1}\n",
    "\\frac{ (\\sum_{i\\in I_j} g_i)^2 }\n",
    "{ \\sum_{i\\in I_j} h_i + \\lambda }\n",
    "+ \\gamma T\n",
    "$$\n",
    "\n",
    "となり、これ木の構造$q$の品質をスコアリングする関数として使うことができる。\n",
    "\n",
    ":::{dropdown} 導出\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\tilde{\\mathcal{L}}^{(t)}\n",
    "&= \\sum^T_{j=1} [\n",
    "     (\\sum_{i\\in I_j} g_i)\n",
    "     (-\\frac{ \\sum_{i\\in I_j} g_i }{ \\sum_{i \\in I_j} h_i + \\lambda })\n",
    "     + \\frac{1}{2} (\\sum_{i\\in I_j} h_i + \\lambda ) \n",
    "     (-\\frac{ \\sum_{i\\in I_j} g_i }{ \\sum_{i \\in I_j} h_i + \\lambda })^2\n",
    "] + \\gamma T\n",
    "\\\\\n",
    "&= \\sum^T_{j=1} [\n",
    "     -\\frac{ (\\sum_{i\\in I_j} g_i)^2 }{ \\sum_{i \\in I_j} h_i + \\lambda }\n",
    "     + \\frac{1}{2} \\frac{ (\\sum_{i\\in I_j} g_i)^2 }{ \\sum_{i \\in I_j} h_i + \\lambda }\n",
    "] + \\gamma T\n",
    "\\\\\n",
    "&= \\sum^T_{j=1} [\n",
    "     - \\frac{1}{2} \\frac{ (\\sum_{i\\in I_j} g_i)^2 }{ \\sum_{i \\in I_j} h_i + \\lambda }\n",
    "] + \\gamma T\n",
    "\\end{align}\n",
    "$$\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f480c9d3-65b1-40f0-876c-17061900efd0",
   "metadata": {},
   "source": [
    "## 参考\n",
    "\n",
    "- [Schapire, R. E. (1990). The strength of weak learnability. Machine learning, 5, 197-227.](http://rob.schapire.net/papers/strengthofweak.pdf)\n",
    "- [Friedman, J., Hastie, T., & Tibshirani, R. (2000). Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the authors). The annals of statistics, 28(2), 337-407.](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=6f4493eff2531536a7aeb3fc11d62c30a8f487f6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6fb8a5-2952-4cbe-a6d6-dada5dc39a28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
