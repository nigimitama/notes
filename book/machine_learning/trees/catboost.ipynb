{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0de891a7-c6b4-4085-af23-d13102a21ed2",
   "metadata": {},
   "source": [
    "# CatBoost\n",
    "\n",
    "CatBoostはYandexが中心となって開発しているGBDTのOSSライブラリ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80926d97-7c0f-4669-b5e2-4af2a3a64140",
   "metadata": {},
   "source": [
    "## 関連論文\n",
    "\n",
    "[Reference papers | CatBoost](https://catboost.ai/docs/en/concepts/educational-materials-papers)\n",
    "\n",
    "- [CatBoost: unbiased boosting with categorical features](https://arxiv.org/abs/1706.09516)\n",
    "    - CatBoost の Ordered Boosting と Ordered Categorical Features Statistics の説明\n",
    "- [CatBoost: gradient boosting with categorical features support](http://learningsys.org/nips17/assets/papers/paper_11.pdf)\n",
    "    - CatBoostの基本的な動作原理の説明\n",
    "- [Minimal Variance Sampling in Stochastic Gradient Boosting](https://arxiv.org/abs/1910.13204)\n",
    "    - CatBoostのデフォルトのサンプリング手法MVSについて\n",
    "- [Finding Influential Training Samples for Gradient Boosted Decision Trees](https://arxiv.org/abs/1802.06640)\n",
    "    - あるサンプルに対する予測について影響の大きい訓練データ中の事例を探索するアルゴリズムについて\n",
    "- [Which Tricks are Important for Learning to Rank?](https://arxiv.org/abs/2204.01500)\n",
    "    - ランキング学習の誤差関数について、LambdaMART, YetiRank, StochasticRankを比較してYetiRankの改良を提案\n",
    "- [Gradient Boosting Performs Gaussian Process Inference](https://arxiv.org/abs/2206.05608)\n",
    "    - 対称決定木（symmetric decision tress）の勾配ブースティングはカーネル法と等価であり、カーネルリッジ回帰問題の解へと変換できることを示した\n",
    "    - つまり、CatBoostでガウス過程と同様の結果が得られることを示した。\n",
    "    - 実装：[sample_gaussian_process | CatBoost](https://catboost.ai/docs/en/concepts/python-reference_sample_gaussian_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c55819-844e-40ba-aa19-a3346bf194f6",
   "metadata": {},
   "source": [
    "[Prokhorenkova, et al. (2018). CatBoost: unbiased boosting with categorical features. NeurIPS2018](https://arxiv.org/pdf/1706.09516.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c322c3d1-8d03-4ae3-8674-c22d7c070be9",
   "metadata": {},
   "source": [
    "[Dorogush, et al. (2018). CatBoost: gradient boosting with categorical features support. NIPS2017](http://learningsys.org/nips17/assets/papers/paper_11.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43e77f8-e36c-4bd5-98a3-1ce8af00cae4",
   "metadata": {},
   "source": [
    "## Prediction Shift\n",
    "\n",
    "[CatBoost論文のprediction shiftについて完全に理解する - threecourse’s blog](https://threecourse.hatenablog.com/entry/2020/05/29/204944)\n",
    "\n",
    "- 各iterationで同じ訓練データを使うことの問題\n",
    "- 1本目と2本目の木の作成で同じデータセットを使うことにより、予測値の期待値に$1 / (n − 1)$ に比例する偏りが発生する。\n",
    "- prediction shiftの影響を受けないよう、CatBoostではデータ数が少ない場合にはordered boostingというアルゴリズムを使用するようになっている。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69308745-3615-4aa2-b277-ba9a1febd9a6",
   "metadata": {},
   "source": [
    "## Categorical featuresの扱い\n",
    "\n",
    "### ラベル統計に基づく方法\n",
    "\n",
    "別の方法として、ラベル値を用いてカテゴリごとの統計量を計算する手法がある。  \n",
    "データセット $\\{ (x_i, Y_i) \\}$ が与えられており、$x_i$ は数値的およびカテゴリ的特徴を含むベクトル、$Y_i$ はラベルであるとする。  \n",
    "最も単純な方法は、カテゴリ全体の平均ラベル値で置き換えるものである。すなわち、カテゴリ $x_{i,k}$ は次のように置き換えられる：\n",
    "\n",
    "$$\n",
    "x_{i,k} \\to \n",
    "\\frac{\\sum_{j=1}^n [x_{j,k} = x_{i,k}] \\cdot Y_j}\n",
    "     {\\sum_{j=1}^n [x_{j,k} = x_{i,k}]}\n",
    "$$\n",
    "\n",
    "ここで、$[\\cdot]$ はイベルソン括弧（条件が真なら1、偽なら0）である。  \n",
    "しかし、この方法は明らかに **過学習** を引き起こす。  \n",
    "例えば、あるカテゴリがデータ中に1回しか出現しない場合、その数値はそのラベルと等しくなってしまう。\n",
    "\n",
    "この問題を避けるための単純な方法は、データセットを2つに分割し、一方で統計量を計算し、もう一方で訓練することである。  \n",
    "しかしこれはデータ量を減らしてしまう。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e79a55c-125d-49ce-b796-2c11921ecfcc",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### CatBoost における戦略\n",
    "\n",
    "CatBoost は過学習を抑制しつつ、全データを利用できる効率的な戦略を採る。  \n",
    "すなわち、データセットの **ランダム順列** $\\sigma$ を生成し、各サンプルについて、その順列内で前に出現する同じカテゴリを持つサンプルのラベル平均を計算する：\n",
    "\n",
    "$$\n",
    "x_{\\sigma_p, k} \\to\n",
    "\\frac{\\sum_{j=1}^{p-1} [x_{\\sigma_j, k} = x_{\\sigma_p, k}] \\cdot Y_{\\sigma_j} + a \\cdot P}\n",
    "     {\\sum_{j=1}^{p-1} [x_{\\sigma_j, k} = x_{\\sigma_p, k}] + a}\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "ここで、  \n",
    "- $a > 0$ は **prior の重み**、  \n",
    "- $P$ は **prior 値**（例えば回帰なら平均ラベル値、二値分類なら正例の事前確率）である。\n",
    "\n",
    "prior を導入することにより、出現頻度の低いカテゴリから生じるノイズを軽減できる。  \n",
    "さらに複数の順列を用いることも可能であるが、単純に統計量を併用すると過学習を起こす。  \n",
    "CatBoost では次節で述べる新しい葉値計算法により、複数順列の利用を安全に行える。\n",
    "\n",
    "---\n",
    "\n",
    "### 特徴の組み合わせ\n",
    "\n",
    "複数のカテゴリ特徴を組み合わせて新しい特徴を作ることもできる。  \n",
    "例えば、音楽推薦タスクにおいて「ユーザーID」と「音楽ジャンル」があるとする。  \n",
    "ユーザーが特定のジャンルを好む情報（例：「ユーザー123はロックが好き」）は、各特徴を別々に変換すると失われるが、2つを組み合わせることで新たな強力な特徴となる。\n",
    "\n",
    "しかし、全ての組み合わせを考慮すると組み合わせ数が指数的に増えるため、CatBoost では **貪欲法 (greedy approach)** により木構築中の分割で生成可能な組み合わせのみを考慮する。\n",
    "\n",
    "- 最初の分割では組み合わせを考慮しない。  \n",
    "- 以降の分割では、現在の木で使われたカテゴリ特徴とデータセット中の他のカテゴリ特徴を組み合わせる。  \n",
    "- 組み合わせ値は「その場で」数値に変換される。  \n",
    "- 数値特徴とカテゴリ特徴の組み合わせも同様に扱われる。\n",
    "\n",
    "---\n",
    "\n",
    "### 実装上の重要な詳細\n",
    "\n",
    "カテゴリを数値に置き換えるもう一つの単純で強力な方法は、**そのカテゴリの出現回数**を計算することである。  \n",
    "CatBoost ではこの統計も計算し、特徴の組み合わせにも適用している。\n",
    "\n",
    "また、CatBoost アルゴリズムの各ステップで最適な prior を適合させるために、複数の prior を試し、それぞれについて特徴を構築する。  \n",
    "これは前述の標準的手法よりも品質面で優れている。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e5f656-76b8-42cd-8cba-2d83f4f0aed4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
