{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95ed8ea6-77e9-4e82-b261-6827b46200f8",
   "metadata": {},
   "source": [
    "# 影響関数（influence function）\n",
    "\n",
    "\n",
    "- [Koh & Liang (2017)](http://proceedings.mlr.press/v70/koh17a/koh17a.pdf)が提案\n",
    "- [Hara et al. (2019)](https://proceedings.neurips.cc/paper/2019/file/5f14615696649541a025d3d0f8e0447f-Paper.pdf)はconvexでない損失関数にも使えるよう拡張\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c97447-bb52-4956-8260-6d9bcaa7e2a2",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "- データ点$z=(x, y)$\n",
    "- 誤差関数$L(z, \\theta)$\n",
    "  - 経験リスク：$R = \\frac{1}{N} \\sum^N_{i=1} L(z_i, \\theta)$\n",
    "- $N$訓練データ点が訓練集合$\\mathcal{Z}$に含まれるとする\n",
    "- 標準的な経験リスク最小化：$\\hat{\\theta} = \\arg \\min_{\\theta} \\frac{1}{N} \\sum^N_{i=1} L(z_i, \\theta)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9189238f-e4a6-4ed7-acec-8bbed212af80",
   "metadata": {},
   "source": [
    "## LOO\n",
    "\n",
    "1つのインスタンスを抜いた状態で訓練した場合にどれだけ予測値が変わるかを考える\n",
    "\n",
    "$n$回学習し直す必要があるので現実的ではない\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b2a68a-b735-4a87-a6f4-9e4127658d9a",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\theta}_{-z} - \\hat{\\theta}\\\\\n",
    "\\hat{\\theta}_{-z} := \\arg \\min_{\\theta \\in \\Theta} \\sum_{z_i \\neq z} L(z_i, \\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f49db0f-ab8f-49ed-bbd8-5ed428039c43",
   "metadata": {},
   "source": [
    "## Influence Function\n",
    "\n",
    "LOOを近似し、再学習を不要にしたい\n",
    "\n",
    "訓練データ全部を使った経験リスクに、データ点$z$についての誤差$L(z, \\theta)$を重み付きで足したリスク関数で訓練したパラメータ$\\hat{\\theta}_{\\epsilon,z}$を考える\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{\\epsilon,z}\n",
    ":= \\arg \\min_{\\theta} \\frac{1}{N} \\sum^N_{i=1} L(z_i, \\theta) + \\epsilon L(z, \\theta)\n",
    "$$\n",
    "\n",
    "$z$を入れることによる変化分がとれるので、これの$\\epsilon$での微分の$\\epsilon = 0$のときの値をパラメータについての上側のinfluence functionとする\n",
    "\n",
    "$$\n",
    "\\mathcal{I}_{up, params} := \\left . \\frac{d \\hat{\\theta}_{\\epsilon,z} }{ d \\epsilon } \\right | _{\\epsilon = 0}\n",
    "= -H^{-1}_{\\hat{\\theta}} \\nabla_{\\theta} L(z, \\hat{\\theta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb902a7-e6a0-4d3a-b99f-111aa1b980a0",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "$$\n",
    "\\left. \\frac{dy}{dx} \\right|_{x=a}\n",
    "$$\n",
    "\n",
    "は「$\\frac{dy}{dx}$に$x=a$を代入した値」あるいは「$x=a$のときの$\\frac{dy}{dx}$」という意味らしい\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e103ac-1ee1-4455-9501-e4fdb3618bb0",
   "metadata": {},
   "source": [
    "## FastFI\n",
    "\n",
    "Influence Functionは計算が重い\n",
    "\n",
    "1. データ点の評価は$O(n)$\n",
    "2. モデルパラメータのinverse Hessianの計算コストが高い\n",
    "3. 上記の計算は並列可能であるが、先行研究のアルゴリズムでは直列\n",
    "\n",
    "FastIfのアイデア\n",
    "\n",
    "1. 全データを探索するのではなく、fast nearest neighbor search（Johnson et al., 2017）で探索範囲を狭め、桁違いに計算量を抑える\n",
    "2. Hessianの推定において、品質を保ちつつ時間を半分以下にするハイパーパラメータ集合を識別\n",
    "3. シンプルに並列計算へ拡張し、さらに2倍高速化\n",
    "\n",
    "実験においてほとんどのケースで全体で2桁程度の高速化が確認された\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bb9a38-07e1-435d-a095-681c8b22d155",
   "metadata": {},
   "source": [
    "## 参考文献\n",
    "\n",
    "- [Influence Functionでインスタンスの重要度を解釈する - Dropout](https://dropout009.hatenablog.com/entry/2021/07/19/223929)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d632e7aa-9023-413a-8fef-4042b7284eed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
