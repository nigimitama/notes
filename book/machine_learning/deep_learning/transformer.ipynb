{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7e5d24c-d232-45fe-ad46-e41125f49d68",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "[Vaswani, et al. (2017). Attention is all you need.](https://arxiv.org/pdf/1706.03762.pdf) で提案されたDeep Learningの新しいアーキテクチャ\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d664c83a-7eb1-4858-badf-1345051570c5",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V)\n",
    "= \\text{softmax}\n",
    "\\left(\n",
    "    \\frac\n",
    "    { Q K^T }\n",
    "    { \\sqrt{d_k} }\n",
    "\\right)\n",
    "V\n",
    "$$\n",
    "\n",
    "単純化のためQ, Kの行列からベクトルをとってきて示すことにする。qとkの内積\n",
    "\n",
    "内積は類似度に使われる（cf. コサイン類似度）\n",
    "\n",
    "$$\n",
    "\\newcommand{\\b}[1]{\\boldsymbol{#1}}\n",
    "\\text{softmax}\n",
    "\\left(\n",
    "    \\frac{\\b{q} \\cdot \\b{k}}\n",
    "    {\\sqrt{d}}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "これは\n",
    "\n",
    "1. クエリ$\\b{q}$とキー$\\b{k}$の内積（＝類似度）を計算する\n",
    "2. 次元数$d$によって正規化する（内積は次元数が多いほど値も大きくなるので）\n",
    "3. softmaxによって確率値へと値の範囲を整える\n",
    "\n",
    "という処理になる。いわば確率ベクトルを返すようなものになる。\n",
    "\n",
    "例えば$i$番目のトークンだけこの出力値が1だとして、ほかが0だとすると、$V$の$i$番目のベクトルだけが出力となる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01843ea3-1762-4f3a-bc26-057ba64eacae",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\b}[1]{\\boldsymbol{#1}}\n",
    "\\b{v}_i = (v_1, \\cdots, v_d)\n",
    "\\\\\n",
    "\\b{V} = \n",
    "\\begin{pmatrix}\n",
    "    \\b{v}_1\\\\\n",
    "    \\b{v}_2\\\\\n",
    "    \\vdots\\\\\n",
    "    \\b{v}_n\n",
    "\\end{pmatrix}\n",
    "\\\\\n",
    "\\b{p} = (p_1, \\cdots, p_n) = \n",
    "\\text{softmax}\n",
    "\\left(\n",
    "    \\frac{\\b{q} \\cdot \\b{k}}\n",
    "    {\\sqrt{d}}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "とする\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\b{p} \\b{V}\n",
    "&=  (p_1, \\cdots, p_n)\n",
    "\\begin{pmatrix}\n",
    "    v_{11} & \\cdots & v_{1d}\\\\\n",
    "    \\vdots & \\ddots & \\vdots\\\\\n",
    "    v_{n1} & \\cdots & v_{nd}\\\\\n",
    "\\end{pmatrix}\n",
    "\\\\\n",
    "&= \n",
    "\\begin{pmatrix}\n",
    "    p_1 v_{11} + \\cdots + p_n v_{n1},\\\n",
    "    \\cdots, \\\n",
    "    p_1 v_{1d} + \\cdots + p_n v_{nd}\n",
    "\\end{pmatrix}\n",
    "\\\\\n",
    "&= p_1 (v_{11}, \\cdots, v_{1d}) + \\cdots + p_n (v_{n1}, \\cdots, v_{nd})\n",
    "\\\\\n",
    "&= \\sum_i^n p_i \\b{v}_i\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf0f75b-b9ee-46bb-b370-4cd69d43955f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83e5ca93-533b-4665-9fba-a368f1d47ebf",
   "metadata": {},
   "source": [
    "## 参考文献\n",
    "\n",
    "- [Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n",
    "  - 和訳：[イラストでみるTransformer](https://tips-memo.com/translation-jayalmmar-transformer)\n",
    "- [Encoder-Decoder ネットワーク [ディープラーニングの文脈で] | CVMLエキスパートガイド](https://cvml-expertguide.net/terms/dl/encoder-decoder/)\n",
    "- [【深層学習】Transformer - Multi-Head Attentionを理解してやろうじゃないの【ディープラーニングの世界vol.28】#106 #VRアカデミア #DeepLearning - YouTube](https://www.youtube.com/watch?v=50XvMaWhiTY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4902e91b-16cb-4fb5-9194-7a370ae9dff3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
