{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72ab4237-b112-4f1f-b6ca-457f220c3642",
   "metadata": {},
   "source": [
    "# 深層学習の歴史\n",
    "\n",
    "## はじまり\n",
    "\n",
    "人工ニューロン\n",
    "\n",
    "\n",
    "## 第1次ブーム\n",
    "\n",
    "1958年にパーセプトロンが提案される\n",
    "\n",
    "多層パーセプトロンにすると非線形モデルにでき、線形分離不可能な問題にも対応できるが、多層のニューラルネットワークを訓練する技術が不十分で下火となった\n",
    "\n",
    "## 第2次ブーム\n",
    "\n",
    "\n",
    "80年代には多層NNの学習方法である**誤差逆伝播法**（back propagation）が発明されてブームになった。\n",
    "\n",
    "しかし、\n",
    "\n",
    "1. 隠れ層1層程度のネットワークならうまくいくが、それ以上に多層になると過学習してしまい汎化誤差が削減できない問題\n",
    "2. 勾配消失問題（出力層から離れた層ほど伝播に伴って勾配が急速に小さくなったり大きくなって発散する問題）\n",
    "3. ネットワークの構造が最終的な性能とどう結びつくかの理論が不十分\n",
    "\n",
    "といった問題があって90年代には下火となった\n",
    "\n",
    "\n",
    "## 冬の時代\n",
    "\n",
    "deep belief networkという、ふるまいが確率的に記述される生成モデルが考案され、注目された。これは学習には事前学習を使うように工夫した。\n",
    "事前学習は層ごとに制約ボルツマンマシン（RBM）という単層ネットワークにした上で、教師無しで学習する。\n",
    "さらにRBMの代わりにautoencoderを使うことで多層ネットワークの学習ができることが確認された。\n",
    "\n",
    "この流れからニューラルネットワークの研究が加速していった。その後はReLU、Dropout、Batch Normalization、残差接続、Adamなどの手法も出てきた\n",
    "\n",
    "\n",
    "## 第3次ブーム\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2012年に大規模画像データベースであるImageNetを用いた画像分類のコンペILSVRCにおいて、AlexNetが2位に大差をつけて優勝し、ニューラルネットワークが注目されるようになった。\n",
    "\n",
    "AlexNetは8層でCNNを用いている\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff86a13b-00bb-4fdc-8bcb-5b099a95f0a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
