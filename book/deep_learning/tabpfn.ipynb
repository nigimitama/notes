{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "942d9605-53be-44bd-a845-41514481eeb4",
   "metadata": {},
   "source": [
    "# TabPFN\n",
    "\n",
    "\n",
    "## 概要\n",
    "\n",
    "**TabPFN（Tabular Prior-Data Fitted Network）** は、ドイツのチューリッヒ工科大学（ETH Zurich）らによって開発された、**事前学習済みのトランスフォーマーモデルによる表形式データ分類器** 。\n",
    "\n",
    "「**1秒でベイズ最適分類を近似**」というコンセプトで、少数データセットにおける汎用的な高精度分類を実現する。\n",
    "\n",
    "通常の機械学習はデータをもとに学習するが、TabPFNは**事前学習済みモデルを使うだけで即座に予測**できる点が大きな特徴。\n",
    "\n",
    "\n",
    "\n",
    "v2も出ている\n",
    "\n",
    "- v1: [[2207.01848] TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second](https://arxiv.org/abs/2207.01848)\n",
    "- v2: [Accurate predictions on small data with a tabular foundation model | Nature](https://www.nature.com/articles/s41586-024-08328-6)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86e030a-7007-4e78-aecf-853bcd39e622",
   "metadata": {},
   "source": [
    "\n",
    ":::{card} テーブルデータでGBDTと同程度の性能をより高速に発揮\n",
    "\n",
    "176のデータセットでGBDT系とNN系アルゴリズムを比較した [McElfresh et al. (2023). When Do Neural Nets Outperform Boosted Trees on Tabular Data?.](https://arxiv.org/abs/2305.02997) ではTabPFNが高い性能を持ちつつCatBoostよりも高速で、n=3000の少ないデータセットでもよいパフォーマンスを発揮していることを報告している。\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8b3453-e4f4-4a88-b413-bf2e351335e7",
   "metadata": {},
   "source": [
    "## コード例\n",
    "\n",
    "Github: [PriorLabs/TabPFN: ⚡ TabPFN: Foundation Model for Tabular Data ⚡](https://github.com/PriorLabs/TabPFN)\n",
    "\n",
    "PyPIからインストールできる\n",
    "\n",
    "```sh\n",
    "pip install tabpfn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696c1477-2f37-49b9-9e3a-1a0a9d89e7c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2849fee736b4f4ba64af9b3f75b186a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tabpfn-v2-regressor.ckpt:   0%|          | 0.00/44.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "995991436c6f4529b22b1709b40233a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/37.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 10.672898149858451\n",
      "R² Score: 0.8684473886974637\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tabpfn import TabPFNRegressor\n",
    "\n",
    "# Load Boston Housing data\n",
    "df = fetch_openml(data_id=531, as_frame=True)  # Boston Housing dataset\n",
    "X = df.data\n",
    "y = df.target.astype(float)  # Ensure target is float for regression\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize the regressor\n",
    "regressor = TabPFNRegressor()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"R² Score:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd461d0-f58d-4bca-9316-1ea809967d3b",
   "metadata": {},
   "source": [
    "## Prior-Data Fitted Networks (PFNs)\n",
    "\n",
    "[[2112.10510] Transformers Can Do Bayesian Inference](https://arxiv.org/abs/2112.10510) (ICLR2022)\n",
    "\n",
    "大規模な事前分布を学習させ、事後予測分布（posterior predictive distribution; PPD）を出力するためのTransformerベースのアーキテクチャであるPFNを提案\n",
    "\n",
    "ガウス過程やBayesian neural networksをMCMCより格段に少ない計算量で近似できる。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f94215-b9fb-45c4-841c-2845212a07ce",
   "metadata": {},
   "source": [
    "## TabPFN\n",
    "\n",
    "[[2207.01848] TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second](https://arxiv.org/abs/2207.01848)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4799e3b-1708-48e7-af71-b23b4bbff4c3",
   "metadata": {},
   "source": [
    "### 1. Prior-Data Fitted Network (PFN)\n",
    "\n",
    "PFNは、**多様なシミュレーションデータから事前に学習**し、「未知の表データ分布に対するベイズ最適分類器」を近似するニューラルネットワーク。  \n",
    "\n",
    "TabPFNはPFNを表形式データに特化させたアーキテクチャ。\n",
    "\n",
    "\n",
    "学習の流れ：\n",
    "\n",
    "1. 人工的に生成した数百万の「仮想データセット」を作成  \n",
    "2. 各データセットに対して「ベイズ最適分類結果（理論上の最良分類）」を計算  \n",
    "3. それを模倣するようにトランスフォーマーを訓練  \n",
    "4. 結果、未知のデータセットでも**ベイズ最適に近い予測**を瞬時に生成できるようになる\n",
    "\n",
    "### 2. モデル構造\n",
    "TabPFNは、**トランスフォーマーアーキテクチャ**を基盤にしています。  \n",
    "各サンプル（行）を「トークン」として処理し、自己注意機構（Self-Attention）によってデータ間関係をモデリングします。\n",
    "\n",
    "- **入力**: 小規模の表データ（通常は1000件未満）\n",
    "- **出力**: 各クラスの事後確率分布（Bayesian posterior）\n",
    "- **特徴**:\n",
    "  - 数値・カテゴリ混在データを直接扱える\n",
    "  - 欠損値処理も自動\n",
    "  - 標準化などの前処理も最小限\n",
    "\n",
    "\n",
    "### 3. 特徴と利点\n",
    "\n",
    "| 特徴 | 説明 |\n",
    "|------|------|\n",
    "| **学習不要** | 事前学習済みモデルを使うため、訓練不要で推論のみ |\n",
    "| **ベイズ的解釈** | 出力は事後確率を近似し、ベイズ分類器に相当 |\n",
    "| **超高速推論** | 小規模データなら数ミリ秒〜数秒で完了 |\n",
    "| **ハイパーパラメータ不要** | モデル選択・学習率・ツリー深度などの調整不要 |\n",
    "| **精度** | 多くのUCIデータセットでLightGBMやCatBoostに匹敵または上回る |\n",
    "\n",
    "### 4. 制約・限界\n",
    "\n",
    "| 制約 | 内容 |\n",
    "|------|------|\n",
    "| **データサイズ制限** | 通常は1000件以下・20特徴量以下が推奨 |\n",
    "| **回帰タスク非対応** | 現行モデルは分類専用（回帰版は研究中） |\n",
    "| **大規模データ非対応** | メモリ使用量が急増し、GPUが必要 |\n",
    "| **外挿に弱い** | 訓練分布と異なる構造のデータには不安定 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd496aa8-0052-48e0-9e8b-a5a9f8536142",
   "metadata": {},
   "source": [
    "## TabPFN v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5a8581-3f13-4057-be45-d5334e395d2d",
   "metadata": {},
   "source": [
    "## 関連論文\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c2a89-cad2-4d1a-a325-6fdc6796717d",
   "metadata": {},
   "source": [
    "\n",
    "### [[2502.17361] A Closer Look at TabPFN v2: Understanding Its Strengths and Extending Its Capabilities](https://arxiv.org/abs/2502.17361)\n",
    "\n",
    "目的：\n",
    "\n",
    "1. TabPFN v2 がなぜ多様なデータに対して高精度を出せるかを理解する\n",
    "2. TabPFN v2 が苦手とする高次元データ，多クラス，および大規模データに対して，その性能を拡張・改善する手法を提案する\n",
    "\n",
    "発見：\n",
    "\n",
    "1. 属性トークン学習を内在化している（Handling heterogeneity）\n",
    "    - TabPFN v2 では，属性ごとに決定的なトークン（Embedding）をあらかじめ定義するのではなく，推論時にランダムに perturbation を加えた属性トークンを用いる設計になっている（ランダム性を導入） \n",
    "    - それにもかかわらず，Transformer の in-context メカニズムを通じて，モデルは属性間の関係を推定し，意味的な特徴を捉える能力を獲得する。すなわち，「トークン学習」を明示的には行わず，推論中に属性関係を学び取っている。 \n",
    "    - 実験的には，複数回異なるランダム初期化でトークンを与えても，モデルの属性間の attention パターンなどは安定する，という結果が示されている。 \n",
    "    - → これにより，異なる次元数や属性意味を持つ新しいデータセットにも，チューニングなしで柔軟に適用できるという性質を実現している。\n",
    "2. 埋め込み器（feature encoder）への転用可能性\n",
    "    - 通常，TabPFN v2 は in-context learning によって直接ラベルを出力するが，本研究ではこれを利用してインスタンス表現（embedding）を取り出し，その上で線形分類器を載せることで予測する実験を行っている。 \n",
    "    - 単純な方法（すべての訓練例に対して dummy ラベルで embedding を抽出しようとする方法）は，訓練データとテストデータで表現空間がずれてしまい性能が出ないことが発覚する。 \n",
    "    - そこで提案されたのが leave-one-fold-out 戦略である。訓練データを複数の折り畳みに分け，ある折りを “クエリ”（dummy ラベル）として embedding を得て，他の折りを “サポート” として学習を保持する。この方式により，訓練とテストで比較可能な embedding を得られるようにする。 \n",
    "    - 実験では，こうして抽出された embedding に線形モデルを載せても，元の TabPFN v2 の性能に匹敵する結果が出るケースが多く，場合によっては複数層の embedding を連結して性能を上げることも可能であった。 \n",
    "    - → この結果は，TabPFN v2 が「強力な特徴空間変換器（変換後にクラスを線形分離しやすい空間を作るもの）」として機能していることを示唆する。\n",
    "3. テスト時の分割・統合法（divide‐and‐conquer）による性能拡張\n",
    "    - TabPFN v2 には，次のような制限が知られている（元論文でも指摘されていた）：\n",
    "        1. 次元数（特徴数）が多いデータ\n",
    "        2. クラス数が 10 を超える多クラス問題\n",
    "        3. サンプル数（規模）が大きいデータ\n",
    "    - これらに対して，本論文では以下のような後処理的（post-hoc）アプローチを提案して改善を図っている：\n",
    "\n",
    "| 問題領域            | 提案手法                                                                                                              | 概要                                                                       |\n",
    "| --------------- | ----------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------ |\n",
    "| 高次元             | 特徴のランダムサブセットを複数抽出 → 各サブセットで TabPFN v2 を適用 → 結果をアンサンブル（平均／多数決）                                                     | 特徴数の二次オーダーの計算負荷を抑えつつ性能を維持または改善|\n",
    "| 多クラス（クラス数 > 10） | 各ラベルを 10 進法表現に分解し，各桁を予測する（digitwise モデル） → 最終的に桁組み合わせでラベルを復元                                                      | 複数の TabPFN v2 モデルを使って多クラス対応を可能にする（およびランダムなマッピングとアンサンブルで安定化） |\n",
    "| 大規模データ          | ① 訓練セットから 10,000 サンプルをサポートセットとして抽出 → embedding 抽出 → 線形分類器訓練 → アンサンブル <br>② 決定木でデータを部分分割 → 各部分に TabPFN を適用 → 結果の統合 | 再訓練をせずに大規模データに対応可能とする工夫|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252ee5ff-10a2-4b1a-b68b-33e65b63eef0",
   "metadata": {},
   "source": [
    "### [[2506.08982] On Finetuning Tabular Foundation Models](https://arxiv.org/abs/2506.08982)\n",
    "\n",
    "Finetuningが不要なことが売りのTabPFNv2だが、Finetuningするとさらに性能が向上することを明らかにした\n",
    "\n",
    "https://www.themoonlight.io/ja/review/on-finetuning-tabular-foundation-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc78e0b-ec53-4be9-a2dc-10e4e0623c85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
