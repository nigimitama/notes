{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c72f007-bad7-4a02-8276-d1509a45ee56",
   "metadata": {},
   "source": [
    "# DeepSeek\n",
    "\n",
    "[DeepSeek](https://www.deepseek.com/)という中国の企業が開発したLLM\n",
    "\n",
    "計算量の削減の工夫が随所に込められており、2025年1月には高性能なLLMを安価に提供してOpenAIを脅かし、NVIDIAの株価を一日で17%暴落させた\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3103ed39-cf62-4a74-a7a9-7cd38a0d50c1",
   "metadata": {},
   "source": [
    "## DeepSeek-V2\n",
    "\n",
    "[\\[2405.04434\\] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434)\n",
    "\n",
    "- すごい点：計算量が圧倒的に少なく、安く作れる\n",
    "- 計算量削減の工夫：\n",
    "  - Multi-head Latent Attention (MLA)：Key, Valueの次元を大きく圧縮\n",
    "  - DeepSeekMoE：Shared ExpertとRouted Expertがあり、上位k個を選んで実行（使う分だけactivate）\n",
    "  - 推論時は高速化のためにモデルを8bit floating pointに変換して推論した\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2336d62a-8c12-490c-9a63-ca41c1349d30",
   "metadata": {},
   "source": [
    "## DeepSeek-V3\n",
    "\n",
    "[\\[2412.19437v1\\] DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437v1)\n",
    "\n",
    "- サイズ：671B params\n",
    "- 性能：GPT-4oやLlama3.1に匹敵\n",
    "- すごい点：計算量が圧倒的に少なく、安く作れる\n",
    "- 計算量削減の工夫：\n",
    "  - 多くの計算を8bit floating pointで行う\n",
    "  - Multi-head Latent Attention (MLA)：Key, Valueの次元を大きく圧縮\n",
    "  - DeepSeekMoE：Shared ExpertとRouted Expertがあり、上位k個を選んで実行（→671Bあっても各トークンで使うのは37B）\n",
    "- データ：14.8兆トークン"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d64b2c-d438-4600-8c72-466dc7dea181",
   "metadata": {},
   "source": [
    "## DeepSeek-R1-Zero\n",
    "\n",
    "- DeepSeek-V3-Baseをもとに強化学習で対話用にしたもの。GPT-3に対するInstructGPTのようなものと思われる。\n",
    "- すごい点：計算量が少ない\n",
    "- ChatGPT (instruct GPT) のようなPPO（生成結果の良し悪しをvalue modelで評価する）ではなくGRPO（自身の平均と比べて良いかどうかで学習）を使うことでvalue modelの学習・推論を不要にし、計算量を削減\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cbacf2-3ff8-4e9c-a191-3349a6142686",
   "metadata": {},
   "source": [
    "## DeepSeek-R1\n",
    "\n",
    "- DeepSeek-R1-Zero + Supervised Fine-Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143f5aa2-80dc-4816-b460-6308ab948215",
   "metadata": {},
   "source": [
    "## 参考\n",
    "\n",
    "- [DeepSeek-R1の論文読んだ？【勉強になるよ】](https://zenn.dev/asap/articles/34237ad87f8511#本論文の流れ)\n",
    "- [DeepSeek v3 の実力と活用法：6710億パラメータのオープンソースMoEモデル｜掛谷知秀](https://note.com/kakeyang/n/n3c9f5d8ad5b5?utm_source=chatgpt.com)\n",
    "- [Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
