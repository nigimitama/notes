{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e235f694-7c88-49ba-bf77-baed9844cc28",
   "metadata": {},
   "source": [
    "# LLMの評価"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bc9176-f755-4ca7-8b42-8b688f9c2059",
   "metadata": {},
   "source": [
    "### 評価方法\n",
    "\n",
    "Chang, et al. (2024). [A survey on evaluation of large language models.](https://dl.acm.org/doi/pdf/10.1145/3641289)\n",
    "\n",
    "#### 自動評価（Automatic Evaluation）\n",
    "\n",
    "\n",
    "精度（Accuracy）の観点では、\n",
    "\n",
    "- **Exact Match** ：文章生成タスクで、参考回答（reference answer）とまったく一緒だったかどうか、$\\{0,1\\}$\n",
    "- **F1 score** ：PrecisionとRecallの調和平均$\\text{F1 score} = \\frac{2 \\times \\text{Precision} \\times \\text{Recall} }{ \\text{Precision} + \\text{Recall} }$\n",
    "- **ROUGE** ：文章要約において、生成された要約文と人が作成した要約文がどれくらい一致しているかを測る\n",
    "    - 参考：[ROUGEを訪ねて三千里:より良い要約の評価を求めて #Python - Qiita](https://qiita.com/icoxfog417/items/65faecbbe27d3c53d212)\n",
    "\n",
    "Calibration（confidence level予測）の観点では\n",
    "\n",
    "- Expected Calibration Error (ECE)\n",
    "- Area Under the Curve (AUC)\n",
    "\n",
    "Robustnessの観点では\n",
    "\n",
    "- Attack Success Rate (ASR): 攻撃の成功率\n",
    "- Performance Drop Rate (PDR): prompt attackの後の性能悪化率\n",
    "\n",
    "#### 人による評価（Human Evaluation）\n",
    "\n",
    "- Human-in-the-loop testing：人間のフィードバックを集める。\n",
    "- Crowd-sourcing testing：クラウドワーカーに外注する。質は落ちるかもしれないが量が確保できる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01613b91-3d4f-4800-b490-58add4354c8a",
   "metadata": {},
   "source": [
    "## テスト / IRTによる評価\n",
    "\n",
    "- Blog: [Automated evaluation of RAG pipelines with exam generation - Amazon Science](https://www.amazon.science/blog/automated-evaluation-of-rag-pipelines-with-exam-generation)\n",
    "- Code: [amazon-science/auto-rag-eval: Code repo for the ICML 2024 paper \"Automated Evaluation of Retrieval-Augmented Language Models with Task-Specific Exam Generation\"](https://github.com/amazon-science/auto-rag-eval)\n",
    "\n",
    "手法の概要：LLMで4択テストを生成 → LLM+RAGで解く → テストの成績をIRTで評価\n",
    "\n",
    "所感：\n",
    "- 評価部分については、これまで人間相手に培ってきたIRTの研究成果をそのままLLMへ応用できるのは大きな強みとなりそう。\n",
    "- 一方で「どうやって良いテストを生成するか」はなかなかむずかしそう。\n",
    "- ただ、IRTの強みは等化だが、その必要性はあるのか？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
