{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b782aa8b-52a3-45c9-a121-457e6e1762e8",
   "metadata": {},
   "source": [
    "# 自然言語処理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a62361-2ffc-4c34-b987-eeb116bd9f9a",
   "metadata": {},
   "source": [
    "## トークン化\n",
    "\n",
    "**トークン化**（tokenize）とは、文を適当な単位に分割すること。分割された文の構成要素のことを**トークン**（token）と呼ぶ。\n",
    "\n",
    "自然言語処理においては、文章を計算可能な形状にするためにトークンへと分割し、対応するidを割り振っていく。\n",
    "\n",
    "ニューラルネットワークなどのアルゴリズムで作られるモデルにとって訓練時に見かけなかったトークンは**未知語**と呼ばれ、予測時には無視するか未知語を表す特定のidにマッピングされ、元の文字列の情報を損失してしまう。そのため、トークン化に際しては未知語を可能な限り少なくすることが望ましい。\n",
    "\n",
    "トークン化の方法は主に\n",
    "\n",
    "1. 単語分割（単語単位に分割する）\n",
    "2. サブワード分割（単語をさらにサブワード単位に分割する）\n",
    "3. 文字分割（文字単位に分割する）\n",
    "\n",
    "の3種類の分割方法がある。\n",
    "\n",
    "\n",
    "### 単語分割\n",
    "\n",
    "英語ならスペースで区切ればよい\n",
    "\n",
    "日本語の場合、形態素という単位に分割する形態素解析という技術を使う。\n",
    "\n",
    "\n",
    "### サブワード分割\n",
    "\n",
    "例えば\n",
    "\n",
    "```python\n",
    "[\"東京タワー\", \"大阪大学\", \"東京大学\"]\n",
    "```\n",
    "\n",
    "といった語彙があるとき、サブワードの語彙は次のようなものが考えられる。\n",
    "\n",
    "```python\n",
    "[\"東京\", \"大阪\", \"##タワー\", \"##大学\"]\n",
    "```\n",
    "\n",
    "地名と「##タワー」「##大学」を組み合わせれば元の語彙を表現できる。\n",
    "\n",
    "\n",
    "OpenAIの[Tokenizer](https://platform.openai.com/tokenizer)を使うとTokenが可視化されてイメージが付きやすい（ただし英語のみ）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015a9fcb-b648-4aee-81a2-bc1d1ac77309",
   "metadata": {},
   "source": [
    "## 言語モデル\n",
    "\n",
    "文章が生成される確率を推定したい。もっともらしい（確率的にありえる）文章を生成したい\n",
    "\n",
    "$$\n",
    "P(昨日は雨が降りました) > P(昨日は飴が降りました)\n",
    "$$\n",
    "\n",
    "文章をトークンに置き換えて考えると、文章の確率ではなくトークンの同時確率として扱うことができる\n",
    "\n",
    "$$\n",
    "P(昨日, は, 雨, が, 降り, ました) \n",
    "$$\n",
    "\n",
    "記号にすると、ある文章$S$をトークン化したのを$(w_1, w_2, \\cdots, w_n)$とすると、\n",
    "\n",
    "$$\n",
    "P(S)=P(w_1, w_2, \\cdots, w_n)\n",
    "$$\n",
    "\n",
    "を求めたいということになる。これは条件付き確率の積として表せる\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(w_1, w_2, \\cdots, w_n)\n",
    "&= P(w_1) \\times P(w_2|w_1) \\times P(w_3|w_1, w_2) \\times \\cdots\\\\\n",
    "&= \\prod_{i=1}^n p(w_i|\\boldsymbol{c}_i)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "ここで$\\boldsymbol{c}_i$は$w_i$より前のトークン列$\\boldsymbol{c}_i=(w_1,w_2,\\cdots,w_n)$で、**文脈**（context）と呼ばれる\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92144774-077f-4dc9-bd92-9ca71b03fd9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
