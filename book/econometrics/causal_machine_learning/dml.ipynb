{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f1d49eb-9627-4dc1-a162-1c460adee592",
   "metadata": {},
   "source": [
    "# Double/Debiased Machine Learning\n",
    "\n",
    "- Paper: [Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., & Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters.](https://academic.oup.com/ectj/article/21/1/C1/5056401?login=false)\n",
    "- Python Package: [DoubleML](https://docs.doubleml.org/stable/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6f2ed0-309a-45a5-9ea9-8d64236641fb",
   "metadata": {},
   "source": [
    "## 概要"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c236da-29af-4b5d-b1b5-865337fa6dcf",
   "metadata": {},
   "source": [
    "### motivation\n",
    "\n",
    "世の中の多くの現象は非線形な関係性が想定される。回帰分析は線形モデルであるため、モデルの定式化の誤りに起因するバイアスが生じかねない。\n",
    "\n",
    "実際に関心のあるパラメータは少なく、交絡のコントロールのために入れている局外母数（nuisance parameters）は高次元になりがち。\n",
    "\n",
    "局外母数を非線形モデルで表し、関心のあるパラメータは線形モデルで表現する部分線形モデル\n",
    "\n",
    "$$\n",
    "Y = \\theta D + g(X) + e\n",
    "$$\n",
    "\n",
    "を作り、非線形モデル$g(X)$を機械学習で構築したい"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a380c7c-6665-4f3a-9056-1fc5b19f000a",
   "metadata": {},
   "source": [
    "### 先行研究\n",
    "\n",
    "ノンパラメトリックモデルの文脈で研究があるが、Donsker条件を満たすような、複雑性の低い関数クラスでないとうまくいかないという理論解析がある\n",
    "\n",
    "機械学習のようなDonsker条件を満たさない複雑な関数でも使えるようにしたい"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a1ff47-0a94-4b95-92aa-3f9e0ffbd329",
   "metadata": {},
   "source": [
    "### 課題\n",
    "\n",
    "$g(X)$を機械学習で作って線形回帰するだけだと推定量は$\\sqrt{n}$の収束レートにならない\n",
    "\n",
    "2つのバイアスがある\n",
    "\n",
    "1. 正則化バイアス（Reguralization bias）\n",
    "2. 過学習によるバイアス（Bias induced by overfifting）\n",
    "\n",
    "それぞれの対策として、\n",
    "\n",
    "1. 正則化バイアス → 残差回帰（直交化）で対応\n",
    "2. 過学習 → Cross-Fittingで対応\n",
    "\n",
    "を行う"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4d7c9b-0829-4f39-8d66-9555a95909a2",
   "metadata": {},
   "source": [
    "## 前提知識\n",
    "\n",
    "知ってると理解を深めやすくなる前提知識まとめ\n",
    "\n",
    "### （参考）残差回帰\n",
    "\n",
    "DMLでは、FWL定理を利用した残差回帰を一般化する。\n",
    "\n",
    "#### 残差回帰\n",
    "\n",
    "通常の残差回帰は線形回帰モデル\n",
    "\n",
    "$$\n",
    "Y = X_1 \\beta_1 + X_2 \\beta_2 + e\n",
    "$$\n",
    "\n",
    "を用いて\n",
    "\n",
    "$$\n",
    "Y - X_2 \\hat{\\gamma} = (X_1 - X_2 \\hat{\\delta}) \\beta_1\n",
    "$$\n",
    "\n",
    "のようにして関心のあるパラメータ$\\beta_1$を推定する。\n",
    "\n",
    "線形回帰モデルは回帰関数$E[Y|X]$を近似するため、上記の残差回帰は期待値を用いて\n",
    "\n",
    "$$\n",
    "Y - E[Y|X_2] = (X_1 - E[X_1|X_2]) \\beta_1\n",
    "$$\n",
    "\n",
    "と表すことができる。\n",
    "\n",
    "#### 部分線形モデルへの拡張\n",
    "\n",
    "部分線形モデル\n",
    "\n",
    "$$\n",
    "Y = \\theta D + g(X) + e\n",
    "$$\n",
    "\n",
    "においても同様に\n",
    "\n",
    "$$\n",
    "Y - E[Y|X] = (D - E[D|X]) \\beta_1\n",
    "$$\n",
    "\n",
    "とするアプローチをとる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37143532-9a0a-4b66-80fe-6f00eb5135de",
   "metadata": {},
   "source": [
    ":::{admonition} FWL定理\n",
    "\n",
    "目的変数のベクトル$Y \\in \\mathbb{R}^{n\\times 1}$、説明変数の行列$X \\in \\mathbb{R}^{n\\times d}$と誤差項$e \\in \\mathbb{R}^{n\\times 1}$による線形回帰モデル\n",
    "\n",
    "$$\n",
    "Y = X \\beta + e\n",
    "$$\n",
    "\n",
    "があるとする。\n",
    "\n",
    "説明変数を$X = (X_1 | X_2)$と2つのグループに分割し、回帰係数ベクトル$\\beta$も合わせて$\\beta = (\\beta_1 | \\beta_2)^T$と2つに分割して\n",
    "\n",
    "$$\n",
    "Y = X_1 \\beta_1 + X_2 \\beta_2 + e\n",
    "$$\n",
    "\n",
    "\n",
    "と表す。\n",
    "\n",
    "この回帰モデルの$\\beta_1$は、**残差回帰**（residual regression）と呼ばれる以下の手順に従うことでも得ることができる。\n",
    "\n",
    "1. $X_1$を$X_2$に回帰して残差$\\tilde{X}_1$を得る：$\\tilde{X}_1 = X_1 - X_2 \\hat{\\delta}$\n",
    "2. $Y$を$X_2$に回帰して残差$\\tilde{Y}$を得る：$\\tilde{Y} = Y - X_2 \\hat{\\gamma}$\n",
    "3. $\\tilde{Y}$を$\\tilde{X}_1$に回帰させる：$\\tilde{Y} = \\tilde{X}_1 \\beta_1$\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eada7e7-27ee-4c44-b6ef-8a4124d457ba",
   "metadata": {},
   "source": [
    "### （参考）モーメント法\n",
    "\n",
    "確率変数$X$が$\\theta$というパラメータをもつ分布に従うとする。\n",
    "\n",
    "$$\n",
    "E[\\psi(X; \\theta)] = 0\n",
    "$$\n",
    "\n",
    "という条件（直交条件）を満たすスコア関数$\\psi(X; \\theta)$があるとき、標本$X_1, \\dots, X_n$を使った直交条件\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum^n_{i=1} \\psi(X_i; \\theta) = 0\n",
    "$$\n",
    "\n",
    "を解いて$\\theta$を推定する方法を **モーメント法** （method of moments） という。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fc2f3e-03c3-43d8-af4c-e9da93b6dcc8",
   "metadata": {},
   "source": [
    "## ナイーブな推定量と正則化バイアス\n",
    "\n",
    "話を簡潔にするために、サンプルを2つにランダム分割するとしよう：メインパートは$n$サンプルで$i\\in I$のインデックスで表し、補助パートは$N-n$として$i \\in I^c$とする。単純のため$n=N/2$とする。補助パートのサンプルで$\\hat{g}_0$を獲得し、$\\theta_0$の推定にメインパートのサンプルを使うことにする\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_0 = \\left( \\frac{1}{n} \\sum_{i\\in I} D_i^2 \\right)^{-1}\n",
    "\\frac{1}{n} \\sum_{i\\in I} D_i (Y_i - \\hat{g}_0(X_i))\n",
    "\\tag{1.3}\n",
    "$$\n",
    "\n",
    "\n",
    "推定量$\\hat{\\theta}_0$は一般に$1/\\sqrt{n}$より遅い収束レート、つまり\n",
    "\n",
    "$$\n",
    "|\\sqrt{n} (\\hat{\\theta}_0 - \\theta_0)| \\overset{p}{\\to} \\infty\n",
    "$$\n",
    "\n",
    "となる。\n",
    "\n",
    "この「劣った」振る舞いの背後には$g_0$の学習におけるバイアスがある。\n",
    "\n",
    "ヒューリスティックにこの$\\hat{g}_0$の学習のバイアスのインパクトを説明すると、スケールされた$\\hat{\\theta}_0$の推定誤差は\n",
    "\n",
    "$$\n",
    "\\sqrt{n} (\\hat{\\theta}_0 - \\theta_0) =\n",
    "\\underbrace{\n",
    "    \\left( \\frac{1}{n} \\sum_{i\\in I} D_i^2 \\right)^{-1}\n",
    "    \\frac{1}{\\sqrt{n}} \\sum_{i\\in I} D_i U_i\n",
    "}_{:=a}\n",
    "+\n",
    "\\underbrace{\n",
    "    \\left( \\frac{1}{n} \\sum_{i\\in I} D_i^2 \\right)^{-1}\n",
    "    \\frac{1}{\\sqrt{n}} \\sum_{i\\in I} D_i (g_0(X_i) - \\hat{g}_0(X_i))\n",
    "}_{:=b}\n",
    "$$\n",
    "\n",
    "である。\n",
    "\n",
    "第1項$a$は$a \\rightsquigarrow N(0, \\bar{\\Sigma})$となるので問題ない。第2項の$b$の項は正則化バイアス項で、一般に中心にならず発散する。first orderで以下を得る\n",
    "\n",
    "$$\n",
    "b = (E[D_i^2])^{-1}\n",
    "\\frac{1}{\\sqrt{n}} \\sum_{i\\in I}\n",
    "m_0(X_i)(g_0(X_i) - \\hat{g}_0(X_i)) + o_P(1)\n",
    "$$\n",
    "\n",
    "ヒューリスティックには、$b$は平均がゼロにならない$m_0(X_i)(g_0(X_i) - \\hat{g}_0(X_i))$の$n$個の総和で、$\\sqrt{n}$で割られる。これらの項は非ゼロの平均になる。なぜなら高次元あるいは複雑な問題設定のために我々はlasso, ridge, boostingあるいはpenalized neural netsなどの正則化推定量を採用するためである。\n",
    "\n",
    "\n",
    "それらの推定量において正則化は推定量の分散が爆発しないようにするものの相当なバイアスを引き起こす。とりわけ、$g_0$への$\\hat{g}_0$のバイアスの収束レートは、RMSEにおいて$n^{-\\phi_g}$（$\\phi_g < 1/2$）である。ゆえに$b$は$D_i$が$m_0(X_i)\\neq 0$で中心化されるとき$\\sqrt{n} n^{-\\phi_g} \\to \\infty$の確率的オーダーになることが期待され、よって$|\\sqrt{n} (\\hat{\\theta}_0 - \\theta_0)| \\overset{p}{\\to} \\infty$となる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a977cf4f-6627-43d8-9337-31905043a50d",
   "metadata": {},
   "source": [
    "### 直交化による正則化バイアスの打破\n",
    "\n",
    "直交化されたregressor $V=D-m_0(X)$を得るためにDからXの効果をpartialling outすることにより得られる直交化された式（orthogonalized formulation）を使う。具体的には\n",
    "$$\n",
    "\\hat{V} = D - \\hat{m}_0(X)\n",
    "$$\n",
    "ここで$\\hat{m}_0$は補助的サンプル（auxiliary sample）からを用いたML推定量。\n",
    "\n",
    "DからXの効果をpartialling out したあとは、main sampleを使って$\\theta_0$のdebiased ML (DML)を構築する\n",
    "$$\n",
    "\\check{\\theta}_0 = \\left( \\frac{1}{n} \\sum_{i\\in I} \\hat{V}_i D_i \\right)^{-1}\n",
    "\\frac{1}{n} \\sum_{i\\in I} \\hat{V}_i (Y_i - \\hat{g}_0(X_i))\n",
    "$$\n",
    "近似的にDをXについて直交化し、$g_0$の推定値を引くことで近似的に交絡の直接効果を除去することで、$\\check{\\theta}_0$は(1.3)の正則化バイアスを除去している。\n",
    "\n",
    "\n",
    "#### $\\check{\\theta}_0$の性質\n",
    "\n",
    "スケールされた推定誤差は３つの要素に分解できる\n",
    "\n",
    "$$\n",
    "\\sqrt{n}(\\check{\\theta}_0 - \\theta_0)\n",
    "= a^* + b^* + c^*\n",
    "$$\n",
    "\n",
    "$a^*$はmild conditionsのもとで以下を満たす\n",
    "\n",
    "$$\n",
    "a^* = (E[V^2])^{-1}\n",
    "\\frac{1}{\\sqrt{n}} \\sum_{i\\in I} V_i U_i \\rightsquigarrow N(0, \\Sigma)\n",
    "$$\n",
    "\n",
    "$b^*$は$g_0, m_0$の推定における正則化バイアスの影響を捉える。具体的には\n",
    "\n",
    "$$\n",
    "b^* = (E[V^2])^{-1}\n",
    "\\frac{1}{\\sqrt{n}} \\sum_{i\\in I} \n",
    "(\\hat{m}_0(X_i) - m_0(X_i))\n",
    "(\\hat{g}_0(X_i) - g_0(X_i))\n",
    "$$\n",
    "\n",
    "で、これは$\\hat{m}_0$と$\\hat{g}_0$の推定誤差の積に依存する。そのため、幅広い範囲のデータ生成過程のもとで消失させることが可能である。\n",
    "\n",
    "実際、この項は$\\sqrt{n}n^{-(\\phi_m+\\phi_g)}$で上界になり、ここで$n^{-\\phi_m}, n^{-\\phi_g}$はそれぞれ$\\hat{m}_0, \\hat{g}_0$の$m_0, g_0$への収束レートである。これは両者が比較的遅い収束レートで推定されたとしても、消失しうる。\n",
    "\n",
    "$c^*$は\n",
    "\n",
    "$$\n",
    "c^* = o_P(1)\n",
    "$$\n",
    "\n",
    "となる。これが弱い条件のもとで成り立つことを保証するにあたって、sample splittingが重要な役割を果たす。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59bc5eb-001c-4421-ad14-bd32e3a1e4bd",
   "metadata": {},
   "source": [
    "### Cross Fittingによる過学習のバイアスの除去\n",
    "\n",
    "sample splittingは$c^*$が確率的に消失するために使われる。\n",
    "\n",
    "$c^*$は\n",
    "$$\n",
    "\\frac{1}{\\sqrt{n}} \\sum_{i \\in I} V_i (\\hat{g}_0(X_i) - g_0(X_i))\n",
    "$$\n",
    "などの項を含む。この項は推定誤差と構造的未観測要因の積の和を$1/\\sqrt{n}$-normalizedしたものである。\n",
    "\n",
    "$E[V_i|X_i]=0$であることを思い出すと、この項は平均ゼロで分散は\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i \\in I} (\\hat{g}_0(X_i) - g_0(X_i))^2\n",
    "\\overset{p}{\\to}\n",
    "0\n",
    "$$\n",
    "であることがわかり、チェビシェフの不等式を使って確率的には消失することがわかる。\n",
    "\n",
    "sample splittingの欠点は推定に使用するサンプル数が減ることによる効率性の低下である。しかし、mainとauxiliaryの2つでそれぞれ推定を行い、両者の平均を取ればfull efficiencyを取り戻す。この手続き（mainとauxiliaryの役割を取り替えて複数の推定値を取得しそれらの平均をとる）を「**cross-fitting**」と呼ぶことにする。一般にk-foldにすることもできる。Section 3で詳細を述べる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33785df-0033-4712-8350-fc832017fe8a",
   "metadata": {},
   "source": [
    "## Neyman Orthogonality\n",
    "\n",
    "partialling outしたモーメント条件による推定量とナイーブな推定量は何が違うのか？\n",
    "\n",
    "→ ネイマン直交性（Neyman orthogonality）がカギになる。\n",
    "\n",
    ":::{admonition} ネイマン直交性\n",
    "\n",
    "サンプル$W$、関心のあるパラメータ$\\theta_0$、局外母数$\\eta_0$についてのスコア関数$\\psi(W; \\theta_0, \\eta_0)$のベクトル$\\psi = (\\psi_1, \\dots, \\psi_d)^T$があるとする。このスコア関数の直交条件\n",
    "\n",
    "$$\n",
    "E[\\psi(W; \\theta_0, \\eta_0)] = 0\n",
    "$$\n",
    "\n",
    "について、ガトー微分が存在し、\n",
    "\n",
    "\n",
    "ネイマン直交性（Neyman orthogonality）\n",
    "\n",
    "\n",
    "\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf05b85-f8ba-454b-8218-ac00f99e60fd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> We show that if the population risk satisfies a condition called Neyman orthogonality, the impact of the nuisance estimation error on the excess risk bound achieved by the meta-algorithm is of second order.\n",
    "> \n",
    "> [Foster, D. J., & Syrgkanis, V. (2023). Orthogonal statistical learning. The Annals of Statistics, 51(3), 879-908.](https://arxiv.org/pdf/1901.09036.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecde7aaf-87a1-491a-992c-d79171202f18",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d49b4121-c86b-4198-b9a5-ddfd0fe4cee6",
   "metadata": {},
   "source": [
    "## モーメント条件\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62298d13-3cb1-4d05-a70e-ff82497a9c4d",
   "metadata": {},
   "source": [
    "### モーメント条件と非線形モデル\n",
    "\n",
    "GMM推定量は非線形モデルに対しても用いることができる（末石2015, p.78）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad72778c-a493-419c-b2a1-8ee7d5364e5a",
   "metadata": {},
   "source": [
    "## 部分線形モデル\n",
    "\n",
    "結果変数$Y$、説明変数$X, Z$についての次のようなモデルを**部分線形モデル（Partially Linear Model）**という。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Y &= X^T \\beta + g(Z) + u\\\\\n",
    "E[u|X,Z] &= 0\\\\\n",
    "E[u^2|X,Z] &= \\sigma^2(X, Z)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "ここで、$g(\\cdot)$は任意の関数（非線形の関数でもよい）である。$\\sigma^2(\\cdot, \\cdot)$も未知の関数で、不均一分散を許容する。\n",
    "\n",
    "関心のあるパラメータ$\\beta$を解釈性の高い線形モデルで推定（パラメトリック推定）しつつ、影響を統制するためだけにモデルに投じている$Z$は関数形を特定せず推定（ノンパラメトリック推定）することができるため、部分線形モデルはセミパラメトリックモデルと呼ばれる。\n",
    "\n",
    "\n",
    "### Robinson (1983)の推定量\n",
    "\n",
    "$Z$で両辺の条件付き期待値をとった\n",
    "\n",
    "$$\n",
    "E[Y|Z] = E[X|Z]^T \\beta + g(Z)\n",
    "$$\n",
    "\n",
    "を差し引くと\n",
    "\n",
    "$$\n",
    "Y - E[Y|Z] = (X - E[X|Z])^T \\beta + u\n",
    "$$\n",
    "\n",
    "$\\tilde{Y}_i = Y_i - E[Y_i|Z]$、$\\tilde{X}_i = X_i - E[X_i|Z]$とおけば、OLS推定量の形になる\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\mathrm{inf}}=\\left(\\sum_{i=1}^n \\tilde{X}_i \\tilde{X}_i^{\\prime}\\right)^{-1} \\sum_{i=1}^n \\tilde{X}_i \\tilde{Y}_i\n",
    "$$\n",
    "\n",
    "が$E[Y|Z], E[X|Z]$が未知のため実行不可能である。$E[Y|Z], E[X|Z]$をそれぞれのノンパラメトリック推定量で置換して推定を行うことは可能であり、その推定量はroot-N consistentで漸近正規性を持つ\n",
    "\n",
    "\n",
    "### 参考\n",
    "\n",
    "- [末石直也 - セミ・ノンパラメトリック計量分析 (京都大学)](https://sites.google.com/site/naoyasueishij/teaching/nonpara) 第5回 [部分線形モデルとセミパラメトリック推定量の性質](https://drive.google.com/file/d/0B6W_J4QoAI6wcWdwYkNwUU5DWTA/view?resourcekey=0--WtAUb3PgzgBpsw1XtvhzQ)\n",
    "- 西山・人見（2023）『ノン・セミパラメトリック統計解析』、共立出版"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce25d2b-17b7-4ee6-8338-b80c47395c7d",
   "metadata": {},
   "source": [
    "### （疑問）Robinsonの推定量とDMLの違い、そうなった原因は？\n",
    "\n",
    "#### Robinsonの推定量とDMLの違い\n",
    "\n",
    "- 期待値を差し引くというFWL-style or Neiman-orthogonalityを使うのは一緒\n",
    "- モーメント条件が議論の根幹なのでOLS推定量の形には限らない？？\n",
    "- 差し引く期待値の推定をMLにしたのがDML\n",
    "\n",
    "#### 新規性は\n",
    "\n",
    "- ML推定ゆえoverfitting biasが入る → Cross Fitting\n",
    "  - なんでMLだとbiasがはいる？高次元や複雑なモデルはDonsker conditionを満たさないから？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f439431-285b-4655-a6d0-f82f07ce5eab",
   "metadata": {},
   "source": [
    "[4. Inference in Partially Linear Models](https://academic.oup.com/ectj/article/21/1/C1/5056401#130274171)\n",
    "\n",
    "PLR model\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Y&=D \\theta_0+g_0(X)+U, & E_P[U \\mid X, D]=0 \\\\\n",
    "D&=m_0(X)+V, & E_P[V \\mid X]=0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "の$\\theta_0$の推定方法は2つある\n",
    "\n",
    "1つめは\n",
    "\n",
    "$$\n",
    "\\psi(W ; \\theta, \\eta):=\\{Y-D \\theta-g(X)\\}(D-m(X)), \\quad \\eta=(g, m)\n",
    "$$\n",
    "\n",
    "というスコア関数のもとでのDML\n",
    "\n",
    "2つ目はRobinsonの\"partialling-out\"のスコア関数\n",
    "\n",
    "$$\n",
    "\\psi(W ; \\theta, \\eta):=\\{Y-\\ell(X)-\\theta(D-m(X))\\}(D-m(X)), \\quad \\eta=(\\ell, m)\n",
    "$$\n",
    "\n",
    "を使うもの"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02dd701-a339-4359-8ba3-a476da9bd36b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae31e8d1-08cf-47df-99f5-49c12ab9ab1a",
   "metadata": {},
   "source": [
    "## Donsker条件\n",
    "\n",
    "機械学習モデルの収束レートが遅い（Donsker条件）→Cross Fitting\n",
    "\n",
    "[XユーザーのMasahiro Katoさん: 「Double/debiased machine learningはどういう手法かというと，機械学習はDonsker条件を満たさないので，サンプルを分割することによって，異なる部分集合間で独立だと思えるような局外母数の推定量を構築することで，収束率だけで漸近正規性を出す手法です．」 / X](https://twitter.com/masakat0/status/1314897112316870657)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7709ed0c-6194-4ce0-977a-9ec4498f931a",
   "metadata": {},
   "source": [
    "## 考察\n",
    "\n",
    "### 機械学習を使うと常にバイアスが入る？？\n",
    "\n",
    "X-learnerとかどうなる？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d207765e-b350-440f-b8e5-51ed2675ea42",
   "metadata": {},
   "source": [
    "## 参考\n",
    "\n",
    "- 解説記事: [機械学習×計量経済学：Double/Debiased Machine Learning | Web日本評論](https://www.web-nippyo.jp/13331/)\n",
    "- [[勉強会資料メモ] Double/Debiased ML - Speaker Deck](https://speakerdeck.com/masa_asa/debiased-ml?slide=32)\n",
    "- [22 - Debiased/Orthogonal Machine Learning — Causal Inference for the Brave and True](https://matheusfacure.github.io/python-causality-handbook/22-Debiased-Orthogonal-Machine-Learning.html)\n",
    "- [CausalML Book](https://causalml-book.org/)の[Chapter 13](https://causalml-book.org/assets/chapters/CausalML_chap_13.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd937274-6e4d-4aa9-b604-5723d6d782dc",
   "metadata": {},
   "source": [
    "### DMLによるDID\n",
    "\n",
    "[Double/debiased machine learning for difference-in-differences models | The Econometrics Journal | Oxford Academic](https://academic.oup.com/ectj/article/23/2/177/5722119)\n",
    "\n",
    "- 解説: [DMLによる差分の差推定 - Speaker Deck](https://speakerdeck.com/masakat0/dmlniyoruchai-fen-falsechai-tui-ding)\n",
    "- 関連: [Double-Debiased-Machine-learning-estimator-for-Difference-in-Difference-with-Multiple-Periods.pdf](https://www.researchgate.net/profile/Di-Liu-124/publication/370440876_DoubleDebiased_Machine-learning_estimator_for_Difference-in-Difference_with_Multiple_Periods/links/645015ce5762c95ac3676c6e/Double-Debiased-Machine-learning-estimator-for-Difference-in-Difference-with-Multiple-Periods.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af36bf40-5c3c-40bc-bb71-8787981bebab",
   "metadata": {},
   "source": [
    "### 講義動画（Youtube）\n",
    "\n",
    "[Double Machine Learning for Causal and Treatment Effects - YouTube](https://www.youtube.com/watch?v=eHOjmyoPCFU&list=PLru50RuxzKFAsi9x3La3pidYURmi17ci6&index=4&t=479s)\n",
    "\n",
    "- MLでのcausal parametersの推定は良いとは限らない\n",
    "- double or orthogonalized MLとsample splittingによって、causal parametersの高精度な推定が可能\n",
    "\n",
    "Partially Linear Modelを使う\n",
    "\n",
    "$$\n",
    "Y = D \\theta_0 + g_0(Z) + U\n",
    ", \\hspace{1em} E[U|Z, D] = 0\n",
    "$$\n",
    "\n",
    "- MLをそのまま使うと一致推定量にならない（例えば$Y - D$で$g_0(Z)$をRandom Forestで学習しても、予測精度は良いがバイアスがある）\n",
    "- FWL定理を用いて、残差の回帰にするとよい\n",
    "\n",
    "$$\n",
    "\\hat{W} = Y - \\hat{E[Y|Z]}\\\\\n",
    "\\hat{V} = D - \\hat{E[D|Z]}\n",
    "$$\n",
    "\n",
    "\n",
    "モーメント条件\n",
    "\n",
    "1. Regression adjustment: $E[(Y - D \\theta_0 - g_0(Z) ) D] = 0$\n",
    "2. \"propensity score adjustment\": $E[(Y - D \\theta_0) (D - E[D|Z])] = 0$\n",
    "3. Neyman-orthogonal (semi-parametrically efficient under homoscedasticity): $E[(\\hat{W} - \\hat{V}\\theta_0) \\hat{V}] = E[\\{(Y - E[Y|Z]) - (D - E[D|Z])\\theta_0\\} (D - E[D|Z])] = 0$\n",
    "\n",
    "3は不偏\n",
    "\n",
    "\n",
    "Sample Splitting\n",
    "\n",
    "\n",
    "Splittingによるefficiencyの低下問題\n",
    "\n",
    "- 2個に分けて2回やって平均とればfull efficiency → k個に分けての分析をk回やって平均とってもfull efficiency\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d75e8c3-e47b-4837-893b-93e8d4e9b7ea",
   "metadata": {},
   "source": [
    "## 応用研究\n",
    "\n",
    "[[2002.08536] Debiased Off-Policy Evaluation for Recommendation Systems](https://arxiv.org/abs/2002.08536)\n",
    "- PR: [AI Lab、推薦システム分野におけるトップカンファレンス「RecSys2021」にて共著論文採択 ー高次元なデータを使った意思決定評価の信頼性を改善ー | 株式会社サイバーエージェント](https://www.cyberagent.co.jp/news/detail/id=26585)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30000948-1e7d-4308-bb74-224a8e0d9ab1",
   "metadata": {},
   "source": [
    "## 関連研究\n",
    "\n",
    "[[2305.04174] Root-n consistent semiparametric learning with high-dimensional nuisance functions under minimal sparsity](https://arxiv.org/abs/2305.04174)\n",
    "\n",
    "- 先行研究まとめがある"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20212094-835a-484d-beee-4a7e6bf16f5e",
   "metadata": {},
   "source": [
    "[[2008.06461] Estimating Structural Target Functions using Machine Learning and Influence Functions](https://arxiv.org/abs/2008.06461)\n",
    "\n",
    "- Influence Function Learningという新しいフレームワークを提案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93153934-45f9-4fdf-b028-2c39a8c280ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "472c2f39-fb84-484a-b008-967718d6161e",
   "metadata": {},
   "source": [
    "[Library Flow Chart — econml 0.15.0 documentation](https://econml.azurewebsites.net/spec/flowchart.html)\n",
    "- 派生モデルの使い分けについて"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0066bdb2-0e96-42e7-98e8-c709e3c3ade8",
   "metadata": {},
   "source": [
    "## 部分線形モデルに対するモーメント条件\n",
    "\n",
    "異なる推定量でどれだけバイアスが入るか確認したい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c4733fd-b92d-4555-8f52-0750e976a461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 1000\n",
    "np.random.seed(0)\n",
    "x = np.random.uniform(size=n)\n",
    "theta = 3\n",
    "d = x + np.random.normal(size=n)\n",
    "y = d * theta + x + np.random.normal(size=n)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"y\": y, \"d\": d, \"x\": x})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81b7d94-9eed-4e8b-b43c-e105bf795810",
   "metadata": {},
   "source": [
    "ナイーブな推定量\n",
    "\n",
    "$$\n",
    "E[UD] = E[(Y  - D\\theta - g(X)) D] = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_0=\\left(\\frac{1}{n} \\sum_{i \\in I} D_i^2\\right)^{-1} \\frac{1}{n} \\sum_{i \\in I} D_i\\left(Y_i-\\hat{g}_0\\left(X_i\\right)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c13ba7-85d9-403d-8ae3-4f2ea433d125",
   "metadata": {},
   "source": [
    "かりに、$\\hat{g}(X_i)$は$Y_i$を$X$に回帰する（予測値$\\hat{Y}_i = \\hat{g}(X_i)$を作る）とするなら"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b43c3ae3-f7d3-4dce-b3ad-600d230d9cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -1.3519</td> <td>    0.045</td> <td>  -29.781</td> <td> 0.000</td> <td>   -1.441</td> <td>   -1.263</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>d</th>         <td>    2.5753</td> <td>    0.040</td> <td>   64.109</td> <td> 0.000</td> <td>    2.496</td> <td>    2.654</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lcccccc}\n",
       "\\toprule\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept} &      -1.3519  &        0.045     &   -29.781  &         0.000        &       -1.441    &       -1.263     \\\\\n",
       "\\textbf{d}         &       2.5753  &        0.040     &    64.109  &         0.000        &        2.496    &        2.654     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Y ~ X\n",
    "g = LGBMRegressor(max_depth=3, verbose=-1)\n",
    "g.fit(df[[\"x\"]], df[\"y\"])\n",
    "\n",
    "# y_res := Y - g(X)\n",
    "# y_res ~ D のOLS\n",
    "import statsmodels.formula.api as smf\n",
    "final_model = smf.ols(\n",
    "    formula='y_res ~ d',\n",
    "    data=df.assign(\n",
    "        y_res = df[\"y\"] - g.predict(df[[\"x\"]])\n",
    "    )\n",
    ").fit()\n",
    "final_model.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6626924-f69d-4712-869d-56c68751619c",
   "metadata": {},
   "source": [
    "### DML score function\n",
    "\n",
    "\n",
    "ネイマン直交性を満たす\n",
    "\n",
    "$$\n",
    "E[ \\{Y-D \\theta-g(X)\\}(D-m(X)) ]=0\n",
    "$$\n",
    "\n",
    "$\\hat{V}= D-\\hat{m}_0(X)$\n",
    "\n",
    "$$\n",
    "\\check{\\theta}_0=\\left(\\frac{1}{n} \\sum_{i \\in I} \\widehat{V}_i D_i\\right)^{-1} \\frac{1}{n} \\sum_{i \\in I} \\widehat{V}_i\\left(Y_i-\\hat{g}_0\\left(X_i\\right)\\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b0b38a-714a-42ad-98cc-25f0bddf1ec6",
   "metadata": {},
   "source": [
    "### Robinson-style \"partialling-out\" score function\n",
    "\n",
    "ネイマン直交性を満たす\n",
    "\n",
    "$$\n",
    "\\mathrm{E}\\left[\\left((Y-E[Y \\mid X])-(D-E[D \\mid X]) \\theta_0\\right)(D-E[D \\mid X])\\right]=0\n",
    "$$\n",
    "\n",
    "$V=D - E[D|X] = D-m_0(X)$として、$\\ell_0(X) := E[Y|X]$とすると\n",
    "\n",
    "$$\n",
    "\\mathrm{E}\\left[\\left((Y-\\ell_0(X))-V \\theta_0\\right)V\\right]=0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f13af7-c789-40c9-a523-7b1cceb01472",
   "metadata": {},
   "source": [
    "### cross fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b4995a-21ef-4639-86c3-ce6a46b45c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-fittingあり\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "debias_m = LGBMRegressor(max_depth=3, verbose=-1)\n",
    "df[\"d_res\"] = df[\"d\"] - cross_val_predict(debias_m, df[[\"x\"]], df[\"d\"], cv=5)\n",
    "\n",
    "denoise_m = LGBMRegressor(max_depth=3, verbose=-1)\n",
    "df[\"y_res\"] = df[\"y\"] - cross_val_predict(denoise_m, df[[\"x\"]], df[\"y\"], cv=5)\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "final_model = smf.ols(formula='y_res ~ d_res', data=df).fit()\n",
    "final_model.summary().tables[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
